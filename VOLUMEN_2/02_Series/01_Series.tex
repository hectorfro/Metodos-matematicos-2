\section*{La ruta de este capítulo}
\section{Sucesiones y Series}
\label{IntroSeries}

Básicamente el tema central de este curso tiene que ver con el estudio de las ecuaciones diferenciales ordinarias y el desarrollo de métodos para  resolverlas. Las ecuaciones diferenciales aparecen ya como tema de estudio, o curiosidad matemática, desde los tiempos de Isaac   Newton\footnote{{\bf Sir Isaac Newton} (1643-1727),  fue un científico, físico, filósofo,  inventor, alquimista y matemático inglés, autor de los {\it Philosophiae Naturalis Principia Mathematica}, más  conocidos como los Principia, donde describió la ley de gravitación universal y estableció las bases de la Mecánica Clásica mediante las leyes que llevan su nombre. Entre sus otros descubrimientos científicos  destacan los trabajos sobre la naturaleza de la luz y la óptica   y el desarrollo del cálculo matemático. Newton fue el primero en demostrar que las leyes naturales que gobiernan el movimiento en la Tierra y las que gobiernan el movimiento de los cuerpos celestes son las mismas. Es, a menudo, calificado como el científico más grande de todos los tiempos, y su obra como la culminación de la revolución científica.  (Tomado  de Wikipedia).}  cuando  se comenzó con el  desarrollo del Cálculo Diferencial. El propio Newton  las estudia en  su tratado de cálculo diferencial donde discute sus soluciones a través de una expansión en series. 

Newton estudia la siguiente ecuación diferencial, que por contener una primera derivada llamaremos una  ecuación diferencial de primer orden:
\begin{equation}
\frac{d y(x)}{d x} = 1-3x+y(x)+x^2+xy(x) \,. 
\label{ecuanewton}
\end{equation}
Para buscar su solución, Newton propone el siguiente método que consiste en suponer una  solución que tiene la forma de una serie infinita. El primer término de la serie es:
\[
y=0 + \cdots 
\]
el cual corresponde al valor inicial $x=0$ en la ecuación (\ref{ecuanewton}). 

Al insertar este valor en (\ref{ecuanewton}) resulta
\[
\frac{d y(x)}{d x}=1+ \cdots 
\]
que al integrarse se obtiene
\[
y=x + \cdots 
\]
Sustituyendo esta última expresión en (\ref{ecuanewton}), resulta
\[
\frac{d y(x)}{d x} = 1-3x+ x + \cdots  = 1-2x + 2x^2 + \cdots 
\]
Integrando esta última ecuación, se obtiene
\[
y=x-x^2  + \frac{2}{3}x^2  + \cdots 
\]
Repitiendo el proceso:
\[
\frac{d y(x)}{d x} = 1-2x + x^2 - \frac{1}{3}x^3+\frac{2}{3}x^4
\cdots \quad \Rightarrow \quad 
y=x-x^2 +\frac{x^3}{3} - \frac{1}{12}x^4 + \frac{2}{15}x^5+ \cdots 
\]
Continuando con el método repetidamente se van calculando los diferentes términos de la serie
\[
y=x-x^2 +\frac{x^3}{3} -\frac{x^4}{6}+  \frac{x^5}{30}  - \frac{x^6}{45}  + \frac{x^7}{630} + \cdots 
\]

En la figura \ref{originalnewton} se puede apreciar parte del manuscrito original donde aparece el esquema de solución por series propuesto por Newton.

\begin{figure}[t]
\begin{center}
\includegraphics[width=5.0in]{VOLUMEN_2/02_Series/Figuras/newton1}
\caption{ \small El esquema de Newton }
\label{originalnewton}
\end{center}
\end{figure}

Por otra parte, notemos que para cada valor de $x$ y $y$, la ecuación (\ref{ecuanewton}) no es más que la derivada $y'(x)$, es decir la pendiente, de las soluciones de:
\[
y'(x)= 1-3x+y(x)+x^2+xy(x) \,.
\]

De esta manera, con un poco de paciencia, o con algún programa de computación algebraico apropiado, se puede obtener el {\it campo vectorial} correspondiente a la ecuación diferencial, como se puede apreciar en la figura  \ref{curvasvectoriales} (figura 2a).  En realidad las soluciones se pueden ver como las curvas determinadas por las direcciones indicadas  en el campo vectorial, figura 2b. Las aproximaciones que se van obteniendo con el método mostrado anteriormente se pueden observar, junto con la solución verdadera en la Figura 2c. Notemos que todas las aproximaciones son más cercanas entre si cuando $x$ toma valores cada vez más próximos a cero.
\begin{figure}[t]
\begin{center}
\includegraphics[width=2.0in]{VOLUMEN_2/02_Series/Figuras/Fig3a}
\includegraphics[width=2.0in]{VOLUMEN_2/02_Series/Figuras/Fig3b}
\includegraphics[width=2.0in]{VOLUMEN_2/02_Series/Figuras/Fig3c}
\caption{\small (a) Representación de los campos vectoriales. (b) Diferentes soluciones para la  ecuación diferencial (\ref{ecuanewton}). (c) La solución correcta correspondiente al  valor inicial $y(0)=0$  con las diferentes soluciones aproximadas.}
\label{curvasvectoriales}
\end{center}
\end{figure}

Comencemos entonces nuestro estudio sobre ecuaciones diferenciales precisamente con las series matemáticas.

\subsection{Introducción a las sucesiones}

Para empezar, vayamos a la noción elemental de sucesiones. Básicamente, una sucesión es una  colección numerable de elementos, dados en cierto orden, como:
\[
1, 2, 3, 4, \, \dots \,, \quad 1, x, x^2, x^3,  \, \dots \,, \quad
a, b, c, d \,.
\]

En matemáticas se puede entender las sucesiones como una aplicación desde los números naturales a cualquier otro conjunto, no necesariamente de la misma naturaleza. Estos números se denominan los términos, elementos o miembros de la sucesión. Siendo más rigurosos, una sucesión se puede definir como una función sobre el conjunto de los números naturales, es decir, que estaríamos hablando en este caso de una función discreta.

Es bastante común que se confundan las sucesiones con series matemáticas, veremos que una serie entenderemos es en realidad una suma de términos.

Las sucesiones se diferencian de los conjuntos en el hecho de que es importante considerar el orden en que aparecen distribuidos.  Además, un mismo término de una sucesión puede aparecer varias veces en posiciones diferentes. Esto significa que las siguientes sucesiones finitas son diferentes:
\[
a, b, c, d  \quad \neq \quad  b, c, d, a \,.
\]

Una sucesión puede contener infinitos elementos y en este caso se denomina una sucesión infinita, por lo tanto, si a cada número entero positivo se le asocia un elemento $u_n$, entonces el conjunto ordenado: 
\[
u_1, u_2, u_3, . . . u_n, \, \dots \,. 
\]
define una sucesión infinita. Cada término $u_n$ tendrá un siguiente término $u_{n+1}$ y por lo tanto no existe un último término. 

Las sucesiones se pueden expresar de una manera más sencilla definiendo el $n$-ésimo término, como por ejemplo: 
\[
u_n=\frac1n \,, \qquad n=1, 2, 3 , \dots
\]
cuyos primeros cuatro términos son: 
\[
1, \frac12, \frac13, \frac14 \,. 
\]

Otra manera de definir sucesiones es por medio de una relación de recurrencia, por ejemplo, para la bien conocida sucesión de Fibonacci la relación de recurrencia es:
\[
u_1=u_2=1\,\,, \qquad u_{n+1}=u_n +u_{n-1} \,\,, \qquad n \geq 2\,, 
\]
y cuyos primeros términos son: 
\[
1, 1, 2, 3, 5, 8, 13, 21, 34, \dots 
\]

Como curiosidad matemática, podemos ver que existe una función que permite generar los números de Fibonacci, esta función cuando se expande en potencias de $x$ tiene como coeficientes, ¡los números de Fibonacci!
\[
f(x)= \frac{x}{1-x-x^2}= x+{x}^{2}+2\,{x}^{3}+3\,{x}^{4}+5\,{x}^{5}+8\,{x}^{6}+13\,{x}^{7}+21
\,{x}^{8}+34\,{x}^{9} + \cdots
\]

También es posible definir una sucesión a través del concepto de función. Se define una función para los enteros positivos, de manera que $f(n)$ es el término $n$-ésimo de la sucesión para cada $n=1, 2, 3, \dots$. Por lo tanto, los términos de las sucesión se escriben como:
\[
f(1), f(2), f(3), . . . f(n), \dots
\]

Las siguientes fórmulas son ejemplos de sucesiones:
\begin{eqnarray*}
f(n) &=& (-1)^n \,\, \Rightarrow \,\, -1, 1, -1, 1, -1, \dots \\
f(n)&=& \mbox{sen}\left(\frac{n\pi}{2}\right) \,\, \Rightarrow \,\, 
\sin \left( \frac{\pi}{2} \right), \sin \left( \pi \right), 
\sin \left( \frac{3\pi}{2}  \right), \sin \left( 2\,\pi \right),  
\sin \left( \frac{5 \pi}{2} \right)\dots \\
f(n)&=&(-1)^n\left[1+\frac1n \right] \,\, \Rightarrow \,\,
-2, \frac32,  -\frac43,  \frac54,  -\frac65 \dots
\end{eqnarray*}

Las sucesiones pueden tener la característica de ser crecientes o decrecientes. Una sucesión  $\{ f(n)\}$ se dice que es creciente si:
\[
f(n) \leq f(n+1) \quad \forall \quad n \geq 1 \,,
\]
es decir, cada término es menor o igual al término siguiente. Y si de impone la condición $f(n) < f(n+1)$ se denomina {\it estrictamente creciente}. Por otro lado, una sucesión se llama decreciente si:
\[
f(n) \geq f(n+1) \quad \forall \quad n \geq 1\,.
\]
y {\it estrictamente decreciente} si  $f(n) > f(n+1)$.

%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{minipage}{7.4cm}
Las sucesiones pueden estar acotadas de diferente manera. Diremos que la sucesión $\{ f(n)\}$ es acotada {\it superiormente}  si existe un número positivo $M$ tal que $f(n)\leq M $ para todo $n$. También puede ser acotada {\it inferiormente} si existe un número positivo $N$ tal que $f(n)\geq N $ para todo $n$. En el caso de cumplirse ambas condiciones hablaremos de una sucesión acotada $N \leq f(n) \leq M$.

Podemos hablar de sucesiones monótonas cuando la diferencia entre un término y el que le sigue siempre es del mismo signo y además son del tipo creciente o decreciente.

Nos vemos ahora en la necesidad de hablar del significado de los términos convergencia o divergencia de una sucesión.
\end{minipage} \hfill 
\begin{minipage}{8.0cm} 
\includegraphics[width=3.0in]{VOLUMEN_2/02_Series/figuras/Converging.jpg}
\caption{Sucesión convergente a cero.}
\label{FigSerieConverge}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%

La convergencia o divergencia se determina de manera sencilla, como lo indica el siguiente teorema:

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema}: Una sucesión monótona converge si y sólo si es acotada.
\end{mdframed}

Revisemos el concepto de convergencia (divergencia). Cuando una sucesión converge, lo que significa es que tiende a un valor particular llamado su límite, diremos en éste caso la sucesión será una {\it sucesión convergente}. Las sucesiones que no son convergentes se denominan {\it divergentes}.

En otras palabras, una sucesión tiene límite si los elementos de la sucesión se hacen cada vez más y más cercanos a algún valor  $L$ (llamado límite de la sucesión). Esto  significa que dado un número real $\varepsilon$  mayor que cero, todos menos un número finito de elementos de la sucesión estarán siempre a una distancia a $L$ menor que $\varepsilon$.

Matemáticamente es lo siguiente: 
\begin{equation}
\lim_{n \rightarrow \infty } f(n)=L \,.
\label{limisuce}
\end{equation}

Si una sucesión $f(n)$ converge, entonces el límite \ref{limisuce}  existe, es único y la sucesión es acotada.

En la figura \ref{FigSerieConverge} podemos ver una representación gráfica para la sucesión:
\[
f(n)=\frac{n+1}{2n^2} \,.
\]
Esta serie converge, y es fácil ver que:
\[
\lim_{n \rightarrow \infty } \frac{n+1}{2n^2}=0 \,.
\]

%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{minipage}{7.4cm}
También existen las sucesiones oscilantes, las cuales suelen ser divergentes por no tener límite. Estas sucesiones presentan términos que se alternan de manera indefinida. Cuando cambian de signo se llaman sucesiones alternadas, como la que mencionamos anteriormente $f(n) = (-1)^n \,\, \Rightarrow \,\, -1, 1, -1, 1, -1, \dots $.

Un ejemplo particular de sucesiones oscilantes convergentes es la {\it sucesión de Cauchy}. Una sucesión de números reales: $x_1, x_2, x_3, \dots$ se denomina una sucesión de Cauchy fundamental si para todo número positivo $\varepsilon$ existe un entero positivo $M$ tal que para todos los números naturales $n, m > M$ se cumple:
\[
\left|x_m-x_n \right| <\varepsilon \,.
\]
\end{minipage} \hfill 
\begin{minipage}{8.0cm} 
\includegraphics[width=3.0in]{VOLUMEN_2/02_Series/figuras/Cauchy_sequence.jpg}
\caption{Sucesión de Cauchy.}
\label{FigSerieCauchy}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%

En este tipo de sucesiones los elementos se la sucesión se vuelven arbitrariamente cercanos entre sí a medida que  la sucesión progresa, como se puede ver en la figura \ref{FigSerieCauchy}. Esta condición, necesaria para hablar de convergencia, se llama la condición de Cauchy. 

Existe una serie de resultados interesantes que podemos mencionar:
\begin{itemize}
\item Toda sucesión convergente es de Cauchy.
\item Toda sucesión de Cauchy está acotada.
\item Para los números reales $\mathds{R}$, toda sucesión de Cauchy es convergente.
\item Toda sucesión convergente está acotada.
\end{itemize}


A medida que $n$ se hace cada más grande, el valor de una sucesión $\{f(n)\}$ o $\{u_n\}$ se puede comportar de una manera bastante particular. Por ejemplo, si $u_n=1/n$, es claro que $u_n$ {\it converge} a cero a medida que $n \rightarrow \infty$. Pero si $u_n=e^{\alpha n}$, el límite dependerá del valor de $\alpha$. 

Por lo tanto, la pregunta a responder tiene que ver con el hecho de saber si los términos de $u_n$ tienden, o no,  a un límite finito cuando $n$ crece indefinidamente.

\subsection{Acercándonos al concepto de series}

Con el concepto de sucesiones es posible definir una expresión analítica que formalmente tiene aspecto de suma, que contiene un número infinito de sumandos y que denominaremos {\it serie infinita}. 

Si $\{u_n\}$ (con $n=1, 2, 3, \dots$) es una sucesión infinita de números reales o complejos, es posible formar una nueva sucesión $\{s_n\}$ a partir de tomar sumas parciales de $\{u_n\}$. Veamos el procedimiento:
\[
s_1=u_1\,,\quad 
s_2=u_1+u_2\,,\quad 
s_3=u_1+u_2+u_3\,, \quad  \dots \quad  
s_n=u_1+u_2+u_3+ \cdots u_n = \sum_{i=1}^n \, u_i\,.
\]

Es decir, partimos con $s_1=u_1$ y decimos que para todo $n \in \mathds{N}$ se tiene que $s_{n+1}=s_n+u_{n+1}$. La sucesión  $\{s_n\}$ la llamaremos la serie de término general $a_n$ definida a través de la sucesión $\{u_n\}$ y la representaremos por:
\begin{equation}
\label{defserie}
\sum_{n=1}^{\infty} \, u_n \,.
\end{equation}
Al número 
\[
s_n=\sum_{i=1}^{n} \, u_i \,
\]
le denominaremos la suma parcial de orden $n$ de la serie (\ref{defserie}).

Es importante aclarar que una serie es una sucesión cuyos términos se obtienen al sumar de manera consecutiva lo términos de una sucesión diferente. 

Si la sucesión $s_n$ tiende a un límite $S$, la serie infinita 
\[
\sum_{i=1}^\infty \, u_i \,,
\]
se dice que es convergente y converge al valor $S$, el cual es único. Entonces se puede escribir:
\begin{equation}
\label{def_serie}
S=u_1+u_2+u_3+\cdots + u_n + \cdots = \sum_{n=1}^\infty \, u_n =\lim_{n \rightarrow \infty} \{s_n\}= \lim_{n \rightarrow \infty} 
\sum_{i=1}^n \, u_i \,.
\end{equation}

El número $S$ se denomina {\it la suma de la serie} infinita y debe ser entendido como el límite de la sucesión. 


Lo anterior se puede formalizar  diciendo que la condición para la existencia de un límite $S$ es que  para cada $\varepsilon > 0$ existe un número $N=N(\varepsilon) \in \mathds{N}$ tal que:
\[
| S - s_{i} | < \varepsilon \quad \text{para} \ i > N 
\,\, \Rightarrow \,\,  
| s_{j}- s_{i} | < \varepsilon \quad \text{para, todo } \ i,j > N \,.
\]

Esta afirmación se denomina \textbf{criterio de Cauchy}\footnote{{\bf Augustin Louis Cauchy} París, 1789 - 1857, matemático francés pionero en los estudios de análisis (real y complejo) y de la teoría de los grupos de permutación. Cauchy hizo aportes importantes en los criterios de convergencia y divergencia de series infinitas, así como también, en ecuaciones diferenciales, determinantes, probabilidades y física matemática} sobre la convergencia de las series parciales, y viene a ser la condición necesaria y suficiente para que una suma parcial $s_{i}$ converja a medida que avanzamos en los términos de la serie.

Se dirá que la serie \textit{diverge} si el valor de la sumatoria aumenta indeteniblemente.

La serie  también puede oscilar:
\[
s_{i}=\sum_{n=1}^{i} ( -1 )^{n}= 1-1+1-1+ \cdots + (-1 )^{i}+ \cdots 
\] 
Aquí $s_n$ será $0$ o $1$ y por lo tanto el límite no existe.

La serie cuyos términos son tomados a partir del $(n+1)$-ésimo término, y en el mismo orden,  de la serie (\ref{def_serie}) se llama el {\it resto} $n$-ésimo de la serie  (\ref{def_serie}) y se denota por:
\begin{equation}
\sum_{k=n+1}^{\infty} u_k \,\, \Rightarrow \,\, 
u_{n+1} +  u_{n+2} +  u_{n+3} + \cdots \,.
\end{equation}

Si el resto $n$-ésimo de la serie (\ref{def_serie}) converge, entonces su suma:
\begin{equation}
r_n= \sum_{k=n+1}^{\infty} u_k \,,
\end{equation}
se denomina el {\it resto de la serie}.

De las series nos interesa conocer cuánto suman. Es decir, cuál es el valor de $s_{i}$ para una serie finita donde $i = N$. Pero también estamos interesados en conocer cuánto suma una serie infinita. 

\subsection{Series elementales}
Probablemente, de cursos anteriores  hemos conocido algunas series emblemáticas, estudiemos aquí algunas de estas series:

\begin{itemize}

\item {\bf Serie aritmética}

Seguramente con anterioridad  hemos oído hablar de progresiones aritméticas. Ellas son, sencillamente series de la forma:
 \[
 s_{N}=\sum_{n=0}^{N-1} (a+nd)= a+(a+d)+(a+2d)+(a+3d)+(a+4d)+\cdots+  \left[a +(N-1)d \right]\,,
 \]
donde $a$ y $d$ son números reales o complejos.
  
Al desarrollar la serie anterior en orden inverso y sumarla con la serie original obtemos:
  \[
 \begin{array}{rcccccc}
s_{N}= & a & +(a+d) & +(a+2d) & +(a+3d)  & +\cdots &+ \left[a +(N-1)d \right] \\
s_{N}= & \left[a +(N-1)d \right] & + \left[a +(N-2)d \right] & + \left[a +(N-3)d \right] & + \left[a +(N-4)d \right] & +\cdots &+ a
 \end{array} 
\]  
resultando:
\[
2 s_{N}={N}\left[a +a + (N-1) d \right] \,\, \Rightarrow \,\, 
s_{N}= \frac{N}{2}\left[\text{Primer Término} + \text{Último Término} \right]
\] 
obviamente, si $N \rightarrow \infty$ la serie diverge.
 
\item {\bf Serie Geométrica}

De ésta serie también sabemos que:
\[
s_{N}=\sum_{n=0}^{N}x^{n} = 1 +x +x^{2}+x^{3}+\cdots+x^{N} \,,
\]  
y si realizamos la resta $s_{N}-x s_{N}$, tenemos
 \begin{equation}
 \begin{array}{rllllll}
s_{N}= & 1 & +x & +x^{2} & +x^{3}  & +\cdots &+ x^{N} \\
xs_{N}= & x & + x^{2} & + x^{3} & +x^{4} &+ \cdots &+ x^{N+1}
 \end{array} \nonumber
\end{equation} 

Es inmediato comprobar que:
\[ 
(1-x)s_{N}=1-x^{N+1} \,\, \Rightarrow \,\,
s_{N}= \frac{1-x^{N+1}}{1-x}= \frac{1}{1-x}-\frac{x^{N+1}}{1-x}\,,
\]

Notemos que si $x\neq1$ podemos reescribir la serie anterior como:
\begin{equation}
\label{seriegeo}
\sum_{n=0}^{n}x^{n} = 1 +x +x^{2}+x^{3}+\cdots+x^{n}=
\frac{1}{1-x}-\frac{x^{n+1}}{1-x}
\end{equation}

Por lo tanto podemos ver que si $|x|<1$ tendremos que la suma será:
\begin{equation}
\label{seriegeo2}
\lim_{n \rightarrow \infty}\frac{x^{n+1}}{1-x}= 0  \,\, \Rightarrow \,\,
\lim_{n \rightarrow \infty} \sum_{n=0}^{n}x^{n} =
 \sum_{n=0}^{\infty}x^{n} =\frac{1}{1-x} \,.
\end{equation}
Por otro lado, la serie divergirá (u oscilará) si $|x| \geqslant 1$.

\item {\bf Series Aritmético-geométricas}

Estas series, un poco más exóticas y como su nombre lo sugiere son una combinación de las anteriores. 
  \[
 s_{N}= \sum_{n=0}^{N-1} (a+nd)x^{n}=
 a+(a+d)x+(a+2d)x^{2}+(a+3d)x^{3}+(a+4d)x^{4}+\cdots+  \left[a +(N-1)d \right]x^{N-1} \,.
  \] 
  
Utilizando la misma estrategia que aplicamos a las series  geométricas (se deja como ejercicio al lector) se llega a encontrar el valor, nada intuitiva,  de la suma $S$:
 \[
s_{N}=\frac{a -\left[ a + (N-1)d \right]x^{N}}{1-x} + 
\frac{ xd(1 -x^{N-1} )}{(1-x)^{2}}  \,. 
\] 

Otra vez,  si $|x|<1$,  entonces cuando $N \rightarrow \infty$ la serie converge a:
\[
S=\frac{a}{1-x} +\frac{xd}{(1-x)^{2}}\,.
\] 

\item {\bf Serie Armónica} 

Quizá no la conocíamos con este nombre (y menos por sus propiedades) pero  seguro nos la hemos tropezado. 
\[
\sum_{n=1}^{\infty} \frac{1}{n}=1+ \frac{1}{2} +\frac{1}{3} +\frac{1}{4} +\frac{1}{5} +\cdots\frac{1}{n} +\cdots 
\]

Esta serie infinita resulta ser engañosa, en apariencia parece converger, pero no es así. Además notemos lo siguiente:
\[
\sum_{n=1}^{20} \frac{1}{n} \approx 3,5977\,, \quad 
\sum_{n=1}^{220} \frac{1}{n} \approx 5,9731\,, \quad
\sum_{n=1}^{20220} \frac{1}{n} \approx 10,492\,, 
\]
la suma de los primeros $20$ términos es más grande que la suma de los siguientes !`$200$ términos! y da la impresión de que la serie crece muy lentamente hacia algún valor límite.

Si analizamos con más cuidado, veremos que hay sutilezas. Acomodemos los términos de la siguiente forma:
\[
\sum_{n=1}^{\infty} \frac{1}{n} = 1  + \underbrace{\frac{1}{2}}_{\sigma_{0}} 
+ \underbrace{ \left( \frac{1}{3} +\frac{1}{4} \right) }_{\sigma_{1}} 
+ \underbrace{\left( \frac{1}{5} +\frac{1}{6}  +\frac{1}{7}  +\frac{1}{8}   \right)}_{\sigma_{2}} 
+ \underbrace{\left( \frac{1}{9} +\frac{1}{10}  + \cdots  +\frac{1}{16}   \right)}_{\sigma_{3}} + \cdots 
\]
la expresión anterior puede ser reescrita como:
\[
\sum_{n=1}^{\infty} \frac{1}{n} =
1 
+  \underbrace{ \frac{1}{1+1} }_{\sigma_{0}} 
+  \underbrace{ \frac{1}{2+1} + \frac{1}{2+2}}_{\sigma_{1}} 
+  \underbrace{ \frac{1}{4+1} + \frac{1}{4+2} + \frac{1}{4+3} + \frac{1}{4+4}}_{\sigma_{2}}
\]
\[
+  \underbrace{ \frac{1}{8+1} + \frac{1}{8+2} + \cdots +\frac{1}{8+8} }_{\sigma_{3}} +\cdots
+ \sum_{j=1}^{2^{n}} \frac{1}{2^{n}+j} +\cdots
\]
con lo cual:
\[
\sigma_{0}=\frac{1}{2}; \quad 
\sigma_{1}=\frac{7}{12} > \frac{1}{2}; \quad 
\sigma_{2}= \frac{533}{840}> \frac{1}{2}; \quad 
\sigma_{3}=\frac{95549}{144144} >\frac{1}{2}; \cdots
\]
y claramente diverge ya que:
\[
1+ \sigma_{0} +\sigma_{1} +\sigma_{2} + \sigma_{3} +\cdots > 1 +\frac{1}{2}  +\frac{1}{2}  +\frac{1}{2}  +\frac{1}{2} +\cdots
\]
Esta prueba aparentemente se le debe a Nicole D'Oresme\footnote{\textbf{Nicole D'Oresme} (1323-1382) Matemático francés que inventó la geometría coordenada antes de Descartes. Sobre la serie armónica consulte: \url{http://mathworld.wolfram.com/HarmonicSeries.html}}. 

Ahora bien, notemos que para todo $n\in \mathds{N}$ resulta:

\begin{eqnarray*}
\int_{1}^n \frac{\mathrm{d}x}{x} = \ln(n) \,\, \Rightarrow \,\,
\ln(n) &=& \int_{1}^2 \frac{\mathrm{d}x}{x} + \int_{2}^3 \frac{\mathrm{d}x}{x} +\int_{3}^4 \frac{\mathrm{d}x}{x} +\cdots +
\int_{k}^{k+1}\frac{\mathrm{d}x}{x} =
\sum_{k=1}^{n-1}\ \int_{k}^{k+1}\frac{\mathrm{d}x}{x} \\
&\leq& \sum_{k=1}^{n-1}\ \int_{k}^{k+1}\frac{\mathrm{d}x}{k} <
1+\frac12+\frac13+\cdots +\frac{1}{n} \,.
\end{eqnarray*}

Es decir
\[
\lim_{n \rightarrow \infty} \left[1+\frac12+\frac13+\cdots +\frac{1}{n} \right] \geq \lim_{n \rightarrow \infty} \ln(n) = + \infty \,.
\]
Por lo tanto:
\[
\sum_{n=1}^{\infty} \frac{1}{n} = +\infty \,.
\]


Una de las generalizaciones de la serie armónica es la función Zeta de Riemann\footnote{\textbf{Georg Friedrich Bernhard Riemann} (1826 Hanover, Alemania - 1866 Selasca, Italia) Matemático alemán cuyas ideas sobre las geometría del espacio han tenido un profundo impacto en el desarrollo de la física teórica. Igualmente clarificó la noción de integral al introducir el concepto de lo que hoy se conoce como \textit{integral de Riemann}.  Más detalles en  \url{http://mathworld.wolfram.com/RiemannIntegral.html}.}
\[
 \zeta(z) = \sum_{n=1}^{\infty} \frac{1}{n^z} =
 \frac{1}{1^z}+\frac{1}{2^z}+\frac{1}{3^z}+\cdots 
 \,,\quad \Re(z)>1 \,.
 \]
 
Esta última expresión es también un ejemplo donde  a partir de una serie se define una función, en este caso una función analítica. Aquí $z$ puede ser un número complejo, $z=a+ib$, y la serie converge cuando su parte real es mayor o igual a 1. 

La función Zeta de Riemann viene también definida por:
\[
 \zeta(z) = \frac{1}{\Gamma(z)}\int_0^\infty 
 \frac{x^z}{x\left(e^x-1\right)} \mathrm{d}x \,, \,\, \mbox{ donde }\,\,
 \Gamma(z)=\int_0^\infty \frac{x^z e^{-x}}{x} \mathrm{d}x \,,
\]
$\Gamma(z)$ es la función gamma. 


\end{itemize}

Las mayoría de las series que hemos mencionado con anterioridad tienen la particularidad de que todos sus términos son positivos, es decir, para la serie $\sum  a_n$ se tiene que $a_n \geq 0$, y por lo tanto:
\[
s_n = s_{n-1} + a_n \geq s_{n-1}\,,
\]
de manera que las sumas parciales $s_n$ son una sucesión monótona creciente.


\subsection{Derivación de  series geométricas elementales}

Las series infinitas se encuentran entre las más poderosas herramientas que se introducen en un curso de cálculo elemental. Son un ejercicio bastante inteligente para la manipulación de limites y son una buena herramienta para el estudio de las ecuaciones diferenciales, en el desarrollo de métodos numéricos y para estimar el comportamiento de funciones. 

Consideremos la serie geométrica: 
\[
a+ az+ az^2 +az^3 +\cdots +az^n+ \cdots \] con $|z|<1$. Este es 
uno de los pocos ejemplos donde se puede encontrar el término de las sumas parciales  a través de una expresión sencilla. Esta serie se puede tomar como punto de partida para encontrar la suma de un gran número de series interesantes. Consideremos el caso $a=1$ y $z=x$, como en la ecuación (\ref{seriegeo2}).
\begin{equation}
\label{seriegeom}
1+ x+ x^2 +x^3 +\cdots +x^n+ \cdots = \frac{1}{1-x} \,,\,\,\,\, |x| < 1\,.
\end{equation}

Si cambiamos $x$ por $x^2$ en (\ref{seriegeom}) resulta:
\begin{equation}
\label{seriegeom2}
1+ x^2+ x^4 +x^6 +\cdots +x^{2n}+ \cdots = \frac{1}{1-x^2} \,,\,\,\,\, |x| < 1\,.
\end{equation}

Si se multiplica (\ref{seriegeom2}) por $x$ se obtiene:
\begin{equation}
\label{seriegeom3}
x+ x^3+ x^5 +x^7 +\cdots +x^{2n+1}+ \cdots = \frac{x}{1-x^2} \,,\,\,\,\, |x| < 1\,.
\end{equation}

Si cambiamos $x$ por $-x$ en (\ref{seriegeom}) resulta:
\begin{equation}
\label{seriegeom4}
1- x+ x^2 -x^3 +\cdots +(-1)^nx^{n}+ \cdots = \frac{1}{1+x} \,,\,\,\,\, |x| < 1\,.
\end{equation}

Si cambiamos $x$ por $x^2$ en (\ref{seriegeom4}) resulta:
\begin{equation}
\label{seriegeom5}
1- x^2+ x^4 -x^6 +\cdots +(-1)^nx^{2n}+ \cdots = \frac{1}{1+x^2} \,,\,\,\,\, |x| < 1\,.
\end{equation}

Si se multiplica (\ref{seriegeom5}) por $x$ se obtiene:
\begin{equation}
\label{seriegeom6}
x- x^3+ x^5 -x^7 +\cdots +(-1)^nx^{2n+1}+ \cdots = \frac{x}{1+x^2} \,,\,\,\,\, |x| < 1\,.
\end{equation}

Si cambiamos $x$ por $2x$ en (\ref{seriegeom2}) resulta:
\begin{equation}
\label{seriegeom7}
1+4x^2+16x^4+\cdots + 4^nx^{2n}+ \cdots=\frac{1}{1-4x^2} \,,\,\,\,\, |x| < \frac12 \,.
\end{equation}

Si se deriva (\ref{seriegeom}) entonces:
\begin{equation}
\label{seriegeom8}
1+2x+3x^2+\cdots + nx^{n-1}+ \cdots=\frac{1}{(1-x)^2} \,,\,\,\,\, |x| < 1 \,.
\end{equation}

Si se integra (\ref{seriegeom4}):
\begin{equation}
\label{seriegeom9}
x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots + \frac{(-1)^nx^{n+1}}{n+1}+ \cdots=\ln(1+x) \,,\,\,\,\, |x| < 1 \,.
\end{equation}

Si se integra (\ref{seriegeom5}) ahora resulta:
\begin{equation}
\label{seriegeom10}
x-\frac{x^3}{3}+\frac{x^5}{5}-\frac{x^7}{7}+\cdots + \frac{(-1)^nx^{2n+1}}{2n+1}+ \cdots=\arctan(x) \,,\,\,\,\, |x| < 1 \,.
\end{equation}

Siguiendo a Laplace, quien dijo que había que leer a Euler: ``{\it Read Euler, read Euler. He is the master of us all} ",  podemos hacer lo siguiente con  la serie (\ref{seriegeom9}):
\begin{equation}
\label{seriegeom9a}
\ln(1+x) = x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots  
\end{equation}

Es bueno acotar que Euler utilizó esta expresión para construir 
¡tablas de logaritmos! 

Ahora hagamos lo siguiente, cambiemos $x$ por $-x$ en la ecuación anterior:
\begin{equation}
\label{seriegeom9b}
\ln(1-x) = -x-\frac{x^2}{2}-\frac{x^3}{3}-\frac{x^4}{4}-\cdots 
\end{equation}
restando (\ref{seriegeom9a}) menos (\ref{seriegeom9b}):
\begin{eqnarray*}
\label{seriegeom9c}
\ln(1+x)-\ln(1-x) &=& \left[x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots \right]  - 
\left[ -x-\frac{x^2}{2}-\frac{x^3}{3}-\frac{x^4}{4}-\cdots \right] \\
&=& 2x +\frac{2x^3}{3}+\frac{2x^5}{5}+\cdots
\end{eqnarray*}

Es decir:
\[
\label{seriegeom9d}
\ln \left( \frac{1+x}{1-x} \right)= 2x +\frac{2x^3}{3}+\frac{2x^5}{5}+\cdots
\]

Para valores pequeños de $x$, digamos $x=1/3$ resulta:
\[
\ln \left( \frac{1+\frac{1}{3}}{1-\frac{1}{3} } \right)=\ln \left( 2\right) = 
2\left(\frac{1}{3}\right) +\frac{2\left({\frac{1}{3}}\right)^3}{3}+\frac{2\left({\frac{1}{3}}\right)^5}{5}+\cdots
\approx 0.6930041152
\]

Euler notó la siguiente conexión entre logaritmos y las series armónicas. Cambiando $x$ por $1/n$ en (\ref{seriegeom9a}) se obtiene
\[
\label{seriearmo}
\ln \left(1+\frac{1}{n}\right) = \frac{1}{n}-\frac{1}{2n^2}+\frac{1}{3n^3}-\frac{1}{4n^4}+\cdots  
\]
por lo tanto:
\[
\label{seriearmo1}
 \frac{1}{n} = \ln \left(1+\frac{1}{n}\right) +\frac{1}{2n^2}-\frac{1}{3n^3}+\frac{1}{4n^4}-\cdots  
\]
y para valores de $n$ muy grandes, se cumple que:
\[
 \frac{1}{n} = \ln \left(\frac{n+1}{n}\right) 
\]

Ahora bien, para diferentes valores de $n$  se obtienen las siguientes relaciones
\begin{eqnarray*}
n=1 \,\, \Rightarrow \,\,1 &=& \ln \left(2\right) +\frac{1}{2}-\frac{1}{3}+\frac{1}{4}-\cdots \\
n=2 \,\, \Rightarrow \,\, \frac{1}{2} &=&  \ln \left(\frac{3}{2}\right) +\frac{1}{8}-\frac{1}{24}+\frac{1}{64}-\cdots \\
n=3 \,\, \Rightarrow \,\, \frac{1}{3} &=&  \ln \left(\frac{4}{3}\right) +\frac{1}{18}-\frac{1}{81}+\frac{1}{324}-\cdots \\
  \vdots & & \qquad  \vdots   \qquad \quad \vdots  \qquad  \vdots   \qquad \vdots \\
 \frac{1}{n} &=& \ln \left(\frac{n+1}{n}\right) +\frac{1}{2n^2}-\frac{1}{3n^3}+\frac{1}{4n^4}-\cdots  
\end{eqnarray*}

Si sumamos  columna por columna  resulta:
\begin{eqnarray*}
\sum_{k=1}^{n} \frac{1}{k} &=& \ln \left(2\right) +\ln \left(\frac{3}{2}\right)+ \ln \left(\frac{4}{3}\right) + \cdots +
 \ln \left(\frac{n+1}{n}\right)\\
 &+& \frac{1}{2}\left[  1 +  \frac{1}{4} + \frac{1}{9} + \cdots +  \frac{1}{n^2} \right] 
 - \frac{1}{3}\left[  1 +  \frac{1}{8} + \frac{1}{27} + \cdots +  \frac{1}{n^3} \right] \\
  &+& \frac{1}{4}\left[  1 +  \frac{1}{16} + \frac{1}{81} + \cdots +  \frac{1}{n^4} \right]   - \cdots
\end{eqnarray*}

Es fácil ver que la suma de los logaritmos de la primera línea se puede escribir  como el logaritmo de sus productos, es decir, 
\[
\ln \left(2\right) +\ln \left(\frac{3}{2}\right)+ \ln \left(\frac{4}{3}\right) + \cdots + \ln \left(\frac{n+1}{n}\right) = 
\ln \left[ 2 \left(\frac{3}{2}\right)  \left(\frac{4}{3}\right)\cdots   \frac{n+1}{n} \right]  = \ln \left(n+1\right) \,.
\]

De manera que, y siguiendo a Euler que llevó a cabo un estimado para los términos sobrantes, resulta lo siguiente:
\[
\sum_{k=1}^{n} \frac{1}{k}  \approx  \ln \left(n+1\right) + 0.577218 \,.
\]

Para valores de $n$ muy grandes la suma de las serie armónica es igual a un logaritmo más una constante. Esta
constante es actualmente llamada la Constante de Euler y denotada por la letra $\gamma$:
\begin{equation}
\gamma = \lim_{n \rightarrow \infty} \left[ \sum_{k=1}^{n} \frac{1}{k}  -  \ln \left(n+1\right) \right] \,.
\end{equation}

La constante de Euler  juega un papel central en otras ramas de las matemáticas, por ejemplo, en el análisis real aparece como:
\[
\gamma = - \int_{0}^{\infty} e^{-x}  \ln \left(x\right) \textrm{d}x \,.
\]

\subsection{El método de la diferencia}

A veces para una serie finita, $\{s_{N}\}$, uno encuentra que para el término $n$-ésimo se tiene que: 
\[
a_{n}= f(n) -f(n-1) \,.
\] 
para alguna función $f(n)$. 

En ese caso es inmediato demostrar lo siguiente:
\begin{equation}
s_{N}=\sum_{n=1}^{N}a_{n}=\sum_{n=1}^{N} f(n) - \sum_{n=1}^{N} f(n-1)= 
f(N) -f(0) \,.
\label{sumadiferencia}
\end{equation} 
Por ejemplo, si tenemos la serie:
\[
\sum_{n=1}^{N} \frac{1}{n(n+1)}\,,
\] 
se puede ver que:	
\[
a_{n}=\frac{1}{n(n+1)}  = - \frac{1}{n+1} +\frac{1}{n}  
\,\, \Rightarrow \,\,  f(n)= -\frac{1}{n+1} \,,
\] 	
por lo tanto, la suma se podrá expresar como: 
\[
s_{N}= f(N)-f(0)= -\frac{1}{N + 1} +1=\frac{N}{N + 1} \,.
\]

Se puede ir más allá si identificamos que el término $n$-ésimo tiene la forma: 
\[
a_{n}= f(n) -f(n-m)\,,
\] 
por lo tanto,  la suma de la serie se puede escribir como:
\begin{equation}
s_{N}=\sum_{n=1}^{N}a_{n}=\sum_{n=1}^{N} f(n) - \sum_{n=1}^{N} f(n-m) \,.
\label{sumadiferencia2}
\end{equation} 

Hay que hacer notar que el argumento $n-m$ puede ser positivo o negativo. Con lo cual el método de la diferencia resulta versátil y muy útil cuando se requiere encontrar la suma de series de variada dificultad. Podemos ver que:
\begin{itemize}
\item Si $m=1$
\[
s_{N}=\sum_{n=1}^{N}a_{n}=  f(N) -f(0) \,.
\]
como la ecuación (\ref{sumadiferencia}).
\item Si $m=2$
\[
s_{N}=\sum_{n=1}^{N}a_{n}=  f(N) +f(N-1)-f(0)-f(-1) \,.
\]
\item Si $m=3$
\[
s_{N}=\sum_{n=1}^{N}a_{n}=  f(N) +f(N-1)+f(N-2) -f(0)-f(-1)-f(-2) \,.
\]
\end{itemize}

Consideremos la siguiente serie y su expansión en fracciones simples
 \[
\sum_{n=1}^{N} \frac{1}{n(n+2)} \,,
  \] 
se tiene que el término $n$-ésimo es:
\[
 a_{n}=\frac{1}{n(n+2)}  = - \frac{1}{2(n+2)} +\frac{1}{2n} 
\,\, \Rightarrow \,\, f(n)= -\frac{1}{2(n+2)} \,,
 \]  
 de manera que:
 \[
 a_n=f(n)-f(n-2)=- \frac{1}{2(n+2)} -\left(-\frac{1}{2n} \right)\,,
 \]
de forma y manera que, como $m=2$, resulta:
\[
s_{N}= f(N) + f(N-1) - f(0) - f(-1) = 
\frac{3}{4} - \frac{1}{2} \left( \frac{1}{N + 2} + \frac{1}{N+1} \right)= 
{{N\,\left(3\,N+5\right)}\over{4\,\left(N+1\right)\,\left(N+2\right)}} \,.
\]

Ahora bien, con alguna frecuencia surgen las series de números naturales. La más simple es: 
\[
s_{N}= 1+2+3+\cdots+N=\sum_{n=1}^{N}n=\frac{N(N+1)}{2} \,,
\]
es decir, una serie aritmética de razón $d=1$.

Más interesante puede ser la serie de cuadrados de números enteros:
\[
s_{N}= 1+2^{2}+3^{2}+\cdots+N^{2}=\sum_{n=1}^{N}n^{2}= \frac{N(N+1)(2N+1)}{6} \,.
\]

Este resultado, nada intuitivo, surge de la aplicación ingeniosa del método de la diferencia. Tal y como hemos dicho, se trata de encontrar que el elemento genérico de la serie sea: $a_{n}= f(n) -f(n-1)=n^{2}$ para alguna función. 

Suponga una función del tipo
\[
f(n)=n(n+1)(2n+1) \,\, \Rightarrow \,\, f(n-1)=(n-1)n(2n-1)\,,
\]
entonces:
\[
f(n) -f(n-1)= n(n+1)(2n+1) -  (n-1)n(2n-1) = 6n^{2}\,,
\]
con lo cual:
\[
a_{n}= 6 n^{2}\,\, \Rightarrow \,\,
s_{N} =\sum_{n=1}^{N}a_{n}=\frac{f(N) -f(0)}{6} = \frac{N(N+1)(2N+1)}{6}\,.
\]

\subsubsection{Sumando por analogía}

Como siempre, intentaremos proceder por analogía. La intención es expresar una serie complicada como sumas de series conocidas. Considere el siguiente ejemplo:
\[
s_{N} = \sum_{n=1}^{N}(n+1)(n+3) = \sum_{n=1}^{N}\left(n^2+4n+3\right)=\sum_{n=1}^{N}n^2 +\sum_{n=1}^{N}4n +\sum_{n=1}^{N}3 \,,
\] 
con lo cual:
\[
s_{N}= \frac{N(N+1)(2N+1)}{6}  + \frac{N(N+1)}{2}  + 3N  = \frac{N(2N^{2} +15N +31)}{6}\,.
\]

\subsection{Algebra elemental de series}

\label{AlgebraElementalSeries}
Las series se suman, se igualan y se multiplican. Para ello es importante que tengamos cuidado con los índices y sus valores. Consideremos un par de series infinitas:
\[
u=\sum_{n=0}^{\infty}a_{n} \quad  \text{y} \quad v=\sum_{n=0}^{\infty}b_{n} \,,
\]
con lo cual la suma de esas series será:
\[
u + v = \sum_{n=0}^{\infty}a_{n} + \sum_{n=0}^{\infty}b_{n} = \sum_{n=0}^{\infty}\left(  a_{n}+b_{n}\right) \,. 
\] 
Los índices son mudos y se acomodan para ser sumados. 

Para sumar series es imperioso que los índices de cada serie comiencen con el mismo valor, esto es:
\[
\sum_{n=0}^{\infty}a_{n} + \sum_{j=1}^{\infty}b_{j} = \sum_{n=1}^{\infty}\left(  a_{n-1}+b_{n}\right) = a_{0} + \sum_{n=1}^{\infty}\left(  a_{n}+b_{n}\right) \,.
\]
Nótese que hemos hecho los siguientes cambios: $j\rightarrow n$ y  $n\rightarrow n-1$.

Algo parecido ocurre cuando las series se igualan:
\[
\sum_{n=0}^{\infty}b_{n}  =\sum_{n=1}^{\infty}na_{n} 
\,\, \Rightarrow \,\,
\sum_{n=0}^{\infty}b_{n} = \sum_{k=0}^{\infty} (k+1)a_{k+1}
\,\, \Longleftrightarrow \,\,
\sum_{n=0}^{\infty}\left[ (n+1)a_{n+1}-b_{n}\right] =0 \,.
\]

Para finalizar se puede comprobar que las series  también se pueden multiplicar:
\[
u\, v =  \sum_{n=0}^{\infty}a_{n}  \, \sum_{n=0}^{\infty}b_{n}   = \sum_{n=0}^{\infty}c_{n} \,,
\]
donde:
\[
c_{n}=a_{0}b_{n}+a_{1}b_{n-1}+\cdots+a_{j}b_{n-j}+\cdots
+a_{n-2}b_{2}+a_{n-1}b_{1}+a_{n}b_{0}\,.
\]

Cuando las sucesiones comprenden sumas y productos de otras sucesiones, es decir, sí 
$u_k$ y $v_k$ son dos sucesiones con $u_k \rightarrow U$ y  $v_k \rightarrow V$ cuando
$k \rightarrow \infty$, entonces se cumple que:
\begin{enumerate}
\item si $a$ y $b$ son números independientes de $k$, entonces $au_k + bv_k \rightarrow aU+bV$ cuando $k \rightarrow \infty$.
\item $u_k v_k \rightarrow U V$ para $k \rightarrow \infty$.
\item si $V\neq 0$ entonces $u_k/v_k \rightarrow U/V$ a medida que  $k \rightarrow \infty$.
\item si $u_k< v_k$ $\forall$ $k>N$ entonces $U \leq V$ cuando $k \rightarrow \infty$.
\end{enumerate}

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema}: Si la serie  $\sum_{n=1}^{\infty} u_{n} $ converge, entonces cualquiera de sus restos converge. Si cualquier resto de la serie $\sum_{n=1}^{\infty} u_{n} $ converge, entonces la propia serie también converge, y si  además:
\[
S= \sum_{n=1}^{\infty} u_n \,,\quad  s_i= \sum_{n=1}^{\infty} u_n\,, 
\quad  r_i= \sum_{n=i+1}^{\infty} u_n\,.
\]
Entonces:
\[
S=s_i+r_i \,.
\]
\end{mdframed}

Se tiene entonces que es posible agregar o quitar un número finito de términos a la serie dada y esta operación no influirá sobre su convergencia. También se desprende del teorema anterior que si la serie converge entonces su resto tiende a cero:
\[
\lim_{i \rightarrow \infty} r_i = 
\lim_{i \rightarrow \infty} \left(S-s_i \right) =0 \,.
\]


\subsection{Series telescópicas}

Una propiedad importante de las series finitas es la propiedad denominada telescópica:
\begin{equation}
\sum_{k=1}^{n}\left(a_k - a_{k+1} \right)= a_1 - a_{n+1}\,,
\end{equation}
para el caso de series infinitas, se consideran aquellas series $\sum u_n$ donde cada término
se puede expresar como una diferencia de la forma:
\[
u_n= a_n - a_{n+1}\,.
\]

Las series telescópica son series cuyas sumas parciales se van cancelando de manera tal que al final resulta un número fijo de términos:
\[(a_{1}-a_{2})+(a_{2}-a_{3})+(a_{3}-a_{4})+\cdots +(a_{n}-a_{n+1})=a_1-a_{n+1} \,.
\]

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema}: Sean $\{u_n \}$ y $\{a_n \}$ dos sucesiones de números reales o complejos tales que:
\[
u_n= a_n - a_{n+1} \quad \mbox{para} \quad n=1,2,3, . . . \,.
\]
Entonces la serie $\sum u_n$ converge si y sólo si la sucesión $\{a_n \}$ converge, en cuyo caso se tiene:
\[
\sum_{n=1}^{\infty} u_n= a_1 - L \quad \mbox{donde} \quad L= \lim_{n \rightarrow \infty} a_n\,.
\]
\end{mdframed}

Consideremos la siguiente serie:
\[
\sum_{n=1}^{\infty} \frac{1}{n^2+n}\,.
\]
Podemos demostrar que:
\[
u_n= \frac{1}{n^2+n}=\frac1n -\frac{1}{n+1} \,,
\]
Es fácil verificar que:
\[
\sum_{n=1}^{N} \left[\frac1n -\frac{1}{n+1}\right]=
\left[1-\frac12\right] + \left[\frac12-\frac13\right]+
\left[\frac13-\frac14\right] + \left[\frac14-\frac15\right]+ \cdots +
\left[\frac{1}{N}-\frac{1}{N+1}\right] = 1-  \frac{1}{N+1} \,.
\]

Pero si aplicamos el teorema anterior, tenemos entonces que: $a_n=1/n$ , $a_1=1$ y además ya vimos que la sucesión $a_n=1/n$ converge:
\[
L= \lim_{n \rightarrow \infty} a_n =  
\lim_{n \rightarrow \infty} \frac1n = 0\,.
\]

Por lo tanto:
\[
\sum_{n=1}^{\infty} \frac{1}{n^2+n} = 1\,.
\]

Las series pueden llegar a tener comportamientos extraños, como se ve con la siguiente serie armónica alternada:
\[
\sum_{n=1}^{\infty} \frac{(-1)^{(n-1)}}{n}= 1-\frac12 + \frac13 - \frac14 + \frac15 - \frac16 +\cdots \,, 
\]

Recordemos  que anteriormente estudiamos la serie geométrica (\ref{seriegeo}), la cual volveremos a escribir pero está vez intercambiando $x\rightarrow -x$, de manera que nos queda de la siguiente forma:
\begin{equation}
\label{seriegeom4b}
1- x+ x^2 -x^3 +\cdots +(-1)^nx^{n}+ (-1)^{n+1}\frac{x^{n+1}}{1+x} = \frac{1}{1+x} \,.
\end{equation}
Ecuación que es válida para todo $n \in \mathds{N}$ y $x\neq 1$.

Integrando (\ref{seriegeom4b}) entre $0$ y $1$ se obtiene:
\begin{eqnarray*}
1- \frac12+ \frac13 -\frac14 +\cdots +(-1)^n\frac{1}{n+1}+ (-1)^{n+1}\int_{0}^{1}\frac{x^{n+1}}{1+x}\mathrm{d}x &=& \ln(2) \\
\sum_{k=1}^{n+1} \frac{(-1)^{(k-1)}}{k}+ 
(-1)^{n+1}\int_{0}^{1}\frac{x^{n+1}}{1+x}\mathrm{d}x  &=& \ln(2) \,.
\end{eqnarray*}

Por lo tanto:
\[
\ln(2)-\sum_{k=1}^{n+1} \frac{(-1)^{(k-1)}}{k}= 
(-1)^{n+1}\int_{0}^{1}\frac{x^{n+1}}{1+x}\mathrm{d}x \,.
\]

Se puede ver también que:
\[
\int_{0}^{1}\frac{x^{n+1}}{1+x}\mathrm{d}x \leq 
\int_{0}^{1}{x^{n+1}}\mathrm{d}x = \frac{1}{n+2}
\,\, \Rightarrow \,\, 
\ln(2)-\sum_{k=1}^{n+1} \frac{(-1)^{(k-1)}}{k}=\frac{1}{n+2}
\]
Entonces:
\[
\lim_{n \rightarrow \infty}
\left[ \ln(2)-\sum_{k=1}^{n+1} \frac{(-1)^{(k-1)}}{k} \right]=0
 \,\, \Rightarrow \,\, 
\ln(2)=\sum_{n=1}^{\infty} \frac{(-1)^{(n-1)}}{n}\,.
\]

La suma tienen el valor de $S=\ln(2)$. 


Se puede mostrar lo que sucede si se arreglan los  términos de la manera siguiente:
\[
S= 1+\left(\frac13 - \frac12 + \frac15 \right) + \left(\frac17 - \frac14 + \frac19 \right)  
+ \left(\frac{1}{11} - \frac16 + \frac{1}{13} \right) + \cdots \,,
\]
la cual contiene exactamente los mismos términos, aunque en orden diferente, pero ahora la serie converge a $S=\frac32 \ln(2)$. Esta aparente contradicción surge cuando  pasamos por alto el hecho de que al sumar los términos de una serie no se hace la suma con los infinitos términos, lo que hacemos es calcular un límite de una sucesión de términos que se obtienen sumando de manera consecutiva los términos de otra sucesión dada, es decir,  lo que se hacemos es aplicar un proceso de límite. 
Para que al sumar no importe el orden de los sumandos tendríamos que sumar los infinitos términos de la sucesión. 

A pesar de que  las series pueden presentar estos comportamientos extraños, las series  resultan muy buenas representaciones aproximadas de funciones, por ejemplo:
\begin{equation}
e^x= 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \cdots = 
\sum_{n=0}^{\infty} \frac{x^n}{n!}\,.
\end{equation}

La suma directa de una serie infinita no es un método práctico para estudiar su convergencia, por ejemplo, la serie
\[
\sum_{k=2}^{\infty} \frac{\ln(k)}{k^2} \,,
\]
converge al valor $0,937548. . . $, pero para obtener estos primeros cinco decimales se tendría que sumar unos $10^7$ términos! 



Es necesario entonces desarrollar algunos criterios que nos permitan saber si una serie puede llegar a converger o no.

\subsection{{\color{Fuchsia}Ejemplos}}

\begin{enumerate}
\item Dada la sucesión $\{u_n\}$ de números reales, se llama sucesión de Cauchy o sucesión fundamental, en el caso de que satisfaga el requisito siguiente: dado un número real $r$ positivo se pueda conseguir dos enteros positivos $p$ y  $q$ tal que de $p > n_0$ y $q > n_0$ se deduzca que $|c_p - c_q| < r$?.

En los números reales toda sucesión de Cauchy converge a algún límite. Esta particularidad implica un resultado importante en el análisis real que es la caracterización de Cauchy para la convergencia de sucesiones:

Una sucesión de números reales es convergente (en los reales) si y solo si es de Cauchy.

\item Demuestre que
\[
\sum_{n=1}^{\infty}\frac{1}{2^n}= 1
\]
Se tiene que
\[
s_n=\frac12+\frac{1}{2^2}+\frac{1}{2^3}+\cdots + \frac{1}{2^n}\,,
\]
y que al multiplicar por $\frac12$ se obtiene
\[
\frac12s_n=\frac{1}{2^2}+\frac{1}{2^3}+\frac{1}{2^4}+\cdots + \frac{1}{2^{n+1}}\,,
\]
restando
\[
\left[1-\frac12\right] s_n=\frac{1}{2}-\frac{1}{2^{n+1}} = 1- \frac{1}{2^n}\,.
\]
por lo tanto
\[
\lim_{n\rightarrow \infty} \left[1- \frac{1}{2^n}\right] =1\,,
\]
la serie converge.


\end{enumerate}

\subsection{{\color{red}Practicando con Maxima}} 

{\bf Maxima} puede calcular fácilmente sumas numéricas finitas, pero cuando le pedimos calcular sumas simbólicas infinitas podrá calcularlas solo en casos particulares.  Es útil al trabajar con series utilizar la librería  {\bf simplify\_sum}

Veamos algunos ejemplos sencillos de sumas

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(simplify_sum)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sum(n^2, n, 1, N);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\sum_{n=1}^{N}{n^2}
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sum(1/n^2, n, 1, inf);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\sum_{n=1}^{\infty }{{{1}\over{n^2}}}
\end{math}
\newline

La función {\bf sum} nos deja indicada la suma porque no hemos especificado el rango para los valores de la suma. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sum(n^2, n, 1, 20);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
2870
\end{math}
\newline

También podemos utilizar  {\bf float} o {\bf numer} para pedirle al programa que nos escriba el valor numérico

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sum(1/n^2, n, 1, 1000),float;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
1.643934566681561
\end{math}
\newline

Con las funciones {\bf simpsum} o  {\bf simplify\_sum} es posible que el programa realice la suma simbólica. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sum(n^2, n, 1, N),simpsum;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
{{2\,N^3+3\,N^2+N}\over{6}}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
simplify_sum(sum(n^2, n, 1, N));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
{{2\,N^3+3\,N^2+N}\over{6}}
\end{math}
\newline

Con este último comando podemos escribir la expresión de la sumatoria de manera más elegante.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sum(n^2, n, 1, N)=simplify_sum(sum(n^2, n, 1, N));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\sum_{n=1}^{N}{n^2}={{2\,N^3+3\,N^2+N}\over{6}}
\end{math}
\newline

Consideremos las siguientes series infinitas:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sum(x^n/n!, n, 0, inf)=simplify_sum(sum(x^n/n!, n, 0, inf));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\sum_{n=0}^{\infty }{{{x^{n}}\over{n!}}}=e^{x}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sum(1/n^2, n, 1, inf)=simplify_sum(sum(1/n^2, n, 1, inf));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\sum_{n=1}^{\infty }{{{1}\over{n^2}}}={{\pi^2}\over{6}}
\end{math}
\newline

Podemos ahorrarnos el estar escribiendo el término enésimo varias veces si definimos la función $f=f(n)$ en primer lugar. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f:(-1)^(n-1)/n$ sum(f, n, 1,inf)=simplify_sum(sum(f, n, 1,inf));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\sum_{n=1}^{\infty }{{{\left(-1\right)^{n-1}}\over{n}}}=\log (2)
\end{math}
\newline

En el siguiente ejemplo el programa no podrá encontrar la serie de manera simbólica, pero como comentamos anteriormente, podemos evaluar la serie para algunos valores de $N$. En este caso, para $N>$10.000 el consumo en tiempo de cálculo para la computadora comienza a notarse.  

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f:log(n)/n^2;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
{{\log (n)}\over{n^2}}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sum(f, n, 1,inf)=simplify_sum(sum(f, n, 1,inf));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
\sum_{n=1}^{\infty }{{{\log(n)}\over{n^2}}}=\sum_{n=1}^{\infty }{{{
 \log(n)}\over{n^2}}}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sum(f,n,2,10),numer;sum(f,n,2,100),numer;sum(f,n,2,1000),numer;sum(f,n,2,10000),numer;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
0.6185026440390787
\end{math}

\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
0.8817261267819703
\end{math}

\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
0.9296439518465429
\end{math}

\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
0.9365272663288963
\end{math}
\newline

Este último ejercicio puede ser resuelto de una manera más eficiente si expresamos la función de variable discreta como una sucesión utilizando la opción de crear una lista, {\bf makelist}, para evaluarla. 

Primero escribimos la función discreta, note que usamos $:=$

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i18) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
g[n]:=log([n])/[n]^2;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
g_n:=\frac{\log([n])}{[n]^2}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i19) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
makelist(sum(g[n],n,2,N),N,[10,100,1000,10000]),numer;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o19) }
[[0.6185026440390787],[0.8817261267819703],[0.9296439518465429],[0.9365272663288963]]
\end{math}

Por lo tanto, hacer cálculos con sucesiones es sencillo, veamos algunos ejemplo de sucesiones. 

Definimos la siguiente sucesión, la de Fibonacci :

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i20) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
F[1]:1$ F[2]:1$ F[n]:=F[n-1]+F[n-2];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
F_n:=F[n-1]+F[n-2]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
makelist(F[n],n,1,15);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
[1,1,2,3,5,8,13,21,34,55,89,144,233,377,610]
\end{math}
\newline

{\bf Maxima} ya contiene la sucesión de Fibonacci $\tt{fib(n)}$ en sus librerías de funciones propias. En este caso aplicaremos la función {\bf map} para evaluar la sucesión en los diferentes valores de su argumento $n$

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
fib(8);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
21
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
map (fib, [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
[1,1,2,3,5,8,13,21,34,55]
\end{math}
\newline

Definamos ahora otra sucesión

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
G[n]:=(-1)^n*(1+1/n);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
G_n:=(-1)^n \left(1+\frac{1}{n}\right)
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
makelist(G[n],n,1,15);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\left[ -2 , {{3}\over{2}} , -{{4}\over{3}} , {{5}\over{4}} , -{{6
 }\over{5}} , {{7}\over{6}} , -{{8}\over{7}} , {{9}\over{8}} , -{{10
 }\over{9}} , {{11}\over{10}} , -{{12}\over{11}} , {{13}\over{12}} , 
 -{{14}\over{13}} , {{15}\over{14}} , -{{16}\over{15}} \right] 
\end{math}
\newline

Podemos hacer operaciones básicas con sucesiones, como sumarlas

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
S[n]:=F[n]+G[n]; makelist(S[n],n,1,15);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
S_n:=F_n+G_n
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\left[ -1 , {{5}\over{2}} , {{2}\over{3}} , {{17}\over{4}} , {{19
 }\over{5}} , {{55}\over{6}} , {{83}\over{7}} , {{177}\over{8}} , {{
 296}\over{9}} , {{561}\over{10}} , {{967}\over{11}} , {{1741}\over{
 12}} , {{3015}\over{13}} , {{5293}\over{14}} , {{9134}\over{15}}
  \right] 
\end{math}
\newline

Operaciones combinadas

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
C[n]:=3*F[n]-n*G[n]; makelist(C[n],n,1,15);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
C_n:=3 F_n-n \ G_n
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
\left[ 5 , 0 , 10 , 4 , 21 , 17 , 47 , 54 , 112 , 154 , 279 , 419
  , 713 , 1116 , 1846 \right] 
\end{math}
\newline

Multiplicación

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M[n]:=F[n]*G[n]; makelist(M[n],n,1,15);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
M_n:=F_n \ G_n
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
\left[ -2 , {{3}\over{2}} , -{{8}\over{3}} , {{15}\over{4}} , -6 , 
 {{28}\over{3}} , -{{104}\over{7}} , {{189}\over{8}} , -{{340}\over{9
 }} , {{121}\over{2}} , -{{1068}\over{11}} , 156 , -{{3262}\over{13}}
  , {{5655}\over{14}} , -{{1952}\over{3}} \right]
\end{math}
\newline

División

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
D[n]:=F[n]/G[n]; makelist(M[n],n,1,15);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
D_n:=\frac{F_n}{G_n}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
\left[ -{{1}\over{2}} , {{2}\over{3}} , -{{3}\over{2}} , {{12
 }\over{5}} , -{{25}\over{6}} , {{48}\over{7}} , -{{91}\over{8}} , {{
 56}\over{3}} , -{{153}\over{5}} , 50 , -{{979}\over{12}} , {{1728
 }\over{13}} , -{{3029}\over{14}} , {{5278}\over{15}} , -{{4575
 }\over{8}} \right] 
\end{math}
\newline

Consideremos la siguiente sucesión:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f[n]:=(n+1)/(2*n^2);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
f_n:=\frac{n+1}{2 n^2}
\end{math}
\newline

Para algún valor de $n$ en particular:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i18) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f[2];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
\frac{3}{8}
\end{math}
\newline

Para un conjunto de valores:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i19) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
makelist(f[n],n,1,15);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o19) }
\left[ 1 , {{3}\over{8}} , {{2}\over{9}} , {{5}\over{32}} , {{3
 }\over{25}} , {{7}\over{72}} , {{4}\over{49}} , {{9}\over{128}} , {{
 5}\over{81}} , {{11}\over{200}} , {{6}\over{121}} , {{13}\over{288}}
  , {{7}\over{169}} , {{15}\over{392}} , {{8}\over{225}} \right] 
\end{math}
\newline

Si queremos graficar la sucesión hacemos lo siguiente

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i20) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
N[n]:=n$ LisN:makelist(N[n],n,1,50)$ Listf:makelist(f[n],n,1,50)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i21) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
wxplot2d([discrete,LisN,Listf],[style,[points,2,2,1]],[xlabel,"n"],[ylabel,"f(n)"]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o21) }
\end{math}
\begin{figure}[h]\nonumber
\begin{center}
\includegraphics[height=2.6in,width=4.5in]{VOLUMEN_2/02_Series/figuras/figseries01}
\end{center}
\end{figure}
\newline

Otro ejemplo de una sucesión que converge es el siguiente

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i22) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
g[n]:=1+(-1)^n/(n);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o22) }
g_n:=1+\frac{(-1)^n}{n};
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i23) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Listg:makelist(g[n],n,1,50)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i24) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
wxplot2d([discrete,LisN,Listg],[style,[points,2,2,4]],[xlabel,"n"],[ylabel,"g(n)"]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o24) }
\end{math}
\begin{figure}[h]\nonumber
\begin{center}
\includegraphics[height=2.6in,width=4.5in]{VOLUMEN_2/02_Series/figuras/figseries02}
\end{center}
\end{figure}

\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Encuentre la suma de los 100 primeros enteros.
\item Encuentre la distancia total que recorre una pelota que rebota verticalmente y que en cada rebote 
pierde 2/3 de su energía cinética.
\item Encuentre la suma de la serie $S=2+\frac{5}{2} + \frac{8}{4} +  \frac{11}{8} +\cdots $.
\item Demuestre que
\begin{enumerate}
\item
\[
\sum_{n=1}^{\infty}\frac{1}{n(n+1)}= 1
\]
\item
\[
\sum_{n=1}^{\infty}\frac{1}{(2n-1)(2n+1)}=\frac12
\]
\end{enumerate}
\item Determine el limite de las siguientes sucesiones cuando $n \rightarrow \infty$.
\begin{enumerate}
\item $u_n=\frac{n}{n+1}$   
\item $u_n=\frac{1}{1+n^2}$  
\item $u_n=\frac{n^2}{1+n}$  
\item $u_n=\frac{(an+b)^2}{cn^2+d}$
\end{enumerate}

\item Muestre que $ s_{N}= 1+2^{3}+3^{3}+\cdots+N^{3}=\sum_{n=1}^{N}n^{3} = \left( \sum_{n=1}^{N} n \right)^{2} =\frac{N^{2}(N+1)^{2}}{4}$.
\item Demuestre que para $|x|<1$
\begin{enumerate}
\item $\sum_{n=1}^{\infty}n x^n=\frac{x}{(1-x)^2}$   
\item $\sum_{n=1}^{\infty}n^2 x^n=\frac{x^2+x}{(1-x)^3}$  
\item $\sum_{n=1}^{\infty}n^3 x^n=\frac{x^3+4x^2+x}{(1-x)^4}$  
\item $\sum_{n=1}^{\infty}n^4 x^n=\frac{x^4+11x^3+11x^2+x}{(1-x)^5}$
\end{enumerate}

\item Demuestre que:
\[
 \lim_{n \rightarrow \infty} \left[ \sum_{k=1}^{n} \frac{1}{k}  -  \ln \left(n+1\right) \right] 
 =  \lim_{n \rightarrow \infty} \left[ \sum_{k=1}^{n} \frac{1}{k}  -  \ln \left(n\right) \right] \,.
\]


\end{enumerate}

\section{Criterios de convergencia }

La pregunta que nos planteamos es la siguiente: Si hacemos que $N \rightarrow \infty$ entonces ¿la suma $\sum _{k=1}^{N} a_{k}$, tiene un límite?  A pesar de que sólo  podremos calcular la suma de algunas series, existen algunas formas de averiguarlo. Es decir, en la mayoría de los casos nos será imposible y nos tendremos que conformar con saber si convergen o no, o peor aún, si una suma parcial converge sin poder calcular el valor de esa suma. 

Los términos de una serie pueden ser positivos, negativos o números complejos y las series pueden converger (decrecer o crecer hacia un valor finito) no converger (incrementar o decrecer indefinidamente) u oscilar. Existe una serie de criterios y teoremas de aplicación general que expondremos a continuación.  

\subsection{Convergencia absoluta o condicional}

Para estudiar la convergencia de una serie  infinita dada, $\sum a_{i} $ veremos que siempre podremos asociarle otra de la forma $\sum | a_{i} | $, es decir, la serie de valores absolutos, con lo cual garantizamos la positividad (y que sean números reales) de los términos de la serie.  Si la serie de los valores absolutos $\sum | a_{i} | $ converge, entonces también convergerá la serie original $\sum a_{i} $ y diremos que esa serie es \textit{absolutamente convergente}. Sin embargo si la serie de valores absolutos diverge, no podremos decir que $\sum a_{i} $ converja. De hecho, si converge diremos que es \textit{condicionalmente convergente} y, con un rearreglo de sus términos podrá converger, diverger u oscilar.

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema}:
Si $\sum| a_{n}  | $ converge, entonces también
converge $\sum  a_{n} $ y se tiene que:
\[
\left| \sum_{n=1}^{\infty}  a_{n} \right| \leq \sum_{n=1}^{\infty} \left| a_{n}  \right|
\]
\end{mdframed}

Para una serie de términos positivos el criterio de convergencia más intuitivo (necesario  pero no suficiente) es que en límite cuando $n \rightarrow \infty$ el término $n$-ésimo tienda a cero. Con lo cual tenemos que si esta condición no se satisface, la serie diverge.

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema}: Si la serie $\sum  a_{n}$ converge, el término n-ésimo tiende a cero, esto
significa que:
\[
 \lim_{n \rightarrow \infty}a_{n}= 0\,.
\]
\end{mdframed}
 
Notemos que para la serie:
\[
\sum_{n=1}^{\infty} \frac{1}{n} \,\, \Rightarrow \,\, \lim_{n \rightarrow \infty}\frac1n= 0\,,
\]
sin embargo, como  ya vimos anteriormente,  esta serie  diverge. Esto significa que el teorema suministra una condición  suficiente para que exista la divergencia de la serie, es decir, si para el  término  n-ésimo de la serie  $a_{n}$ no se cumple que tiende a cero cuando $n \rightarrow \infty$, entonces la serie  $\sum  a_{n} $ diverge.

Una serie que es convergente pero que no es absolutamente convergente es la siguiente
\[
 \sum_{n=1}^{\infty}  (-1)^{n+1}\frac{1}{n}= 1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4} + \cdots = \ln(2) 
\]
porque ya vimos que la serie de los valores absolutos asociada a la serie anterior  es
\[
 \sum_{n=1}^{\infty}  \frac{1}{n}
\]
la cual diverge.

\subsection{Criterio de Comparación}

En segundo lugar de simplicidad está el criterio de comparación entre un par de series de términos positivos. Si conocemos el comportamiento de una de ellas comparamos el de la otra. Esto es, suponga que consideramos dos series: una de prueba $\sum_{n=0}^{\infty}a_{n}$  y una serie conocida y convergente (o divergente) $\sum_{n=0}^{\infty}\tilde{a}_{n}$, entonces
\[
\text{Si} \  \sum_{n=0}^{\infty} \tilde{a}_{n} \ \text{converge y} \ \forall \, n \ \text{se tiene que} \  
\tilde{a}_{n} \geqslant  a_{n} \,\, \Rightarrow \,\, \sum_{n=0}^{\infty} \tilde{a}_{n} 
\geqslant \sum_{n=0}^{\infty}a_{n} \,\, \Rightarrow \,\,\sum_{n=0}^{\infty}a_{n} \ \text{converge} 
\]
Por otro lado
\[
\text{Si} \ \sum_{n=0}^{\infty} \tilde{a}_{n} \ \text{diverge y} \ \forall \, n \ \text{se tiene que} \ 
0 \leqslant \tilde{a}_{n} \leqslant  a_{n} \,\, \Rightarrow \,\, \sum_{n=0}^{\infty} \tilde{a}_{n} 
\leqslant \sum_{n=0}^{\infty} a_{n} \,\, \Rightarrow \,\,\sum_{n=0}^{\infty} {a}_{n} \ \text{diverge} 
\]

Para ilustrar esta estrategia consideremos la siguiente serie:
\[
\frac{1}{2} +  \frac{1}{3} +  \frac{1}{7} +  \frac{1}{25} + \cdots=\sum_{n=1}^{\infty}\frac{1}{n! +1}  
\]
En ese caso la comparamos con una serie conocida
\[
\sum_{n=0}^{\infty}\frac{1}{n!}=\frac{1}{0!} +\frac{1}{1!} +\frac{1}{2!} +\frac{1}{3!} +\cdots =
		1 + \underbrace{1  +\frac{1}{2!} +\frac{1}{3!} +\cdots}_{e} = 1+ e
\] 
y es claro que la serie indicada no es otra cosa que $e$, con lo cual la serie claramente converge y su suma es $1+e$.

\subsection{Criterio de la Raíz}
Dada una serie de términos positivos $\sum_{n=0}^{\infty}a_{n} $, el criterio de la raíz  (o también de la raíz de Cauchy) puede resumirse en el siguiente par de afirmaciones. Sí:
\[
\begin{array}{lccl}
 \left(a_{n} \right)^{ \frac{1}{n} } \leqslant \rho < 1 & \text{para un  } n \text{ suficientemente grande y } \rho \text{ independiente de }n  & \,\, \Rightarrow \,\,  & \text{converge}  \\ \\
 \left(a_{n} \right)^{\frac{1}{n}}  > 1 & \text{para un  } n \text{ suficientemente grande y } \rho \text{ independiente de }n  & \,\, \Rightarrow \,\,  &  \text{diverge} \\ \\
 \left(a_{n} \right)^{\frac{1}{n}}  = 1 & \text{para un  } n \text{ suficientemente grande y } \rho \text{ independiente de }n  & \,\, \Rightarrow \,\,  &  \text{(?)}
\end{array}
\]
Otra forma, más compacta de expresarlo sería
\[
\text{Sí } \rho = \lim_{n \rightarrow \infty}  \left(a_{n} \right)^{ \frac{1}{n} }  \quad \text{entonces:} \quad
\left\{
\begin{array}{lcl}
 \rho < 1  & \,\, \Rightarrow \,\,  & \text{converge}   \\ \\
  \rho > 1 & \,\, \Rightarrow \,\,  & \text{diverge}   \\ \\
  \rho = 1 & \,\, \Rightarrow \,\,  & \text{(?)}
\end{array}
\right.
\]

Es fácil ver que si utilizamos el criterio de comparación, entonces
\[
 \left(a_{n} \right)^{ \frac{1}{n} } \leqslant \rho \qquad \Rightarrow  a_{n}  \leqslant \rho^{n} 
 \qquad \Rightarrow \left\{
 \begin{array}{ll }
     \text{cuando } \rho < 1    		& \text{la serie converge}  \\ \\
     \text{cuando } \rho \geqslant 1& \text{la serie diverge}    
\end{array} 
\right.
\]

Dada la siguiente serie:
\[
\sum_{n=0}^{\infty} \left[\frac{n}{n+1} \right]^{n^2}\,,
\]
por lo tanto:
\[
\left(a_{n} \right)^{ \frac{1}{n} } = \left[\frac{n}{n+1} \right]^{n} = \frac{1}{\left(1+\frac1n\right)^n}
\,\, \Rightarrow \,\, \quad \rho=  \lim_{n \rightarrow \infty} \frac{1}{\left(1+\frac1n\right)^n} = \frac1e < 1\,.
\]
La serie converge.

\subsection{Criterio de d'Alembert}
Dada una serie de términos positivos $\sum_{n=0}^{\infty}a_{n} $, el criterio de d'Alembert\footnote{\textbf{Jean Le Rond d'Alembert} París, Francia 1717 - 1783. Matemático francés pionero en el estudio de las ecuaciones diferenciales y su utilización en la Física, en particular en el estudio de los fluídos. Más detalles en  \url{http://es.wikipedia.org/wiki/Jean_Le_Rond_d'Alembert} } o también llamado criterio del cociente,  compara el valor relativo de un término de la serie con el que le precede. Este criterio se resume también fácilmente
\[
\text{Si } \rho = \lim_{n \rightarrow \infty}  \ \frac{ a_{n+1} }{a_{n} }    \quad \text{entonces:} \quad
\left\{
\begin{array}{lcl}
 \rho < 1  & \,\, \Rightarrow \,\,  & \text{converge}   \\ \\
 \rho > 1 & \,\, \Rightarrow \,\,  & \text{diverge}   \\ \\
 \rho = 1 & \,\, \Rightarrow \,\,  & \text{indeterminado}
\end{array}
\right.
\]

Nótese que si 
\[ 
\rho < 1 \,\, \Rightarrow \,\, \rho < x< 1 \,\, \Rightarrow \,\, \frac{a_{n+1}}{a_{n}} < x \,\, \Rightarrow \,\,  a_{n+1} = a_{n} x \,.
\]

Entonces para un $N <n$, pero también suficientemente grande, tendremos que los términos de la  serie a partir de ese $N$ serán
\[
a_{N} +a_{N+1} +a_{N+2} +a_{N+3} \cdots = a_{N} +xa_{N} +x^{2}a_{N} +x^{3}a_{N}  \cdots = 
a_{N} \left( 1 + x +x^{2} +x^{3} +x^{4} \cdots \right) 
\]
y que no es otra cosa que una serie geométrica con razón $x<1$ y por consiguiente converge. Es claro que un argumento similar se puede utilizar para probar la divergencia.

Un ejemplo inmediato lo constituye la serie
\[
\frac{1}{2} + \frac{1}{2} + \frac{3}{8} + \frac{1}{4} +\frac{5}{32} +\cdots = \sum_{n=1}^{\infty} \frac{n}{2^{n}} \,\, \Rightarrow \,\, \frac{  \frac{n+1}{2^{n+1}}  }{\frac{n}{2^{n}}} = 
\frac{1}{2}  \frac{n+1}{n}  = \frac12 \left(1 + \frac1n \right)\,,
\]
\[
\rho = \lim_{n \rightarrow \infty} \left[ \frac12 \left(1 + \frac1n \right) \right] = \frac{1}{2} < 1 \,,
\]
con lo cual tiene que converger.

\subsection{Criterio de la Integral de Maclaurin}
\label{IntegralMaclaurin}
El criterio de la Integral de Maclaurin\footnote{\textbf{Colin Maclaurin} 1698, Argyllshire, Escocia - 1746  Edinburgo, Escocia. Matemático escocés quien escribió el  \textit{Tratado de los Fluxiones} el primer tratado que expuso de una manera sistemática y rigurosa el cálculo diferencial ideado por Newton. Este tratado fue como respuesta a la crítica de Berkeley sobre la falta de rigurosidad de los métodos Newton.} es otro criterio de comparación, pero esta vez se compara la serie con una integral.   Así supondremos que existe  una función $f(x)$ continua y monótonamente decreciente para un valor de $x \geqslant x_{0}$ y que, adicionalmente, se cumple que para algún valor entero $x=n$ el valor de la función es igual a un término de la serie, esto es, $f(n)=a_{n}$. Entonces se tendrá que si el límite $\lim_{N \rightarrow \infty} \int^{N} \text{d}x\ f(x)$ existe y es finito, entonces $\sum_{n=1}^{\infty} a_{n}$ converge. Por el contrario si el límite no existe o es infinito, entonces diverge.

La idea de este criterio es comparar la integral de $f(x)$ (el área bajo la curva) con la suma de rectángulos que representa la serie. Entonces, la suma parcial
\[
s_{i}=\sum_{n=1}^{i} a_{n} \equiv \sum_{n=1}^{i} f(n) \,.
\]
Pero:
\[
\left.
\begin{array}{l}
   s_{i} > \int_{1}^{i+1}  \text{d}x\ f(x)       \\ \\
   s_{i} - a_{1} < \int_{1}^{i}  \text{d}x\ f(x)         
\end{array}
\right\} \,\, \Rightarrow \,\, \int_{1}^{i+1}  \text{d}x\ f(x) \leq  s_{i} \leq  \int_{1}^{i}  \text{d}x\ f(x)  +a_{1}
\]
donde $a_1=f(1)$, 
con lo cual, al hacer $i \rightarrow \infty$ tendremos que si el límite de la integral existe, entonces la serie $\sum_{n=1}^{\infty} a_{n}$ converge. 
\[
\int_{1}^{\infty}  \text{d}x\ f(x) \leq \sum_{n=1}^{\infty} a_{n} \leq  \int_{1}^{\infty}  \text{d}x\ f(x)  +a_{1}
\]

\begin{figure}[t]
\begin{center}
\includegraphics[width=4.0in]{VOLUMEN_2/02_Series/Figuras/fdeN}
\caption{ \small El criterio de la integral}
\label{fvsn}
\end{center}
\end{figure}


Un ejemplo inmediato podría ser determinar si la siguiente serie converge
\[
\sum_{n=1}^{\infty} \frac{1}{\left( n - \frac{3}{2} \right)^2} \,.
\]
Hacemos
\[
f(x)=\frac{1}{\left( x - \frac{3}{2} \right)^2} \,\, \Rightarrow \,\,
 \int^N \frac{1}{\left( x - \frac{3}{2} \right)^2} \text{d}x = \frac{-1}{N -\frac{3}{2}}\,,
\]
debemos ahora calcular el límite
\[ 
 \lim_{N \rightarrow \infty} \left( \frac{-1}{N -\frac{3}{2}} \right)=0
\]
con lo cual claramente converge.

Este criterio es muy útil para acotar (entre un ínfimo y un supremo) el residuo de una determinada serie. Vale decir
\[
\sum_{n=1}^{\infty} a_{n}=\sum_{n=1}^{N} a_{n} + \underbrace{\sum_{n=N+1}^{\infty} a_{n}}_{\text{Residuo}} 
\,\, \Rightarrow \,\,
\int_{N+1}^{\infty}  \text{d}x\ f(x) \leq \sum_{n=N+1}^{\infty} a_{n} \leq  \int_{N+1}^{\infty}  \text{d}x\ f(x)  +a_{N+1}\,.
\]

Podemos  comprobar que la función Zeta de Riemann, 
\[
\zeta(z) = \sum_{n=1}^{\infty} n^{-z}\,,
\] 
efectivamente converge. En este caso $f(x)=x^{-z}$, entonces
\[
\zeta(z) = \sum_{n=1}^{\infty} n^{-z} \,\, \Rightarrow \,\, \int_{1}^{\infty}  \text{d}x\ x^{-z} =
\left\{ 
\begin{array}{lc}
\left. \frac{x^{1-z}}{1-z} \right|_{1}^{\infty}      & \text{ Para } z \neq 1  \\ \\
   \left. \ln(x) \right|_{1}^{\infty}     &   \text{ Para } z = 1 
\end{array}
\right.
\]
y es claro que para $z>1$ el límite existe y es finito, por lo tanto, la función Zeta de Riemann, $\zeta(z) = \sum_{n=1}^{\infty} n^{-z}$, converge para $z>1$.



\subsection{Series alternantes y convergencia condicional}

Hasta ahora todos los criterios que analizamos eran para una serie de términos positivos, por lo cual todos esos criterios nos llevaban al concepto de series absolutamente convergente. Esto es, si  $\sum_{n=0}^{\infty} |a_{n} |$  converge, entonces $ \sum_{n=0}^{\infty}  a_{n}$   también converge. Sin embargo, muchas veces nos tendremos que conformar con que una serie sea simplemente convergente y no requerir que sea absolutamente convergente. Este es el caso de las series alternantes. Series en las cuales se alternan términos positivos y negativos:
\[
a_{1} -a_{2} +a_{3} -a_{4} +a_{5} -a_{6} +\cdots+a_{2n-1} -a_{2n} +\cdots =\sum_{n=1}^{\infty} (-1)^{n+1} _{n}  \quad \text{ con } a_{n} \geq 0 \,.
\]
Entonces, si la serie es monótona decreciente para un $n$ suficientemente grande tenemos lo que se denomina el Criterio de Leibniz:
\[
\sum_{n=1}^{\infty} (-1)^{n+1} a_{n} \text{ converge, si:\, } \quad 
\left\{
\begin{array}{lc}
a_{n} > a_{n-1}       & \forall \quad n >N \\ \\ & \wedge  \\ \\
a_{n} \rightarrow 0   & \text{cuando } n \rightarrow \infty
\end{array}
\right. 
\]
De otro modo la serie oscilará. 

Estas condiciones son fáciles de ver si reorganizamos la serie de los primeros $2m$ términos, a partir de un determinado $N$ par y  $ N > n $, entonces
\[
s_{2m}=(a_{N}-a_{N-1})+(a_{N-2}-a_{N-3})+\cdots+(a_{N+2m-2}-a_{N+2m-1}) \,,
\]
donde todos los paréntesis son positivos, con lo cual $s_{2m} > 0$ y se incrementa al incrementar $m$. Ahora bien, si rearreglamos la serie tendremos que 
\[
s_{2m}=a_{N}-(a_{N-1}-a_{N-2})- (a_{N-3}-a_{N-4})+\cdots-(a_{N+2m-1}-a_{N+2m-2})- a_{N+2m-1} \,,
\]
donde, otra vez los paréntesis son positivos y es inmediato comprobar que entonces $s_{2m} < a_{n}$ para todo $m$. 

Como  $a_{n} \rightarrow 0 $   cuando  $n \rightarrow \infty$, la serie alternante necesariamente converge.

La series alternantes ya eran conocidas desde hace mucho tiempo, como por ejemplo la serie
\[
\sum_{n=1}^{\infty}  a_{n} = x - \frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+ \cdots + 
(-1)^{n-1} \frac{x^n}{n}+ \cdots \,.
\]

Esta serie converge y su suma es $\ln(1+x)$ para $-1 < x \leq 1$. Para $x$ positivo es una serie alternante y en el caso particular de $x=1$ se tiene:
\[
1 - \frac{1}{2}+\frac{1}{3}-\frac{1}{4}+ \cdots + (-1)^{n-1}\left(\frac{1}{n}\right)+ \cdots = \ln(2) \,.
\]

Otra relación interesante es:
\[
1 - \frac{1}{3}+\frac{1}{5}-\frac{1}{7}+ \cdots + (-1)^{n-1}\left(\frac{1}{2n-1}\right)+ 
\cdots = \frac\pi4 \,.
\]

Consideremos ahora el siguiente teorema:
\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema}: Si $\{a_n\}$ es una sucesión monótona decreciente con límite igual a  cero, la serie alternante 
\[\sum_{n=1}^{\infty} (-1)^{n-1} a_n \,,
\]
converge.
\end{mdframed}

Si $S$ es su suma y $s_n$ su suma parcial $n$-ésima, se tiene que:
\[
0 < (-1)^n\left(S-s_n \right) < a_{n+1} \quad \mbox{para} \quad  n \geq 1\,.
\]

Consideremos  la siguiente serie
\[
\sum_{n=1}^{\infty} (-1)^{n-1} \left( \frac1n \right) = 1 - \frac{1}{2}+\frac{1}{3}-\frac{1}{4}+ \cdots\,.
\]

Sabemos que $1/n$ es una sucesión monótona decreciente y que:
\[
\lim_{n\rightarrow \infty} \frac1n = 0\,,
\]
por lo tanto, de acuerdo al teorema anterior la serie converge; como ya hemos visto.

Consideremos ahora la serie
\[
a_{2n-1}= \frac12 \quad \mbox{y} \quad a_{2n}=\int_n^{n+1}\frac{dx}{x} \quad \mbox{para} \quad n=1,2,3,. . . \,.
\]
Por otro lado, se tiene también que:
\[
\lim_{n\rightarrow \infty} a_n = 0\,,
\]
y que $a_n$ es monótona decreciente, por lo tanto la serie 
\[
\sum_{n=1}^{\infty} (-1)^{n-1} a_n\,,
\]
converge.


\subsection{{\color{Fuchsia}Ejemplos}}



\newpage
\subsection{{\color{red}Practicando con Maxima}}

Demostremos que determinada sucesión converge a partir de la definición
de límite. Consideremos la siguiente sucesión de número reales:
\[
a_n=\frac{n-1}{n+2}
\]
convergente y de límite $L=1$. 

Aplicaremos la definición de límite, es decir, utilizaremos la condición para la existencia de un límite $L$, que  para cada $\varepsilon > 0$ existe un número $N=N(\varepsilon) \in \mathds{N}$ tal que:
\[
| L - a_{n} | < \varepsilon \quad \text{para} \ n> N 
\,\, \Rightarrow \,\,  
| s_{n}- a_{m} | < \varepsilon \quad \text{para, todo } \ n,m > N \,.
\]

Consideremos la siguiente sucesión:
\[
a_n=\frac{n-1}{n+2}\,.
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
a[n]:=(n-1)/(n+2);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
a_n:=\frac{n-1}{n+2}
\end{math}
\newline

Calculemos $| L - a_{n} |$, con $L=1$

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
I1:(factor(1-a[n]));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
{{3}\over{n+2}}
\end{math}
\newline

Entonces ${{3}\over{n+2}}<\varepsilon$ para todo $n>N$. Lo que implica que
\[
\varepsilon -{{3}\over{n+2}} >0
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
I2:factor(epsilon-I1);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
{{\varepsilon\,n+2\,\varepsilon-3}\over{n+2}}
\end{math}
\newline

Lo que tenemos entonces es:
\[
{{\varepsilon\,n+2\,\varepsilon-3}\over{n+2}} >0 \,,
\]
para todo $n>N$. Despejemos $n$ de la desigualdad. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sol:solve(I2=0,n);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\left[ n=-{{2\,\varepsilon-3}\over{\varepsilon}} \right] 
\end{math}
\newline

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
I3:rhs(sol[1]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
-{{2\,\varepsilon-3}\over{\varepsilon}}
\end{math}
\newline

Por lo tanto
\[
n>-{{2\,\varepsilon-3}\over{\varepsilon}}\,.
\]

Entonces, dado un $\varepsilon$, y por muy pequeño que éste sea, se tiene

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
epsilon:0.01; N:I3,numer; assume(n>N)$ is(I1<epsilon);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\mbox{0.01}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\mbox{298.0}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
{\tt true}
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
epsilon:0.0001; N:I3,numer; assume(n>N)$ is(I1<epsilon);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\mbox{1.0} \times 10^{-4}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\mbox{29998.0}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
{\tt true}
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
epsilon:0.000001; N:I3,numer; assume(n>N)$ is(I1<epsilon);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
\mbox{9.999999999999999} \times 10^{-7}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
\mbox{2999998.0}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
{\tt true}
\end{math}
\newline

Este procedimiento que podemos seguir haciendo para $\varepsilon$ cada vez más pequeño, pero siempre con $\varepsilon>0$. La condición se seguirá cumpliendo para un  $N$ que se incrementará a medida que $\varepsilon$  se tiende a cero.

Es claro que también podemos escribir

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
'limit(a[n],n,inf)=limit(a[n],n,inf);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
\lim_{n\rightarrow \infty }{{{n-1}\over{n+2}}}=1
\end{math}
\newline





%\begin{center}
%\begin{boxedminipage}[h]{16.9cm}
\textcolor{red}{ $>$ {\tt restart:} } \\
\textcolor{red}{ $>$ {\tt assume(p>1):}} \\
\textcolor{red}{ $>$ {\tt Limit(Int((x)$^{\wedge}$(-p),x=1..infinity),x=infinity)=\\
limit(int((x)$^{\wedge}$(-p),x=1..infinity),x=infinity);} } \\
\textcolor{red}{ $>$ {\tt assume(p<=1):} }\\
\textcolor{red}{ $>$ {\tt Limit(Int((x)$^{\wedge}$(-p),x=1..infinity),x=infinity)=\\
limit(int((x)$^{\wedge}$(-p),x=1..infinity),x=infinity); } }
%\end{boxedminipage}
%\end{center}

\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Encuentre el radio de convergencia de las siguientes series
\begin{enumerate}
\item  \[ \sum_{n=0}^{\infty} n^2x^n\]
\item  \[\sum_{n=0}^{\infty} \frac{2^n}{n!} x^n \]
\item  \[\sum_{n=0}^{\infty} \frac{5^n}{n^{3/2}} x^n \]
\item  \[\sum_{n=0}^{\infty} \frac{n^4}{5^n} x^n \]
\item  \[\sum_{n=0}^{\infty} \frac{x^n}{[3+(-1)^n]^n}  \]
\item  \[\sum_{n=0}^{\infty} \frac{x^n}{2^{n+(-1)^n}} \]
\end{enumerate}
\end{enumerate}



\section{Series de funciones}

La idea de series se puede ampliar al permitir que sus términos sean función de alguna variable (una o varias), esto es $a_{n}=a_{n}(x)$. Esta extensión del concepto se serie, trae como consecuencia que ahora las sumas parciales dependen de $x$
\[
s_{n}(x)=\sum_{k=0}^{n} a_{k}(x) = a_{0}(x) +a_{1}(x) +a_{2}(x) + \cdots \,,
\]
con lo cual, si
\[
\lim_{n \rightarrow \infty} s_{n}(x) = S(x)= \sum_{k=1}^{\infty} a_{k}(x)\,,
\]
entonces, el comportamiento de las serie también dependerá de la variable. 

La convergencia de la serie podrá ser posible para algunos valores de $x$ y no para otros. El punto central con las series de funciones $f(x)$ complicadas es la de tratar de construir funciones como una serie de términos, $a_{k}(x)$, más simples. Así, esas sumas parciales $f_{n}(x)$  constituirán la función deseada
\[
f(x)= \sum_{k=1}^{\infty} a_{k}(x) = 
\lim_{n \rightarrow \infty} \sum_{k=1}^{n} a_{k}(x)\,.
\]

Estaremos interesados en aquellas funciones a las cuales converjan las sumas parciales de una serie. Para fijar conceptos, comenzaremos por las series de funciones más comunes: Las Series de Potencias. 

\subsection{Series de potencias}

Asociaremos una serie de potencias $a_{n}=c_{n} x^{n}$ a un polinomio de grado infinito.
\[
P(x)=c_{0}+ c_{1}x+ c_{2}x^{2}+ c_{3}x^{3}+ c_{4}x^{4}+\cdots =\sum_{n=0}^{\infty} c_{n} x^{n} \,, \,\, 
\text{o también} \,\, P(x-x_{0})=\sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right) ^{n}\,.
\]
Esta asociación tiene la ventaja de permitirnos intuir algunos comportamientos de la serie para algunos valores de $x$. Los coeficientes $ c_{n}$ son números independientes de $x$. Pero, más aún, estas series pueden ser series de potencias de número complejos. Vale decir, $\sum_{n=0}^{\infty}  c_{n} z^{n}$ con $z = x +iy$.
 
\subsection{Convergencia de una serie de potencias}

Se pueden utilizar todos los criterios que hemos desarrollado anteriormente. Así  una serie de potencias $\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right) ^{n}$ converge en un punto $x_{0}$ si el límite
\[
\lim_{n\rightarrow\infty}\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)  ^{n}
\]
existe, para $x=x_{0}$, para todo $x$ o para algunos $x$.

Una serie de potencias $\sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right)^{n}$ convergerá absolutamente 
sí 
\[ 
\lim_{n \rightarrow \infty} \sum_{j=0}^{n}\left|  c_{j}\left(x-x_{0}\right)  ^{j}\right|=\rho \,, \qquad  \mbox{existe.}
\]

También se cumplirá el criterio de convergencia absoluta. Esto es, si 
$\sum_{n=0}^{\infty}\left|  c_{n}\left(  x-x_{0}\right)^{n}\right|  $ converge, entonces,  
$ \sum_{n=0}^{\infty} c_{n}\left( x-x_{0} \right)^{n}$ converge, pero el inverso no es siempre verdad.

Los criterios más populares para evaluar la convergencia, se seguirán cumpliendo. Así el criterio de d'Alembert y el de la raíz de Cauchy se podrán reescribir como:
\[
\rho(x)=
\left\{
\begin{array}[c]{c}
\lim_{n \rightarrow \infty}  \left|  \dfrac{c_{n+1}\left(  x-x_{0}\right)^{n+1}}{c_{n}\left(x-x_{0}\right)  ^{n}}\right| \\ \\  
\lim_{n \rightarrow \infty}  \sqrt[n]{c_{n}\left(x-x_{0}\right) ^{n}}
\end{array}
\right.  \quad \Rightarrow \quad 
\begin{array}[c]{c}
\rho(x)<1 \quad \Rightarrow \quad \text{converge} \\ \\
\rho(x)>1 \quad \Rightarrow \quad \text{diverge}
\end{array}
\]

Sólo que ahora es bueno enfatizar que $\rho=\rho(x)$ dependerá de la variable. 
Llamaremos, de ahora en adelante a este límite el \textit{radio o entorno de convergencia},
el cual delimitará los valores de $x$ para que la serie de potencias converja. 

Consideremos el siguiente ejemplo, dada  la serie

\[
1 + x+\frac{x^{2}}{2} + \frac{x^{3}}{6} +\cdots +\frac{x^{n}}{n!} +\cdots=\sum_{n=0}^{\infty} \frac{x^{n}}{n!}\,, 
\]
por lo tanto
\[
\lim_{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}} = 
\lim_{n \rightarrow \infty} \left| \dfrac{  \dfrac{x^{n+1} }{ (n+1)! } }{ \dfrac{x^{n} }{ n!}} \right| = 
\lim_{n \rightarrow \infty} \left| \dfrac{ x }{n +1 } \right|  =0 \,,
\]
es decir, 
\[
\rho (x)= \lim_{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}} =0\,,
\]
con lo cual la serie converge para todo valor de $x$. 


Para puntualizar:
\begin{itemize}
\item  Si una serie converge en $x=x_{1}$, convergerá absolutamente para $\left|  x-x_{0}\right|  <\left|  x_{1}-x_{0}\right|  $ y divergerá para $\left|  x-x_{0}\right|  >\left|  x_{1}-x_{0}\right|$.
\item  Se llama radio de convergencia, $\rho =\rho(x)$ a aquella cantidad tal que la serie $\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right) ^{n}$ converge para $\left|  x-x_{0}\right|  < \rho$ y diverge para $\left|  x-x_{0}\right| >\rho.$ 
\end{itemize}

Una serie $\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)  ^{n}$ que converge únicamente para $x=x_{0}$ tendrá un radio de convergencia $\rho=0,$ mientras que una que converja para todo $x$ tendrá un radio de convergencia $\rho=\infty$.


\subsection{Convergencia uniforme}

Se puede refrasear el criterio de convergencia de Cauchy que vimos anteriormente. Para cualquier valor de $\epsilon > 0$, tan pequeño como uno quiera, siempre existirá un número $N$ \textbf{independiente de} $x$, con $a \leq x \leq b$,  tal que:
\[
\label{con_uni}
\text {si } S(x) = \lim_{n \rightarrow \infty} s_{n}(x)  = \sum_{n=1}^{\infty} a_{n}(x) 
\,\, \Rightarrow \,\, \left| S(x) - s_{n}(x)  \right| < \epsilon \quad \forall \; x \in \left[a,b\right] \: 
\wedge \; n \geq N.
\]

Con ello es inmediato identificar el error que se comete cuando se corta la serie en un $N$ suficientemente grande
\[
S(x) =\underbrace{ \sum_{n=1}^{N} a_{n}(x)}_{s_{n}(x)} + \underbrace{\sum_{n=N+1}^{\infty} a_{n}(x)}_{\approx \epsilon} 
\]

Hay que resaltar el hecho de que las suma de funciones continuas $a_{n}(x)$ no necesariamente habrá de ser continua, el concepto de convergencia uniforme busca garantizar que esa suma de funciones continuas también sea continua. 

Recordemos la idea de continuidad de una función. Una función  será continua si sus límites por la derecha y por izquierda coinciden
\[
\lim_{t \rightarrow x^{\pm}} f(t) = f(x) 
\]
Por otro lado, a partir del hecho de que
\[
f(x)=\lim_{n \rightarrow \infty} f_n(x) 
\]
es valido preguntarse si el límite de la sucesión de sumas parciales es continua, esto es:
\[
\lim_{t \rightarrow x^{\pm}} \left[ \lim_{n \rightarrow \infty} f_{n}(x) \right]  \stackrel{?}{=} 
 \lim_{n \rightarrow \infty}  \left[ \lim_{t \rightarrow x^{\pm}} f_{n}(x)\right] \,.
\]

Es decir, al suponer que la suma de términos continuos tiende a una función continua estamos suponiendo que podemos intercambiar los límites, pero eso no es siempre cierto. Consideremos el caso (extremo)
\[
f_{n}= n^2 x \left(1 -x^2 \right)^n  \,\,  \text{con:  } 0 \leq x \leq 1 \quad  \text{y }  \quad n=1,2,3, \dots 
\]
entonces:
\begin{eqnarray*}
\lim_{n \rightarrow \infty} f_{n}= 0 & \Rightarrow& \int_{0}^{1} \mathrm{d}x \; \left[ \lim_{n \rightarrow \infty} f_{n}(x) \right] =0 \\
\int_{0}^{1} \mathrm{d}x \; f_{n}(x) =\frac{n^2}{2(n +1)}  &\Rightarrow &   \lim_{n \rightarrow \infty}  \int_{0}^{1} \mathrm{d}x \;  f_{n} \rightarrow \infty
\end{eqnarray*}

Claramente no se pueden intercambiar los límites.

Por ejemplo,  sea la serie 
\[
f(x) = \sum_{k=0}^{\infty} a_{k}(x) = x^2 + \frac{x^2}{1+x^2}+ \frac{x^2}{\left(1+x^2\right)^2}+ \frac{x^2}{\left(1+x^2\right)^3}+
\cdots +  \frac{x^2}{\left(1+x^2\right)^k}+ \cdots \,,
\]
de manera que
\[
a_k(x)=\frac{x^2}{\left(1+x^2\right)^k}\,.
\]
Como 
\[
\frac{a_{n+1}(x)}{a_{n}(x)}= \frac{1}{1+x^2} < 1 \quad \forall \quad x \neq 0\,,
\]
la serie es absolutamente convergente $\forall \, x$ $(x \neq 0)$.

Sin embargo, tenemos que $f(0)=0$. El término $n$-ésimo  para la suma parcial es 
\[
f_n(x)= x^2 \sum_{k=0}^{n-1} \frac{1}{\left(1+x^2\right)^k} =1+x^2-\frac{1}{\left(1+x^2\right)^{n-1}}=
1+x^2-\frac{1+x^2}{\left(1+x^2\right)^{n}}  \,,
\]
como $1+x^2>1$, entonces:
\[
\lim_{n \rightarrow \infty} f_n(x)=1+x^2 \,, \qquad x \neq 0 \,.
\]
pero hemos establecido que $f(0)=0$ de manera que $f(x)$ no es continua.


Para el caso de series de funciones, existen un par de criterios que identifican la convergencia uniforme. El criterio Mayorante de Weierstrass\footnote{\textbf{Karl Theodor Wilhelm Weierstrass} (1815 - 1897). Matemático Alemán con importantes contribuciones al análisis complejo mediante la utilización de series.} y el criterio de Abel\footnote{\textbf{Niels Henrik Abel} (1802-1829). Matemático Noruego. Su primera mayor aportación fue la prueba de la imposibilidad de resolución algebraica de la ecuación quíntica mediante radicales.}. Estos criterios desarrollan la noción de convergencia uniforme la cual es necesaria para asegurar el intercambio en los límites.


\subsection{Criterio Mayorante de Weierstrass}

La idea de convergencia uniforme  se introduce para garantizar que la sumas infinitas 
de un conjunto de funciones sea continua. 

Una condición suficiente, pero no necesaria, para que la serie 
\[
a_1(x)+a_2(x)+a_3(x) + \cdots + a_n(x)+ \cdots = \sum_{n=1}^{\infty} a_{n}(x)\,
\] 
sea uniformemente convergente es dada por la condición de Weierstrass:

Si encontramos una serie  convergente  de números positivos
\[
\mathcal{M} = \sum_{j=1}^{\infty} M_{j} \quad \text{con } M_{i} \geq |a_{i}(x) | \quad \forall \; x \in \left[ a,b \right] \quad \text{entonces la serie }  \sum_{n=1}^{\infty} a_{n}(x) 
\]
es {\bf uniformemente} convergente.

La demostración se obtiene a partir de la definición misma de convergencia. Si $\sum_{j=1}^{\infty} M_{j}$ converge, entonces para $n+1 \geq N$ se tiene
\[
\sum_{j=n+1}^{\infty} M_{j} < \epsilon \,\,\, \text{y como }  |a_{i}(x) | \leq M_{i} \,\,\Rightarrow \,\,
\sum_{j=n+1}^{\infty}  |a_{i}(x) | < \epsilon \,\, \Rightarrow  \,\, \left| S(x) - s_{n}(x)  \right| \equiv 
\sum_{j=n+1}^{\infty}  |a_{i}(x) | < \epsilon
\]
con la cual la serie $\sum_{n=1}^{\infty} a_{n}(x)$ será uniformemente convergente para todo $ x \in \left[ a,b \right]$. 

Ahora bien, como consideramos los $M_{i} \geq 0$. La serie en cuestión también será absolutamente convergente. Otra vez, los criterios de convergencia absoluta y, en este caso, de convergencia uniforme, no son consecuencia uno del otro, ni están relacionados. 

Las series 
\[
\sum_{n=1}^{\infty} \frac{(-1)^n}{n+x^2} \quad \text{para } -\infty < x < \infty \qquad \wedge \qquad 
\sum_{n=1}^{\infty} (-1)^{n-1} \frac{x^{n}}{n} \quad \text{para } 0 \leq x \leq 1
\]
convergen uniformemente pero NO absolutamente. 

Sin embargo, en el intervalo $0 \leq x \leq 1$ la serie $\sum_{j=0}^{\infty} (1-x)x^{j}$ converge absolutamente pero no uniformemente, por cuanto tiene una discontinuidad. Se puede demostrar que 
\[
\sum_{j=0}^{\infty} (1-x)x^{j} = \left\{ \begin{array}{lc}
 1 \,,    & 0 \leq x < 1   \\
 0 \,,  &  x =1 
\end{array}
\right.
\]
con lo cual se puede concluir que una serie arbitraria $f(x)= \sum_{j=1}^{\infty}  a_{i}(x) $ no podrá converger uniformemente en intervalos en los cuales la función $f(x)$ sea discontinua.

Por ejemplo,  la serie
\[
f(x) = \cos(x) + \frac{1}{2^2}\cos^2(x)+ \frac{1}{3^2}\cos^3(x) + \cdots 
\]
es uniformemente convergente, porque al tomar $M_k = 1/k^2$ la serie
\[
\sum_{k=1}^{\infty} \frac{1}{k^2}\,,
\]
converge a $\pi^2/6$.


\subsection{Criterio de Abel}

El criterio de Abel se puede resumir de la siguiente manera.  Sí 
\[
f(x)=\sum_{i=0}^{\infty}  a_{i}(x) \;\, \wedge \;\,  a_{i}(x)=c_{i}(x) f_{i}(x) \,,
\]
donde $\sum_{i=0}^{\infty}  c_{i}(x)$ converge, es decir:
\[
\lim_{n \rightarrow \infty}  \sum_{i=0}^{n}  c_{i}(x) =S\,,
\]
entonces la serie \textit{converge uniformemente} en $\left[a,b \right]$. 

Para que se cumpla el criterio de Abel, $f_{n}(x)$ tiene que estar acotada ($0 \leq f_{n} \leq M \; \forall \: n$) y tiene que ser monótonamente decreciente en el intervalo en el cual esté  definida, $f_{n+1}(x) \leq f_{n}(x)$ con $x \in \left[a,b \right]$.



En resumen, si la serie 
\[
f(x)= a_1(x)+a_2(x)+a_3(x) + \cdots + a_n(x)+ \cdots = \sum_{n=1}^{\infty} a_{n}(x)\,
\] 
es uniformemente convergente para $a \leq x \leq b$, entonces es posible integrar y diferenciar término por término.

\begin{eqnarray*}
\frac{\mathrm{d}f}{\mathrm{d}x} &=& \sum_{k=1}^{\infty} \frac{\mathrm{d}a_k}{\mathrm{d}x}\\
\int_\alpha^\beta f(x)\,\mathrm{d}x &=& \sum_{k=1}^{\infty} \int_\alpha^\beta a_k(x)\,\mathrm{d}x\,,
\end{eqnarray*}
donde $a\leq \alpha < \beta \leq b$.

La convergencia uniforme no implica convergencia absoluta y convergencia absoluta no implica convergencia uniforme, como se vio anteriormente, la serie
\[
\sum_{n=0}^{\infty} \frac{x^2}{\left(1+x^2\right)^n}\,,
\]
es absolutamente convergente pero no uniformemente convergente cerca de $x=0$.

Las series absolutamente convergentes tienen la propiedad de comportarse como las series finitas, los términos pueden ser multiplicados e intercambiado el orden de la suma. Las series uniformemente  convergentes se  comportan como las series finitas donde la serie es continua si cada término de la serie también lo es.

La serie
\[
\sum_{n=1}^{\infty}  \frac{(-1)^{n-1}}{n+x^2} = \frac{1}{1+x^2} - \frac{1}{2+x^2}+ \frac{1}{3+x^2}+ \cdots
\]
es  únicamente condicionalmente convergente, pero, es también  uniformemente convergente.

\subsection{Nota sobre el algebra  de series de potencias}

El álgebra elemental de series se puede reconsiderar a la luz de las series de potencias. De esta forma recordamos que  los índices en las series son mudos
\[
\sum_{n=1}^{\infty}a_{n}\ n\ \left(  x-x_{0}\right)  ^{n-1}=\sum_{j=1}^{\infty}a_{j}\ j\ \left(  x-x_{0}\right)  ^{j-1}=\sum_{k=0}^{\infty}
a_{k+1}\ \left(  k+1\right)  \ \left(  x-x_{0}\right)  ^{k}
\]
en la última sumatoria hemos hecho $k=j-1,$ por lo cual $j=k+1.$

Las series se igualan
\begin{align*}
\sum_{n=0}^{\infty}b_{n}\left(  x-x_{0}\right)  ^{n}  &  =\sum_{n=1}^{\infty}a_{n}\ n\ \left(  x-x_{0}\right)  ^{n-1} \\
\sum_{n=0}^{\infty}b_{n}\left(  x-x_{0}\right)  ^{n}  &  =\sum_{k=0}^{\infty}a_{k+1}\ \left(  k+1\right)  \ \left(  x-x_{0}\right)  ^{k}=\sum_{n=0}^{\infty}a_{n+1}\ \left(  n+1\right)  \ \left(  x-x_{0}\right)  ^{n}
\end{align*}
por lo cual
\[
b_{n}=a_{n+1}\ \left(  n+1\right)  \,.
\]

Si la igualdad hubiera sido
\[
\sum_{n=0}^{\infty} a_{n}\left(  x-x_{0}\right)  ^{n}=\sum_{n=1}^{\infty}a_{n}\ n\ \left(  x-x_{0}\right)  ^{n-1}=
\sum_{n=0}^{\infty}a_{n+1}\ \left(n+1\right)  \ \left(  x-x_{0}\right)  ^{n} 
\quad \Longrightarrow \quad a_{n+1}=\frac{a_{n}}{\left(  n+1\right)  }
\]

Las series se suman
\[
\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)^{n} + \sum_{k=2}^{\infty}b_{k}\left(  x-x_{0}\right)^{k} 
	= a_{0} + a_{1}\left(  x-x_{0}\right) + \sum_{n=2}^{\infty}\left(  a_{n}+b_{n}\right)  \left(  x-x_{0}\right)  ^{n}
\]	 
o también 
\begin{eqnarray*}
\sum_{n=0}^{\infty}a_{n}\left(x-x_{0}\right)^{n} + \sum_{k=0}^{\infty}b_{k+2}\left(x-x_{0}\right)^{k +2} &=& a_{0} + a_{1}\left(  x-x_{0}\right) + \sum_{n=2}^{\infty}\left(  a_{n} + b_{n}\right)  \left(  x-x_{0}\right)^{n} \\
&=&\sum_{n=0}^{\infty}\left(  a_{n} + c_{n-2}\right)  \left(  x-x_{0}\right)^{n}\,,
\end{eqnarray*}
y en este último caso $c_{-2} = c_{-1}=0$ y $c_{i} = b_{i +2}$. 

Nótese como en los dos ejemplos anteriores hemos hecho coincidir los dos índices de la sumatoria desde el comienzo. 

La series también se multiplican, esto es
\[
\left[  \sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)^{n} \right]  \left[ \sum_{n=0}^{\infty}b_{n}\left(  x-x_{0}\right)^{n}\right]  =
\sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right)^{n}
\]
con
\[
c_{n}=a_{0}b_{n}+a_{1}b_{n-1}+a_{2}b_{n-2}+\cdots+a_{j}b_{n-j}+\cdots
+a_{n-2}b_{2}+a_{n-1}b_{1}+a_{n}b_{0}
\]
Si alguna de las series de potencias es absolutamente convergente, entonces su multiplicación con otra, será absolutamente convergente.

Pero también las series de potencias se !`invierten! y para ello utilizamos todo lo visto anteriormente veamos. Supongamos que se tiene una serie del tipo 
\[
y -y_0 =a_{0} +a_{1}\left(  x-x_{0}\right) +a_{2}\left(  x-x_{0}\right)^{2} +\cdots +a_{n}\left(  x-x_{0}\right)^{n}+\cdots
=\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)^{n} 
\]
Es decir tenemos $y -y_0$ expresado en términos de una serie de potencias de $\left(  x-x_{0}\right)^n$ entonces, igual podremos plantearnos invertir el proceso, vale decir, expresar $\left(  x-x_{0}\right)$ en términos de potencias $\left(  y - y_{0}\right)^n$ Esto es
\[
x-x_{0}  = \sum_{n=0}^{\infty} b_{n} \left(  y-y_{0} \right)^{n} \quad \Rightarrow  \quad
 x-x_{0} = \sum_{k=0}^{\infty} b_{k}\left [ \sum_{j=0}^{\infty}a_{j}\left(  x-x_{0}\right)^{j}  \right]^{k} 
\]
y al igualar términos con la misma potencia, despejamos los coeficientes $b_n$ en términos de los $a_n$, de forma que
\begin{align*}
  b_1 =  & \frac{1}{a_1}  \\
  b_2 =  & -\frac{a_2}{(a_1)^3}  \\
  b_3 =  & \frac{2(a_2)^2 -a_1 a_3}{(a_1)^5}  \\
  b_4 =  & \frac{5a_1 a_2 a_3 -a_1^2 a_4 -5a_2^3}{(a_1)^7}  \\
\vdots = & \, \quad \quad \quad \quad \vdots
\end{align*}

Igualmente, si una serie $f(x) = \sum_{n=0}^{\infty} a_{n}(x -x_{0}) = \sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right)^{n} $ converge para un entorno $-R \leq x \leq R$ entonces por el criterio de Mayorante de Weierstrass, convergerá absoluta y uniformemente para $-S \leq x \leq S$ con $0 \leq S \leq R$.

 Más aún, el criterio de Abel nos garantiza las siguientes propiedades:
\begin{itemize}
  \item Dado que todos los términos  $a_{n}(x) =c_{n}\left(  x-x_{0}\right)^{n}$ son funciones continuas de $x$ y $f(x)  = \sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right)^{n} $ converge uniformemente para un entorno  $-S \leq x \leq S$, entonces la función $f(x)$ es continua en el intervalo de convergencia.
  \item Si los términos $a_{n}(x) =c_{n}\left(  x-x_{0}\right)^{n}$ son funciones continuas de $x$, entonces la serie puede ser derivada término a término 
  \[
\frac{\mathrm{d}}{\mathrm{d}x}\left[  \sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right)^{n}\right]  = 
\sum_{n=1}^{\infty}c_{n}\ n\ \left(x-x_{0}\right)^{n-1}
\]
(nótese como cambia el comienzo de la serie) y convergerá a 
\[
\sum_{n=1}^{\infty}c_{n}\ n\ \left(x-x_{0}\right)^{n-1} \rightarrow \frac{\mathrm{d} f(x) }{\mathrm{d}x} \quad 
a_{n}(x) \; \wedge \frac{\mathrm{d}}{\mathrm{d}x} a_{n}(x)\quad \text{continuas} \; \wedge \; \sum_{n=0}^{\infty} a_{n}(x)\,, 
\]
converge uniformemente en $\left[a,b \right]$. 
\item De igual manera las series pueden ser integradas término a término
  \[
  \int_{a}^{b} \mathrm{d}x \; f(x) =  \int_{a}^{b} \mathrm{d}x \; \sum_{n=0}^{\infty}c_{n}\left(  x-x_{0} \right)^{n} 
  =  \sum_{n=0}^{\infty} \int_{a}^{b} \mathrm{d}x \; c_{n}\left(  x-x_{0} \right)^{n}
  = \sum_{n=0}^{\infty}\frac{c_{n}}{n+1}\left(  x-x_{0} \right)^{n+1} \,.   
  \]
\end{itemize}



\subsection{{\color{Fuchsia}Ejemplos}}

\begin{enumerate}

\item  Otro caso ocurre cuando consideramos la siguiente serie de potencias:
\[
\sum_{n=1}^{\infty} \left(  -1\right)  ^{n+1}\ n\ \left(  x-2\right)^{n}=
x-2-2\, \left( x-2 \right) ^{2}+3\, \left( x-2 \right) ^{3}-4\,
 \left( x-2 \right) ^{4}+\cdots\,,
\]
por lo tanto:
\[
\rho(x) = \lim_{n \rightarrow \infty} 
\left|
\frac{\left( -1\right)  ^{n+2}\ \left(  n+1\right)  \ \left(  x-2\right)^{n+1}}{\left(  -1\right)^{n+1}\ n\ \left(  x-2\right)^{n}}\right|
= \left|  x-2\right|  \lim_{n\rightarrow\infty}\left|  \frac{n+1}{n}\right| = \left|  x-2\right| 
\]
lo que implica que la serie:
\[
\text{converge si:}\, \left| x-2\right|  <1 \Rightarrow \,\, 1<x<3 \quad \text{y} \quad 
\text{diverge si:}\, \left|  x-2\right|  >1\,.
\]
Es decir, la serie $ \sum_{n=1}^{\infty} \left(  -1\right)  ^{n+1}\ n\ \left(  x-2 \right)^{n} $ convergerá únicamente para $1 < x < 3 $. Para otros valores de $x$, diverge.

\item Dada la serie
\[
\sum_{n=1}^{\infty} a_{n}(x) = \sum_{n=1}^{\infty} 
\frac{x}{\left[ (n-1)x+1 \right]\left[ nx+1 \right]}\,,
\]
cuya suma $n$-ésima  parcial es
\[
s_n(x) = \frac{nx}{nx + 1}\,.
\]
La función $s_n(x)$ es una función continua de $x$ $\forall$ $ 0 \leq x \leq 1 $, y para todo $n$. 
Por otro lado,
\begin{eqnarray*}
S(x) &=& \lim_{n \rightarrow \infty} s_n(x) =0 \,,\,\,\mbox{si} \,\,\, x=0 \\
S(x) &=& \lim_{n \rightarrow \infty} s_n(x) =1 \,,\,\,\mbox{si} \,\,\, x \neq 0 \,.
\end{eqnarray*}
Existe una discontinuidad en $x=0$ para $S(x)$ y por lo tanto la condición 
(\ref{con_uni}) no se  cumplirá.






\end{enumerate}



\subsection{{\color{red}Practicando con Maxima}}

\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Demuestre que la serie
\[
\sum_{n=0}^{\infty} \frac{(-1)^n}{n+x^2}\,,
\]
es uniformemente y condicionalmente convergente.
\item Demuestre que la serie
\[
\sum_{n=1}^{\infty}(-1)^n \frac{n+x^2}{n^2}\,,
\]
converge uniformemente, pero no absolutamente.

\item Determine el radio de convergencia de la serie
\[
\frac{x}{a+1}+ \frac{x^2}{a+\sqrt{2}} + \frac{x^3}{a+\sqrt{3}} + \cdots +  \frac{x^n}{a+\sqrt{n}}+ \cdots
\]
donde $a$ es un número real y positivo. Determine si la serie es o no uniformemente convergente.

\item Considere la siguiente sucesión 
\[
f_n(x)=\frac{\mbox{sen}(nx)}{\sqrt{n}}\,,\,\, n=1,2,3,\dots
\]
demuestre que:
\[
\frac{\mathrm{d}}{\mathrm{d}x}\left[\lim_{n \rightarrow \infty} f_n(x) \right] \neq  
\lim_{n \rightarrow \infty} \frac{\mathrm{d}}{\mathrm{d}x}f_n(x)\,,
\]

\item Discuta la convergencia de las siguientes series
\begin{enumerate}
\item
\[
1-\frac23+\frac35-\frac47  + \cdots
\]
\item
\[
\left(a-\frac{b}{2} \right)+\left(\frac{a}{3}-\frac{b}{4} \right)+ \cdots + \left(\frac{a}{2n-1}-\frac{b}{2n} \right) + \cdots
\]
donde $a$ y $b$ son constantes positivas.
\end{enumerate}

\item Utilizando fracciones parciales, demuestre que
\begin{enumerate}
\item
\[
\sum_{k=1}^{\infty} \frac{1}{(k+1)(k+3)(k+5)} = \frac{23}{480}
\]
\item
\[
\sum_{k=1}^{\infty} \frac{3k-2}{k(k+1)(k+2)} =1 \,.
\]
\end{enumerate}

\end{enumerate}


\section{Serie de Taylor}

Para los físicos el uso apropiado (y frecuente) de la serie Taylor facilita muchos cálculos. La idea detrás de este tipo de series es la de la aproximación de una determinada función por una serie de potencias en donde existe una forma sistemática de construir los coeficientes y, dependiendo de el número de términos que utilicemos en la serie, tendremos una idea de cuan aproximada es la serie y cuanto es el error que cometemos al desarrollar la serie hasta un determinado término. Así supondremos que $f=f(x)$ es una función continua y continuamente diferenciable. Con lo cual, si denotamos  $\frac{\mathrm{d} f(x) }{\mathrm{d}x} = f'(x)$, entonces supondremos que $f'(x),f''(x),f'''(x),\cdots,f^{(n)}(x)$  están definidas en el intervalo $\left[ a,b\right]$. 

De cursos anteriores se sabe que:
\[
\int^{a+h}_{a} \mathrm{d}x \; f'(x)= f(a+h) - f(a) \, \Rightarrow \, f(a+h) = f(a)+ \int^{a+h}_{a} \mathrm{d}x \; f'(x)  \, \Rightarrow \, f(a+h) \approx  f(a) + hf'(a)\,,
\] 
donde hemos supuesto que en intervalo $\left[a,a + h\right]$ la función $f'(x)$ es constante y tiene como valor $f'(a)$. Ahora bien, esto vale para todo $x$ y para cualquier función, por lo tanto se cumple que: 
\[
\begin{array}{ r l}
  f(x)  \approx & f(a) + (x- a)f'(a) \,,\\ \\
  f'(x)  \approx & f'(a) + (x- a)f''(a) \,,\\ \\
 f''(x)  \approx & f''(a) + (x- a)f'''(a) \,, \\ 
\vdots    &  \vdots \\ 
 f^{(n-1)}(x)  \approx & f^{(n-1)}(a) + (x- a)f^{(n)}(a) \,.
\end{array}
\]
Con lo cual podemos construir
\[
 f(a+h) =   f(a) +  \int^{a+h}_{a} \mathrm{d}x \; f'(x) \approx  f(a) +  \int^{a+h}_{a} \mathrm{d}x \left[ f'(a) + (x- a)f''(a) \right]
  \approx  f(a) +h f'(a) +\frac{h^2}{2}f''(a)\,,
\]
que no es otra cosa que una aproximación de segundo orden a $f(a+h)$. En general podemos construir

\begin{eqnarray}
 f(a+h) & = & f(a) +  \int^{a+h}_{a} \mathrm{d}x \; f'(x) =   f(a) +  \int^{a+h}_{a} \mathrm{d}x \; \left[  f'(a) +  \int^{a+h}_{a} \mathrm{d}x \; f''(x) \right] \nonumber \\ 
 	& = & f(a) +h f'(a) +  \int^{a+h}_{a} \mathrm{d}v \; \left[  \int^{a+h}_{a} \mathrm{d}x \; f''(x)\right] \nonumber \\ 
     	& = & f(a) +h f'(a) +  \int^{a+h}_{a} \mathrm{d}u \; \left(  \int^{a+h}_{a} \mathrm{d}v \; \left[  f''(a) +  \int^{a+h}_{a} \mathrm{d}x \; f'''(x) \right] \right) \nonumber \\
     	& = &  f(a) +h f'(a) +\frac{h^2}{2} f''(a) + \int^{a+h}_{a} \mathrm{d}u \; \left(  \int^{a+h}_{a} \mathrm{d}v \; \left[  \int^{a+h}_{a} \mathrm{d}x \; f'''(x) \right] \right)
\nonumber
\end{eqnarray}
y si repetimos ese procedimiento $n$ veces, suponiendo que las derivadas de $f(x)$ existan, tendremos la aproximación $n-1$ a la función. Esto es
\[
  f(a+h) =  f(a) +h f'(a) +\frac{h^2}{2!}f''(a) +\frac{h^3}{3!}f'''(a)+ \cdots+\frac{h^{n-1}}{(n-1)!}f^{(n-1)}(a) + \mathcal{R}_{n}
\]
y también es fácil convencerse por inspección que el residuo o el error que cometemos en la aproximación $n-1$ viene dado por la integración enésima de la derivada enésima, vale decir 
\[
\mathcal{R}_{n} =  \int^{a+h}_{a} \mathrm{d}u \;  \int^{a+h}_{a} \mathrm{d}v \; \underbrace{\cdots \; \; \cdots}_{n \text{ veces} } \int^{a+h}_{a} \mathrm{d}x \; f'''(x) 
\]
y por el Teorema del Valor medio
\[
\int^{a+h}_{a} \mathrm{d}\tau g(\tau) = hg(\xi) \quad \Rightarrow \mathcal{R}_{n} =\frac{h^n}{n!}f^{(n)}(\xi) \quad \text{con } a \leq \xi \leq a + h
\]
Ahora bien, una elección astuta del parámetro $h = x-a$ nos lleva a la conocida expresión de la serie de Taylor para una función de una variable
\[
 f(x) =  f(a) +(x-a) f'(a) +\frac{(x-a)^2}{2!}f''(a) +\frac{(x-a)^3}{3!}f'''(a)+ \cdots+\frac{(x-a)^{n-1}}{(n-1)!}f^{(n-1)}(a) + \mathcal{R}_{n}
\]
y el error vendrá dado por 
\[
 \mathcal{R}_{n} =\frac{(x-a)^n}{n!}f^{(n)}(\xi) \quad \text{con } a \leq \xi \leq a + h
\]
así la expansión de Taylor especifica el valor de una función en un punto $x$ en términos de el valor de la función y sus derivadas en un punto de referencia $a$. La expansión se hace en términos de potencias de la diferencia, $(x-a)$, entre el punto que se evalúa y el punto de referencia. 

Algunas otras formas de expresar la serie de Taylor, serían
\[
f(x +h) = \sum^{\infty}_{n=0}  \frac{h^n}{n!}f^{(n)}(x) =   \sum^{\infty}_{n=0} \frac{h^n \frac{\mathrm{d}^n}{\mathrm{d}x^n}}{n!}f(x) =
 \sum^{\infty}_{n=0} \frac{h^n \mathbb{D}^{n} }{n!}f(x) = e^{h\mathbb{D}}f(x) \quad \text{donde,  } \mathbb{D} \equiv \frac{\mathrm{d}}{\mathrm{d}x}
\] 

Si el punto de referencia es $a=0$ tendremos la serie de Maclaurin 
\[
 f(x) =  f(0) + x f'(0) +\frac{x^2}{2!}f''(0) +\frac{x^3}{3!}f'''(0)+ \cdots+\frac{x^{n-1}}{(n-1)!}f^{(n-1)}(0) + \mathcal{R}_{n} 
\]

\subsection{Algunas series de Taylor}

Un listado incompleto de las series de Taylor más utilizadas es:
 \begin{eqnarray}
  e^{x}   & = &  1 + x +\frac{x^2}{2!} +\frac{x^3}{3!}+\frac{x^4}{4!}+\cdots + \frac{x^n}{n!} + \cdots   \quad \text{para }  -\infty < x < \infty  \nonumber \\  
 \mathrm{sen} (x)  & = &  x - \frac{x^3}{3!} +\frac{x^5}{5!} - \frac{x^7}{7!} +\cdots + (-1)^{n+1} \frac{x^{2n-1}}{(2n-1)!} + \cdots \quad    \text{para}  -\infty < x < \infty  \nonumber \\ 
 \cos(x) &	=& 1 - \frac{x^2}{2!} +\frac{x^4}{4!} -\frac{x^6}{6!} +\cdots + (-1)^{n+1} \frac{x^{2n-2}}{(2n-2)!} + \cdots     \quad \text{para}  -\infty < x < \infty  \nonumber  \\   
 \arctan(x)	& = &  x - \frac{x^3}{3} +\frac{x^5}{5} - \frac{x^7}{7} +\cdots + (-1)^{n+1} \frac{x^{2n-1}}{(2n-1)} + \cdots  \quad    \text{para}  -1 < x < 1  \nonumber \\ 
 \ln (1+x)	& = & x -\frac{x^2}{2} +\frac{x^3}{3} - \frac{x^4}{4} +\cdots + (-1)^{n+1} \frac{x^{n}}{n} + \cdots     \quad \text{para }  -1 < x < 1  \nonumber \\
(1 + x)^m	& = & 1 +mx + m(m-1) \frac{x^2}{2} + m(m-1)(m-2) \frac{x^3}{3!} +\cdots + \frac{m!}{n!(m -n)!}x^n + \cdots	 \quad \forall \, x  \nonumber   
 \end{eqnarray}
 
Si tenemos una función y queremos determinar su serie de potencias, es posible que los cálculos se simplifiquen notablemente si utilizamos apropiadamente las series elementales anteriores, por ejemplo:
\[
 e^{\alpha x^2+\beta x} =1 + \left(\alpha x^2+\beta x\right) +
 \frac{\left(\alpha x^2+\beta x\right)^2}{2} +\frac{\left(\alpha x^2+\beta x\right)^3}{3!}+
 \frac{\left(\alpha x^2+\beta x\right)^4}{4!}+\cdots 
\]
desarrollando los términos binomiales
\[
 e^{\alpha x^2+\beta x} =1+\beta\,x+ \left( \alpha+\frac12\,{\beta}^{2} \right) {x}^{2}+ \left( 
\beta\,\alpha+\frac16\,{\beta}^{3} \right) {x}^{3}+ \left( \frac12\,{\alpha}^{
2}+\frac12\,\alpha\,{\beta}^{2}+\frac{1}{24}\,{\beta}^{4} \right) {x}^{4}
+\cdots 
 \]
Si se quiere hacer el desarrollo alrededor de un punto diferente de $x_0=0$, podemos completar cuadrados:
\begin{eqnarray*}
 e^{\alpha x^2+\beta x} &=& e^{\alpha \left(x^2+\beta x/\alpha\right)} =
 e^{\alpha \left(x^2+\beta x/\alpha + \beta^2/4\alpha^2 - \beta^2/4\alpha^2\right)} =
 e^{\alpha \left(x^2+\beta x/\alpha + \beta^2/4\alpha^2 \right)-\beta^2/4\alpha}\\
 &=& e^{\alpha \left(x+ \beta/2\alpha \right)^2-\beta^2/4\alpha} = 
 e^{\alpha \left(x+ \beta/2\alpha \right)^2 } e^{-\beta^2/4\alpha} \\
 &=&e^{-\beta^2/4\alpha} \left[ 1+ \alpha(x+\beta/2\alpha)^2 +  \alpha^2(x+\beta/2\alpha)^4/2  +\cdots  \right]
\end{eqnarray*}
y esta es la expansión en series de potencias alrededor del punto $x_0=-\beta/2\alpha$.

\subsection{La expansión binomial}

Por su uso  frecuente, consideremos el caso de la expansión binomial 
\begin{eqnarray*}
(1 + x)^m &=& 1 +mx + m(m-1) \frac{x^2}{2} + m(m-1)(m-2) \frac{x^3}{3!} +\cdots 
=\sum_{n=0}^{\infty} \frac{m!}{n!(m -n)!}x^n \,, \\
&=& \sum_{n=0}^{\infty} \left(\begin{array}{c}m \\ n \end{array}\right) x^n\,,
\end{eqnarray*}
donde el término $\left(\begin{array}{c}m \\ n \end{array}\right)$ se denomina el coeficiente binomial y la serie termina cuando $m=n$. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=4.5in]{VOLUMEN_2/02_Series/Figuras/gamm}
\caption{ \small La función Gamma para $n=5$}
\label{fGamma}
\end{center}
\end{figure}

Ahora bien, escrito de la forma compacta  se sugiere que el exponente $m$ tendría que ser entero y positivo. Pero no es así. La serie explícita  no se restringe a valores enteros y positivos de $m$. Por ello, la forma compacta pero exacta de la expansión binomial es 
\begin{eqnarray*}
\left(1 + \frac{x}{a}\right)^m &=&  1 +m\left(\frac{x}{a} \right) + \frac{ m(m-1)}{2}\left(\frac{x}{a} \right)^2 +  \frac{m(m-1)(m-2)}{3!}\left(\frac{x}{a} \right)^3 +\cdots  \\
&=& \sum_{n=0}^{\infty} \frac{\Gamma(1+m)}{\Gamma(1+n)\Gamma(1+m - n)}\left(\frac{x}{a} \right)^n\,. 
\end{eqnarray*}
Donde hemos utilizado la función $\Gamma(x) $ como la generalización del factorial para valores que no se restringen a enteros positivos. Nótese también que si el exponente es negativo, $\left(1 + \frac{x}{a}\right)^m$ tiene una singularidad o un polo en $x = -a$.

Cuando $n$ es un entero positivo tendremos 
\[
n!= \Gamma(1 +n)=\int_{0}^{\infty} e^{-t} \, t^n \, \textrm{d}t = \int_{0}^{\infty} e^{-t+n\ln(t)}  \, \textrm{d}t 
\]

Esta integral, como se puede ver en la figura, nos recuerda la forma de una Gaussiana con un máximo en 
$t=n$. Al hacer una expansión alrededor de este punto
\begin{eqnarray*}
f(t)=-t+n\ln(t) &=& f(n)+(t-n)f'(n)+(t-n)^2f''(n)/2 + \cdots  \\
&=& -n +n\ln(n)+0+(t-n)^2(-n/n^2)/2 + \cdots
\end{eqnarray*}

Si conservamos los términos hasta segundo orden, la integral puede ser aproximadamente igual a:
\[
n! \sim \int_{0}^{\infty} e^{-n +n\ln(n)-(t-n)^2/2n}  \, \textrm{d}t =n^ne^{-n}\int_{0}^{\infty} e^{-(t-n)^2/2n}  \, \textrm{d}t
\]

Para valores de $n$ grandes, y esto es lo que se conoce como la aproximación de Stirling, se tiene:
\[
n! \sim n^ne^{-n} \int_{-\infty}^{\infty}  e^{-(t-n)^2/2n}  \, \textrm{d}t = n^ne^{-n}\sqrt{2\pi n}
\]
Aquí, el símbolo $\sim $ se refiere a un comportamiento asintótico de la función Gamma.  

En la siguiente tabla se muestran, para algunos valores de $n$, el valor exacto del factorial, el valor por la fórmula de Stirling y el cociente entre estos dos valores. Se puede apreciar entonces lo buena que resulta tal aproximación.
\begin{table}[h]
  \centering
  \begin{tabular}{@{} |c|c|c|c|@{}} \hline
    $n$ & $n!$  &  $n^ne^{-n}\sqrt{2\pi n}$ &  $n!/(n^ne^{-n}\sqrt{2\pi n})$\\ \hline\hline
    1 & 1 & 0,922 & 0,922 \\ 
    2 & 2 & 1,919 & 0,960 \\ 
    5 & 120 & 118,019 & 0,983 \\ 
  10 & 3628800 & 3598695,619 & 0,992 \\ \hline\hline
  \end{tabular}
  \label{tab:label}
\end{table}

\subsection{Sobre la función Gamma}
Con ya vimos, con la función Gamma es posible generalizar la idea del factorial cuando $n$ es cualquier número real positivo. La función Gamma se define de la siguiente manera
\begin{equation}
\Gamma(k)=\int_{0}^{\infty} x^{k-1} e^{-x}\mathrm{d}x\,,  \quad k> 0 \,.
\label{fungamma}
\end{equation}

Por ejemplo, si $k=1$, tenemos
\[
\Gamma(1)=\int_{0}^{\infty} x^{0} e^{-x}\mathrm{d}x= \lim_{b\rightarrow \infty} \left[ - e^{-x}\ \right]_0^b=1\,.
\]

Podemos integrar (\ref{fungamma}) por partes:
\begin{equation}
\Gamma(k)=   \lim_{b\rightarrow \infty} \left[ \frac{e^{-x}x^k}{k}\ \right]_0^b +
\frac{1}{k} \int_{0}^{\infty} x^{k} e^{-x}\mathrm{d}x\,,  \quad k> 0 \,,
\label{fungamma2}
\end{equation}
se puede demostrar que el primer término de (\ref{fungamma2}) se hace cero, por lo tanto:
\begin{equation}
\Gamma(k)= \frac{1}{k}  \Gamma(k+1)  \,,  \quad k> 0
\label{fungamma3}
\end{equation}
es decir:
\begin{equation}
\Gamma(k+1)= k \Gamma(k)  \,,  \quad k> 0 \,.
\label{fungamma4}
\end{equation}
Veamos:
\begin{center}
\begin{tabular}
[c]{lll}
$k=1 $ & $\rightarrow$ & $\Gamma(2)=1\Gamma(1)=1=1!$  \\
$k=2 $ & $\rightarrow$ & $\Gamma(3)=2\Gamma(2)=2\cdot1=2!$ \\
$k=3 $ & $\rightarrow$ & $\Gamma(4)=3\Gamma(3)= 3 \cdot  2 \cdot 1=3!$\\
$k=4 $ & $\rightarrow$ & $\Gamma(5)=4\Gamma(4)=4 \cdot  3 \cdot 2 \cdot 1=4!$ \\
\end{tabular}
\end{center}
entonces, si $k=n$ es un entero positivo, se tiene que
\begin{equation}
\Gamma(n+1)= n!
\end{equation}

Ahora bien, podemos hacer lo siguiente. Partimos de (\ref{fungamma3}):
\begin{equation}
\Gamma(k)= \frac{1}{k}  \Gamma(k+1)  \,,  \quad k \neq 0 \,,
\label{fungamma5}
\end{equation}
reemplazamos $k \rightarrow k+1$ en (\ref{fungamma5}) y obtenemos:
\begin{equation}
\Gamma(k+1)= \frac{1}{k+1}  \Gamma(k+2)  \,,  \quad k \neq -1 \,,
\label{fungamma6}
\end{equation}
si sustituimos (\ref{fungamma6}) en (\ref{fungamma5}) resulta:
\begin{equation}
\Gamma(k)= \frac{ \Gamma(k+2)}{k(k+1)}   \,,  \quad k \neq 0, -1 \,,
\label{fungamma7}
\end{equation}
si reemplazamos  $k \rightarrow k+1$ en (\ref{fungamma6}) obtenemos
\begin{equation}
\Gamma(k+2)= \frac{ \Gamma(k+3)}{(k+2)}   \,,  \quad k \neq  -2 \,,
\label{fungamma8}
\end{equation}
al sustituir (\ref{fungamma8})  en (\ref{fungamma7}) resulta lo siguiente
\begin{equation}
\Gamma(k)= \frac{ \Gamma(k+3)}{k(k+1)(k+2)}   \,,  \quad k \neq 0, -1, -2 \,.
\label{fungamma9}
\end{equation}

Estas operaciones se pueden repetir hasta $n$:
\begin{equation}
\Gamma(k)= \frac{ \Gamma(k+n)}{k(k+1)(k+2)\cdots(k+n-1)}   \,,  \quad k \neq 0, -1, -2, \dots , -(n-1) \,.
\label{fungamma10}
\end{equation}

Todo esto sirve para lo siguiente. Si $k=-1/2$, entonces por (\ref{fungamma5}) 
\[
\Gamma\left(-\frac12\right)= -2 \Gamma\left(\frac12\right) \,,
\]
el valor de $\Gamma\left(\frac12\right)$ lo podemos buscar en alguna tabla matemática. 


En este caso, lo que se obtiene es $\Gamma\left(\frac12\right)=\sqrt{\pi}$. Esto significa que si  conocemos $\Gamma\left(\frac12\right)$  se puede determinar $\Gamma\left(-\frac12\right)$, por lo tanto
\[
\Gamma\left(-\frac12\right)= -2 \sqrt{\pi} \,.
\]

Algunos ejemplos:
\begin{center}
\begin{tabular}
[c]{rll}
$0!$&=&$\Gamma(0+1)=\Gamma(1)=1$  \\ \\
$\left(-\frac{1}{2}\right)! $&=&$\Gamma(-\frac12+1)=\Gamma\left(\frac12\right)= \sqrt{\pi}$ \\ \\
$\left(-\frac{3}{2}\right)! $&=&$\Gamma(-\frac32+1)=\Gamma\left(-\frac12\right)= -2 \sqrt{\pi}$ \\ \\
$\left(\frac{1}{2}\right)! $&=&$\Gamma(\frac12+1)=\Gamma\left(\frac32\right)=  \frac12 \Gamma\left(\frac12\right)=\frac12 \sqrt{\pi}$ 
\end{tabular}
\end{center}


\subsection{Taylor en varias variables}

Sólo por razones de completitud, y para reforzar los conceptos de que es un desarrollo en series para una función alrededor de un determinado punto, escribiremos el desarrollo en series de Taylor para una función de dos variables $f=f(x,y)$. Esta es
\begin{eqnarray*}
f(x,y)& = & f(a,b) + ( x -a )  \left. f_x \right|_{ ab } + (y-b)\left. f_y \right|_{ ab } \\
    & + & \frac{1}{2!} \left[   \left. ( x -a )^2 f_{xx}  \right|_{ ab } + 2( x -a )( y -a )   \left.  f_{xy} \right|_{ab } +( y -a )^2   \left. f_{yy} \right|_{ ab } \right]\\
 	& + & \frac{1}{3!} \left[ ( x -a )^3  \left. f_{xxx}  \right|_{ ab } + 3( x -a )^2 ( y -a )   \left. f_{xxy} \right|_{ ab } + 3( x -a )( y -a )^2  \left. f_{xyy} \right|_{ ab } +( y -a )^3  \left. f_{yyy}  \right|_{ ab } \right]   \\
	& + & \cdots
\end{eqnarray*}

De una manera  más compacta
\[
 f\left( x^j + x_0^j\right)  =  \sum_{n=0}^{\infty} \frac{1}{n!} \left. \left( x^k \partial_{k} \right)^n  f \left( x^m \right)  \right|_{x^m = x^m_0}  \,\, \Rightarrow  \,\,
f\left( { \mathbf{r}} + \mathbf{a}  \right) =  \sum_{n=0}^{\infty}  \frac{1}{n!} \left. \left( \mathbf{r} \cdot {\nabla} \right)^n  f\left( x^m \right)  \right|_{\mathbf{r} = \mathbf{a} }  \,.
\]

Dónde hemos utilizado la siguiente convención
\[
f_{x} = \frac{\partial}{\partial x } = \partial_{x}; \quad f_{y} = \frac{\partial }{ \partial y} = \partial_{y};  \quad f_{xx} = \frac{\partial^2}{\partial x^2 } = \partial_{xx}; \quad f_{xy} = \frac{\partial^2}{\partial x \partial y} = \partial_{xy};  \quad f_{yy} = \frac{\partial^2}{\partial y^2} = \partial_{yy};  \quad \cdots
\]


\subsection{{\color{Fuchsia}Ejemplos}}

\subsection{{\color{red}Practicando con Maxima}}
%\begin{center}
%\begin{boxedminipage}[h]{16.9cm}
La gráfica mostrada en la Figura 1 pueden obtenerse de la siguiente manera: \\
\textcolor{red}{ $>$ {\tt restart:} } \\
\textcolor{red}{ $>$ {\tt n :=  5:}} \\
\textcolor{red}{ $>$ {\tt f :=  exp(-t)*t$^{\wedge}$n;} } \\
\textcolor{red}{ $>$ {\tt Int(f,t=0..infinity)=int(f,t=0..infinity);} }\\
\textcolor{red}{ $>$ {\tt GAMMA(5+1);} } \\
\textcolor{red}{ $>$ {\tt plot(f,t=0..20); } }\\
\textcolor{red}{ $>$ {\tt `f(5)`=evalf(subs(t=5,f));} }\\
%\end{boxedminipage}
%\end{center}

\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Utilice la siguiente definición 
\[
\tan^{-1} x = \int_{0}^{x} \frac{1}{1+t^2}\,,
\]
expanda el integrando y luego integre término por término para derivar la siguiente
expansión conocida como expansión de Gregory
\[
\tan^{-1} x = x- \frac{x^3}{3}+\frac{x^5}{5}- \cdots = \sum_{n=0}^{\infty}\frac{(-1)^n}{2n+1} x^{2n+1}\,.
\]
Evalúe  la serie  para $x=\frac{\pi}{4}$.

\item Utilizando la definición
\[
\mbox{sen}^{-1} x = \int_{0}^{x} \frac{1}{\sqrt{1+t^2}}\,,
\]
derive las expresiones siguientes
\[
\mbox{sen}^{-1} x =\sum_{n=0}^{\infty} \frac{(2n)!}{4^n (n!)^2} \frac{x^{2n+1}}{2n+1}\,,
\]
$$
\mbox{sen}^{-1} (1-x) = \frac{\pi}{2}-\sqrt{2x}
\left( 1+ \sum_{n=1}^{\infty} \frac{1\mbox{.}  3\mbox{.}   5.  \cdots . (2n-1)}{4^n (2n+1)n!} x^n \right)\,.
$$

\item Encuentre los primeros cinco términos, diferentes de cero, de la serie de Taylor, de la función
\[
f(x)= \frac{1+x}{x^2}\left[\frac{2+2x}{1+2x} - \frac{\ln(1+2x)}{x} \right] \,.
\]
Puedes usar el programa Maxima. 

\end{enumerate}


\section{Series de Fourier}
Otro de los casos de expansión en una base completa de funciones lo constituyen la base de Fourier. En este caso la serie de Fourier la constituyen funciones continuas, reales de variable real y definidas en $\left[  0,2\pi\right]  $, $\mathcal{C}_{\left[0,2\pi\right]  }^{\infty}$, en término de funciones trigonométricas. 

Esto es el conjunto de funciones $\left\{  \left| {u}_{1}\right\rangle ,\ \left| {u}_{2}\right\rangle ,\ \left| {u}_{3}\right\rangle ,\cdots,\left| {u}_{n}\right\rangle \cdots\right\}  $ representadas por
\[
\left| {u}_{0}\right\rangle =1,\qquad\left| {u}_{2n}\right\rangle =\cos(nx)\qquad\text{y}\qquad\left| {u}_{2n-1}\right\rangle =\operatorname{sen}(nx),\qquad\text{con }n=1,2,3,\cdots
\]

Es claro que $\left\{  \left| {u}_{1}\right\rangle ,\ \left|{u}_{2}\right\rangle ,\ \left| {u}_{3}\right\rangle \cdots,\left| {u}_{n}\right\rangle ,\cdots\right\}  $  es un conjunto de funciones ortogonales por cuanto
\[
\left\langle{u}_{n}\right.  \left| {u}_{m}\right\rangle
=\delta_{nm} | \left| {u}_{n}\right\rangle|^{2} \Rightarrow \left\{
\begin{array}
[c]{rcl}
0\quad\text{si} & n\neq m & \left\{
\begin{array}
[c]{c}
\int_{0}^{2\pi}\mathrm{d}x\ \operatorname{sen}(nx)\operatorname{sen}(mx)=0 \\ \\
\int_{0}^{2\pi}\mathrm{d}x\ \cos(nx)\operatorname{sen}(mx)=0 \\ \\
\int_{0}^{2\pi}\mathrm{d}x\ \cos(nx)\cos(mx)=0
\end{array}
\right. \\
&  & \\
|  | {u}_{n}\rangle |^{2}\quad\text{si} &
n=m & \left\{
\begin{array}
[c]{lcl}
\int_{0}^{2\pi}\mathrm{d}x\ = 2\pi \\
&  & \\
\int_{0}^{2\pi}\mathrm{d}x\ \cos^{2}(nx)=\pi \\
&  & \\
\int_{0}^{2\pi}\mathrm{d}x\ \operatorname{sen}^{2}(nx)=\pi & 
\end{array}
\right.
\end{array}
\right.
\]

\begin{figure}[t]
\begin{center}
\includegraphics[width=5.6in]{VOLUMEN_2/02_Series/Figuras/FourierSeriesExamples}
\caption{ Expansiones de Varias funciones en sumas parciales de  Series de Fourier. Tomado de Eric W. Weisstein. \textbf{Fourier Series}. \url{http://mathworld.wolfram.com/FourierSeries.html}}
\label{FourierSeriesExamples}
\end{center}
\end{figure}

Por lo tanto, podremos construir una base ortonormal de funciones \newline $\left\{  \left| {e}_{1}\right\rangle ,\ \left| {e}_{2}\right\rangle ,\ \left| {e}_{3}\right\rangle ,\cdots,\left| {e}_{n}\right\rangle ,\cdots\right\}  $ de la forma
\[
\left| {e}_{0}\right\rangle =\frac{1}{\sqrt{2\pi}},\qquad\left|
{e}_{2n}\right\rangle =\frac{1}{\sqrt{\pi}}\cos(nx)\qquad
\text{y}\qquad\left| {e}_{2n-1}\right\rangle =\frac{1}{\sqrt{\pi}
}\operatorname{sen}(nx)
\]

Tal y como se muestra en la figura \ref{FourierSeriesExamples} distintas funciones pueden ser expandidas con sumas parciales de Fourier.  A diferencia de las series de potencias, que imponen que las funciones a ser expandidas deben ser continuas y continuamente diferenciables en el intervalo, la series de Fourier pueden representar funciones continuas a trozos, siempre y cuando cumplan con algunas condiciones.

Por lo tanto cualquier función definida en el intervalo $\left[ 0,2\pi\right]  $ puede expresarse en términos de esta base como
\[
\left| {f}\right\rangle = \sum_{i=0}^{\infty}  c_{i}\ \left|{e}_{i} \right\rangle \quad \Rightarrow 
c_{i} = \left\langle{e}_{i} \right.  \left| {f}\right\rangle =
\left\{
\begin{array}[c]{lcl}
\frac{1}{\sqrt{2\pi} } \int_{0}^{2\pi} \mathrm{d}x  \ f(x)  = c_{0} \equiv a_{0} & \text{si} & i=0 \\
&  & \\
\frac{1}{\sqrt{\pi} } \int_{0}^{2\pi}\mathrm{d}x\ f( x)  \ \cos(nx) = c_{2n} \equiv a_{m} & \text{si} & i=2n\\
&  & \\
\frac{1}{\sqrt{\pi} } \int_{0}^{2\pi}\mathrm{d}x\ f( x )  \ \operatorname{sen}(nx) = c_{2n-1} \equiv b_{m} & \text{si} & i=2n-1
\end{array}
\right.
\]
donde los $c_{i}$ son los coeficientes de Fourier, con lo cual podemos escribir
\[
F(x) = \frac{a_{0}}{2} +  \sum_{n=1}^{\infty} \left[ a_{n} \cos(nx) + b_{n} \mathrm{sen}(nx) \right] \,,
\]
el término $a_0$ es colocado fuera de la sumatoria, y multiplicado por $1/2$, solo por conveniencia.

De manera equivalente, si el período es $T$ y para un $t_{0}$ genérico
\[
F(t) = \frac{a_{0}}{2} +  \sum_{n=1}^{\infty} \left[ a_{n} \cos \left( \frac{2 \pi nt}{T} \right) + b_{n} \mathrm{sen}\left( \frac{2 \pi nt}{T} \right) \right] \quad \text{con }
\left\{
\begin{array}{cl}
   a_{0} =  & \frac{2}{T} \int_{t_{0}}^{t_{0} + T} \mathrm{d}t  \ f(t) \\
  & \\
   a_{n} =  &  \frac{2}{T} \int_{t_{0}}^{t_{0} + T} \mathrm{d}x\ f(t) \cos \left( \frac{2 \pi nt}{T} \right)   \\
    & \\
   b_{n} =  &  \frac{2}{T} \int_{t_{0}}^{t_{0} + T}\mathrm{d}t\ f(t) \ \mathrm{sen} \left( \frac{2 \pi nt}{T} \right)  
\end{array}
\right. 
\]

La figura \ref{FourierSeriesExamples} muestra la aproximación de las distintas sumas parciales para distintas funciones, a medida que aumentamos el número de términos la aproximación mejora.

Podemos expresar la expansión de una serie de Fourier de manera más compacta, ésta expresión se conoce en algunos ámbitos como la expresión integral para la series de Fourier
\begin{eqnarray}
F(x)	& = & \frac{1}{\sqrt{2 \pi} } \int_{0}^{2\pi} \mathrm{d}t  \ f(t)  \nonumber \\
 	& + &   \sum_{n=1}^{\infty}  \left\{ \left[ \int_{0}^{2\pi}\mathrm{d}t \ f(t)  \cos(nt) \right]\cos(nx) + \left[ \int_{0}^{2\pi}\mathrm{d}t \ f(t) \operatorname{sen}(nt) \right] \mathrm{sen}(nx) \right\} 											\nonumber \\
	 	&  &  	\nonumber \\
  	& = &  \frac{1}{\sqrt{2 \pi} } \int_{0}^{2\pi} \mathrm{d}t  \ f(t)  +  \sum_{n=1}^{\infty} \int_{0}^{2\pi}\mathrm{d}t \ f( t)  \cos(n[t-x]) \,.
 \nonumber
\end{eqnarray}

También es muy común expresar una serie de Fourier en término de una base compleja. Vale decir  $ \big\{ \cdots | \mathbf{ \tilde{\phi} }_{k}  \rangle \cdots \big\} \leftrightarrow \{ \cdots e^{-ikx} \cdots \}   $ con $k=0,\pm1,\pm2,\cdots$. Con lo cual 
\[
|{f} \rangle = \sum_{k=-\infty }^{\infty}  \tilde{C}_{k} |{\tilde{\phi}}_{k} \rangle \equiv \sum_{k=-\infty }^{\infty}  \tilde{C}_{k} e^{-ikx} \qquad \text{con} \quad
\tilde{C}_{k}  = \frac{ \langle{ \tilde{\phi}}_{k} | {f}  \rangle  }{ \langle{\tilde{\phi}}_{k} |{\tilde{\phi}}_{k}  \rangle} = \frac{1 }{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}x \ e^{-ikx} f(x)\,.
\]

Podremos reescribir (una vez más) la expresión de una suma parcial de la serie de Fourier, dado que 
\[
a_{n}\cos(nx) + b_{n} \operatorname{sen}(nx) = \frac{1}{\pi} \int_{-\pi}^{\pi} \mathrm{d}t  \ f(t) \cos(n[t-x]) \,,
\]
tendremos que
\begin{eqnarray}
F_{n}(x) & = &  \frac{a_{0}}{2} +  \sum_{k=1}^{n} \left[ a_{k} \cos(kx) + b_{k} \mathrm{sen}(kx) \right] =  \frac{a_{0}}{2} +  \sum_{k=1}^{n} \left[ \frac{1}{\pi} \int_{-\pi}^{\pi} \mathrm{d}t  \ f(t) \cos(n(t-x)) \right]  \nonumber \\
 & = & \Re \left[  \int_{-\pi}^{\pi} \mathrm{d}t  \ f(t) \Big\{ \frac{1}{2} + \sum_{k=1}^{n} \left( 
 e^{-i(t-x)k}
 \right) \Big\} \right]  \,.
 \nonumber
\end{eqnarray}
y al sumar la progresión geométrica que representa una serie de exponenciales llegamos a
\[
F_{n}(x)  = \frac{1}{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}t \ f(t)
 \left[ \dfrac{ \operatorname{sen}\left( \left( n +\frac{1}{2} \right)(t -x)  \right)}{ \operatorname{sen}\left( \frac{1}{2}(t -x)  \right)}  \right] \equiv  \frac{1}{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}t \ f(t) \ \mathcal{K}(x,n,t)\,,
\]
la cual siempre es convergente y el término 
\[
\mathcal{K}(x,n,t) = \left[ \dfrac{ \operatorname{sen}\left( \left( n +\frac{1}{2} \right)(t -x)  \right)}{ \operatorname{sen}\left( \frac{1}{2}(t -x)  \right)}  \right] \,,
\] 
se conoce como el núcleo de la transformación de $F$, el \textit{Kernel} de Dirichlet.

La pregunta básica que sigue es, en todos estos casos: ¿cómo se relaciona la expansión de Fourier $\left|  {f}\right\rangle \Leftrightarrow F(x)$ con la función $f(t)$ que genera los coeficientes de la expansión? Nótese que es una forma de mirar una relación entre $F(x) \leftrightarrow f(t)$.  Pasamos de $f(t)$ a $F(x)$ mediante una ``transformación'' 
\[
F_{n}(x)  = \frac{1}{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}t \ f(t) \ \mathcal{K}(x,n,t)\,.
\]

Este tipo de relaciones se denomina transformación integral y en particular ésta es una de las expresiones de las llamadas \textit{Transformadas de Fourier}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=6.9in]{VOLUMEN_2/02_Series/Figuras/Cuadratura_Gauss}
\end{center}
\end{figure}

\subsection{Condiciones de Dirichlet}

Las condiciones que una determinada función $f(x)$ debe cumplir para poder ser representada como una serie de Fourier, se conocen con el nombre de condiciones de Dirichlet\footnote{\textbf{Johann Peter Gustav Lejeune Dirichlet} 1805 - 1859. Matemático Alemán con importantes contribuciones en Teorías de números Algebraica, Series y aproximaciones de funciones y ecuaciones diferenciales parciales.} las cuales pueden ser esquematizadas en los siguientes puntos:
\begin{itemize}
\item la función $f(x)$ debe ser periódica
\item la función $f(x)$ debe se univaluada y continua a trozos (continua menos, en un número finito de puntos) con  un número finito de máximos y mínimos
\item la integral $\int_{-T/2}^{T/2} \mathrm{d}x |f(x)|$ debe ser convergente. Donde $\left[ -T/2, T/2 \right]$ quiere indicar el intervalo de definición de una función con período $T$.
\end{itemize}

Podemos formalizar un poco más las condiciones de Dirichlet en el llamado teorema de Fourier.
\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema de Fourier}: Sea $f(x)$ una función en el intervalo $-\pi \leq x \leq \pi$ y definida para el resto de la recta real tal que cumpla con $f(x +2\pi) = f(x)$. Es decir $f(x)$ es $2\pi-$periódica. Supongamos además que existe la integral
\[
\int_{-\pi}^{\pi} \mathrm{d}x \  f(x)\,, \quad \text{y que} \quad {C}_{k}  = \frac{1 }{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}x \ e^{-ikx} f(x) \quad  \text{con } k=0,\pm1,\pm2,\cdots.
\]  
y si $|f(x)|$ está acotada para un intervalo $[a,b]$ con $-\pi < a \leq x \leq b < \pi$, entonces
\[
F(x) = \sum_{k=-\infty }^{\infty}  {C}_{k} e^{-ikx} \quad \text{es convergente al valor  }
F(x) = \frac{1}{2} \left( \lim_{\epsilon \rightarrow 0_{+}} f(x + \epsilon ) +  \lim_{\epsilon \rightarrow 0_{-}} f(x - \epsilon ) \right)
\]
y si $f(x)$ es continua en $x=x_{0}$ entonces $ F(x_{0}) \rightarrow f(x_{0})$.
\end{mdframed}

En este punto se pueden puntualizar varias cosas:
\begin{enumerate}
\item El valor $F(x) = \frac{1}{2} \left( \lim_{\epsilon \rightarrow 0_{+}} f(x + \epsilon ) +  \lim_{\epsilon \rightarrow 0_{+}} f(x - \epsilon ) \right)$ al cual converge la expansión de Fourier, cobra particular importancia cuando el punto $x =x_{0}$ es una discontinuidad. Tal y como veremos más adelante (sección \ref{FourierDiscontinuidad}) y expresa este teorema, las series de Fourier son particularmente apropiadas para expandir funciones discontinuas (en un número finito de puntos en el intervalo), sin embargo, por ser una base de funciones continuas no puede reproducir la discontinuidad como tal. La expansión de Fourier alrededor de un punto de discontinuidad $x \rightarrow x_{\pm 0}$ tenderá al valor $F(x) \rightarrow F(x_{\pm 0}) \equiv F_{m}$ donde $F_{m} = \frac{F(x_{+ 0}) + F(x_{- 0}) }{2}$. Es decir, tenderá al valor medio de los valores de la discontinuidad por la izquierda $F(x_{- 0}) $ y por la derecha $F(x_{+ 0}) $.  
 \item Si los coeficientes de Fourier tienen variaciones acotadas en el intervalo y $|{C}_{k}  | \rightarrow 0$  con $k \rightarrow \infty $. Entonces 
\[
 \sum_{k=-\infty }^{\infty} | {C}_{k} |^{2} =  \frac{1 }{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}x \ | f(x) |^{2}
 \qquad \Leftrightarrow \qquad \frac{1}{2} a_{0}^{2} +  \sum_{n=1 }^{\infty} | a_{n}^{2} + b_{n}^{2} |=  \frac{1 }{\pi} \int_{-\pi}^{\pi} \mathrm{d}x \ | f(x) |^{2} \,,
\]
que no es otra cosa que la expresión de la completitud de esta base de funciones. 
\end{enumerate}

Para ilustrar esta relación entre la función $f(x)$ y su expansión en serie de Fourier $F(x)$ analicemos el siguiente ejemplo 

La siguiente función, muy conocida en el ámbito de los circuitos eléctrico, se denomina la función onda cuadrada
\[
f(t)=
\left\{
\begin{array}{rl}
    -1  & \text{si } -\frac{1}{2}T \leq t <  0   \\
      &    \\
    +1  & \text{si } 0 \leq t \leq \frac{1}{2}T \,,
\end{array}
\right.
\]

En este caso se puede integrar entre $[0,T/2]$ y luego multiplicar todo por $2$.
\begin{eqnarray*}
a_{0} &=& \frac{2}{T} \int_{0}^{\frac{T}{2}} \mathrm{d}t = 1 \,, \quad 
a_{n} = \frac{2}{T} \int_{0}^{\frac{T}{2}} \cos\left( \frac{2 \pi n t}{T} \right) \mathrm{d}t = 
\frac{\mathrm{sen}\left(n \pi  \right)}{n\pi} =0 \,,\\
b_{n} &=&   \frac{2}{T} \int_{0}^{\frac{T}{2}}\mathrm{sen}\left(\frac{2 \pi n t}{T} \right)\mathrm{d}t 
= \frac{1- \cos\left(n \pi  \right)}{n\pi} = \frac{1- (-1)^{n}}{n \pi}\,,
\end{eqnarray*}

Entonces solo sobreviven los $b_{2n+1}$ ya que coeficientes pares se anulan:  $b_{2n}=0$.
\[
f(t)= {a_0} + 2\sum_{n=1}^{\infty} b_n \mathrm{sen}\left(\frac{2 \pi n t}{T} \right)= 
1+\frac{4}{\pi} \left(\mathrm{sen}( \omega t) + \frac{\mathrm{sen}(3 \omega t)}{3} + 
\frac{\mathrm{sen} (5 \omega t)}{5} +  \frac{\mathrm{sen}(7 \omega t)}{7} + \cdots \right)
\]
donde hemos denotado $\omega = 2 \pi/ T $. 

Al definir la función $\omega$ podemos interpretar los coeficientes de Fourier $a_{n},b_{n}$ como las contribuciones de cada uno de los armónicos $a_{n},b_{n} \rightarrow \omega_{n} = \frac{2 n \pi}{T} $. A partir de estas contribuciones se construye el espectro de potencia, el cual está relacionado con la energía que aporta cada uno de estos armónicos. Por ello construimos un cantidad $E_{n} = \sqrt{a_{n}^{2} + b_{n}^{2}}$ y graficamos $E_{n}$ vs $n$ tal y como se puede comprobar en la figura \ref{FigSeriesFourier}, cuadrantes IV y VII. Se encuentra que se puede asociar un espectro de potencia a cada señal y con lo cual realizar una especie de identificación.

En este punto podemos hacernos algunas preguntas: 
\begin{itemize}
 \item ¿qué hubiera pasado si en vez de considerar el intervalo $\left( -\frac{T}{2},\frac{T}{2} \right)$ hubiéramos considerado $\left( 0, T \right)$?
 \item ¿tendríamos el mismo desarrollo en serie de Fourier?
 \item ¿el mismo espectro?
\end{itemize}
Justifique sus respuestas.


\subsection{Consideraciones de simetría en series de Fourier}
Es de hacer notar que estas propiedades de simetría respecto al período de la función ($f(x) = f(-x)$ simetría y $f(x) = -f(-x)$ antisimetría) para un período $-\frac{T}{2} \leq x \leq \frac{T}{2}$ pueden y deben ser explotadas para simplificar los cálculos. Esto se puede resumir en 
\[
f(x) = f(-x) \,\, \Rightarrow \,\,
\left\{ 
\begin{array}{l}
  a_{n} \neq 0      \\
  b_{n} = 0        
\end{array}
\right. \quad \text{y alternativamente} \quad
f(x) = -f(-x) \,\, \Rightarrow \,\, 
\left\{ 
\begin{array}{ l}
  a_{n} = 0      \\
  b_{n}  \neq 0        
\end{array}
\right.
\]

Pero más interesante aún es cuando estas propiedades de simetría se presentan en un cuarto  del período. Vale decir, que $f(x)$ será par o impar respecto a $T/4$ i.e. $f\left( \frac{T}{4} +x \right) = \pm f\left( \frac{T}{4} -x \right) \Rightarrow f(-s) = \pm f(s)$ donde $s = \frac{T}{4} -x$. Entonces
\[
 b_{n} =   \frac{2}{T} \int_{x_{0}}^{x_{0} + T} \mathrm{d}s \ f( s) \ \mathrm{sen} \left( \frac{2 \pi ns}{T}  + \frac{\pi n}{2} \right) \,.
\]

Donde los límites de integración no se han visto alterados porque la función es periódica. Es inmediato comprobar que 
\[
\mathrm{sen} \left( \frac{2 \pi ns}{T}  + \frac{\pi n}{2} \right)  =
\mathrm{sen} \left( \frac{2 \pi ns}{T} \right)  \cos \left( \frac{\pi n}{2} \right) + 
 \cos \left( \frac{2 \pi ns}{T} \right)   \mathrm{sen} \left( \frac{\pi n}{2} \right) \,,
\]
es decir 
\[
b_{n} =   \frac{2}{T} 
\left[
\cos \left( \frac{\pi n}{2} \right) \int_{x_{0}}^{x_{0} + T} \mathrm{d}s \ f( s) \mathrm{sen} \left( \frac{2 \pi ns}{T} \right)   + 
\mathrm{sen} \left( \frac{\pi n}{2} \right) \int_{x_{0}}^{x_{0} + T} \mathrm{d}s \ f( s) \ \cos \left( \frac{2 \pi ns}{T} \right)   
\right] \,,
\]
por lo que si $n =2k \Rightarrow \mathrm{sen} \left( \frac{\pi n}{2} \right) = \mathrm{sen}( \pi k ) = 0 $  y si $n =2k-1 \Rightarrow \cos \left( \frac{ 2k-1}{2} \pi\right) = 0 $. 

La misma consideración se puede hacer para los coeficientes $a_{n}$ (queda como ejercicio para el lector) y se puede concluir que 
\begin{itemize}
  \item Si $f(x)$ par en $T/4$ entonces $a_{2n -1} = b_{2n} = 0$.
  \item Si $f(x)$ impar en $T/4$ entonces $a_{2n} = b_{2n-1} = 0$.
\end{itemize}


\subsection{El Fenómeno de Gibbs}
\label{FourierDiscontinuidad}

Tal y como hemos mencionado, a diferencia de las series de potencias, las series de Fourier manejan razonablemente bien las discontinuidades, pero por ser una base de funciones continuas, no puede reproducirlas. Tal y como comentamos en el Teorema de Fourier  y muestra la figura \ref{EscalonFourier} el valor de las sumas parciales de Fourier en un punto de discontinuidad $x = x_{\pm 0}$ será el promedio de los valores $F(x_{- 0}) $ (por la izquierda) y $F(x_{+ 0}) $ (por la derecha)  en la discontinuidad.  Esto es la expansión de Fourier alrededor de un punto de discontinuidad $x \rightarrow x_{\pm 0}$ tenderá al valor $F(x) \rightarrow F(x_{\pm 0}) \equiv F_{m}$ donde $F_{m} = \frac{F(x_{+ 0}) + F(x_{- 0}) }{2}$. 


Podemos ver  en la figura \ref{EscalonFourier} que, tanto por la izquierda como por la derecha de la discontinuidad de la función escalón, las sumas parciales de Fourier oscilan y no convergen a los valores $x_{\pm 0}$. El comportamiento oscilante de las sumas parciales de Fourier alrdedor de las discontinuidades, que no desaparecen ni en el límite se   denominan \textit{fenómeno de Gibbs} en honor a su descubridor Josiah Willard Gibbs.\footnote{\textbf{Josiah Willard Gibbs} 1839 - 1903. Algunos lo consideran el primer Físico Norteamericano, de hecho fue el primero en recibir un título de doctorado por una universidad norteamericana (Yale University). Hizo importantes aportes en electromagnetismo  y sobre todo en termodinámica y física estadística, sentando las bases matemáticas para estas disciplinas. En matemáticas es conocido su estudio de las oscilaciones de las expansiones de las series de Fourier en los puntos de discontinuidad.}

Para entender qué pasa en la discontinuidad consideremos una variación de la onda cuadrada considerada anteriormente (\ref{FourierEjemplos}). Entonces sus sumas parciales serán
\[
f(t)=
\left\{
\begin{array}{rl}
    1  & \text{si } 0 \leq t <  \pi   \\
      &    \\
    0  & \text{si } \pi \leq t < 2 \pi      
\end{array}
\right. \,\, \Rightarrow \,\, F_{2n}^{c}(x) = \frac{1}{2} + \frac{2}{\pi} \sum_{k=1}^{n} \frac{1}{2k - 1}
\mathrm{sen}\left((2k - 1)x\right) \,, 
\]
porque los coeficientes pares ($a_{n}$) se anulan. 

Para estudiar el fenómeno de Gibbs reescribimos la suma parcial anterior de una manera ingeniosa 
\[
F_{2n}^{c}(t) =   \frac{1}{2} +  \frac{2}{\pi} \sum_{k=1}^{n} \left( \int_{0}^{t} \mathrm{d}s \ \cos(2k -1)s \right) =  
 \frac{1}{2} +  \frac{2}{\pi} \int_{0}^{t} \mathrm{d}s \left( \sum_{k=1}^{n} \cos(2k -1)s  \right) = 
  \frac{1}{2} +  \frac{1}{\pi} \int_{0}^{t} \mathrm{d}s \left( \frac{ \mathrm{sen}(2ns)}{ \mathrm{sen}( s)} \right)
\]
donde, utilizando la fórmula de Moivre y convirtiendo esa serie de cosenos en una de exponenciales la cual, a su vez es una progresión geométrica (y le queda la comprobación al lector), hemos sustituido
\[
\sum_{k=1}^{n} \cos(2k -1)s = \frac{ \mathrm{sen}(2ns)}{ \mathrm{sen}( s)}\,.
\]

Es inmediato convencerse que las sumas parciales $F_{2n}^{c}(x)$ siempre tendrán máximos y mínimos 
\[
\dfrac{ \mathrm{d} F_{2n}^{c}(x)}{ \mathrm{d}x} =  \frac{ \mathrm{sen}(2nx)}{ \mathrm{sen}( x)} = 0 
\\,\, \Rightarrow \,\, \text{para  }
x = \frac{m \pi}{2n } \quad \text{con } m=1,2,3,\cdots 
\]

Las Series de Fourier tienden a sobre-estimar el valor de los puntos de discontinuidad en $\pm 18 \%$ esto es un valor de $\approx 1.1789797$. La inclusión de más términos en las sumas parciales no mejoran la situación. El fenómeno de Gibbs no se restringe a Series de Fourier sino que también se presenta en las demás series de funciones (ver detalles en la referencia: {\tt Arfken-Weber-2000}) .

El fenómeno de Gibbs fue observado ¡experimentalmente! por primera vez por Albert Michelson.\footnote{\textbf{Albert Abraham Michelson} Strelno, Prussia, 1852 - Pasadena EEUU. 1931. Premio Nobel en Física (1907) uno de los físicos experimentales más habilidosos de todos los tiempos. La precisión y lo ingenioso de los instrumentos creados por él son famosos.  Con importantes contribuciones en medidas de fenómenos en óptica. Una de sus contribuciones más conocidas son los experimentos para mostrar la inexistencia del Ether como medio de transmisión para el fenómeno electromagnético. Más detalles \url{http://nobelprize.org/physics/laureates/1907/michelson-bio.html}}.  Para finales de 1800 Michelson había creado un dispositivo mecánico para medir las componentes de Fourier de señales eléctricas. Al incorporarle una onda cuadrada observó que una oscilación inesperada en los puntos de discontinuidad. Creyó que esa oscilación se debía a defectos del dispositivo. Luego de probar múltiples tipos de señales periódicas y observar un comportamiento similar, decidió comentárselo a su amigo Willard Gibbs, de la Universidad Yale. Al poco tiempo Gibbs volvió con una explicación que dejó intacta la fama de Michelson como instrumentista. El fenómeno es una consecuencia de la teoría de series de Fourier y no del equipo diseñado por Michelson\footnote{Más detalles \url{http://en.wikipedia.org/wiki/Gibbs_phenomenon}}. 

\begin{figure}[t]
\begin{center}
\includegraphics[width=4.0in]{VOLUMEN_2/02_Series/Figuras/escalonfourier.eps}
\caption{ Aproximaciones por series de Fourier para la función escalón, linea roja.  Las curvas corresponden a sumas parciales de Fourier: $F_{40}(x), F_{100}(x), F_{200}(x), $ }
\label{EscalonFourier}
\end{center}
\end{figure}


\subsection{Corrección al fenómeno de Gibbs: Factor $\sigma$ de Lanczos}
Una de las estrategia para corregir las oscilaciones del fenómeno de Gibbs se le debe a Lanczos\footnote{\textbf{Cornelius Lanczos} 1893 - 1974 Hungría. Matemático húngaro con contribuciones importante en Relatividad y Física Teórica. En matemáticas es conocido inventar la transformada rápida de Fourier. Más detalles en \url{http://www-history.mcs.st-and.ac.uk/Biographies/Lanczos.html}}. Considerando el mismo caso de la función onda cuadrada, se puede intentar sustituir la función oscilante $F_{n}^{c}(x)$ por su promedio $\bar{F}_{n}^{c}(x)$ alrededor del punto $x$. Vale decir
\[
F_{2n}^{c}(x) \rightarrow \bar{F}_{2n}^{c}(x) = \frac{n}{\pi}  \int_{x - \frac{\pi}{2n} }^{x + \frac{\pi}{2n} } \mathrm{d}s \ F_{2n}^{c}(s) =  \frac{n}{\pi}  \int_{x - \frac{\pi}{2n} }^{x + \frac{\pi}{2n} } \mathrm{d}s \left[ \frac{1}{2} + \frac{2}{\pi} \sum_{k=1}^{n} \frac{1}{2k - 1} \mathrm{sen}((2k - 1)s) \right]\,,
\]
desarmando tendremos que 
\begin{eqnarray}
\bar{F}_{2n}^{c}(x) & = &   \frac{n}{\pi}  \int_{x - \frac{\pi}{2n} }^{x + \frac{\pi}{2n} } \mathrm{d}s \left[ \frac{1}{2} + \frac{2}{\pi} \sum_{k=1}^{n} \dfrac{1}{2k - 1} \mathrm{sen}((2k - 1)s) \right] 
\nonumber \\
 & = &  \frac{n}{\pi} \left[ \frac{\pi}{2n} + \frac{2}{\pi} \sum_{k=1}^{n}  \left. \dfrac{1}{(2k - 1)^{2}} \cos((2k-1)s) \right|_{x - \frac{\pi}{2n} }^{x + \frac{\pi}{2n} }  \right] \nonumber \\
\bar{F}_{2n}^{c}(x) & = &  \frac{1}{2} + \frac{2}{\pi}  \sum_{k=1}^{n}  \dfrac{1}{2k - 1}
\underbrace{ \left[ \dfrac{ \mathrm{sen}\left( \frac{\pi}{2n} (2k - 1) \right) }{\frac{\pi}{2n}(2k - 1)}\right] }_{\sigma} \mathrm{sen}((2k - 1)x) \,.
\nonumber
\end{eqnarray}

Con lo cual hemos identificado el factor $\sigma$ de Lanczos. Siguiendo este mismo proceso se puede generalizar para cualquier función de tal modo que una serie de Fourier genérica podrá ser corregida con un factor $\sigma$ para lograr
\[
\bar{F}_{n}(x) =  \frac{a_{0}}{2} +  \sum_{k=1}^{n-1} 
\left[ \dfrac{ \mathrm{sen}\left( \frac{k\pi}{n} \right) }{\left( \frac{k\pi}{n} \right) } \right]
 \left( a_{k} \cos (kx) + b_{k} \mathrm{sen} (kx) \right) \equiv
  \frac{a_{0}}{2} +  \sum_{k=1}^{n-1} \sigma_{k} \left( a_{k} \cos (kx) + b_{k} \mathrm{sen} (kx) \right)\,.
\]


\subsection{{\color{Fuchsia}Ejemplos}}

\begin{enumerate}
\item {\bf Variedades de dientes de sierra}

Otra función muy común es la denominada dientes de sierra
\[
f(t)= a t  \quad \text{si } 0 \leq t \leq  T  \,, \quad \text{con } a \text{ constante }
\] 
los coeficientes son los siguientes:
\begin{eqnarray*}
a_{0} &=& \frac{2}{T} \int_{0}^{T}a t \mathrm{d}t  = a T \,,\\
a_{n} &=& \frac{2}{T} \int_{0}^{T}a t \cos \left( \frac{2 \pi nt}{T}\right) \mathrm{d}t  = 
{\frac {a\,T}{{\pi }^{2}{n}^{2}}} \left[ n \pi \mathrm{sen} \left( 2 n\,\pi \right) -
\mathrm{sen}^2 \left(n \pi  \right)\right]=0\,,\\
b_{n} &=& \frac{2}{T} \int_{0}^{T}a t\,\mathrm{sen}\left(\frac{2\pi nt}{T}\right)\mathrm{d}t  = 
-\frac {a T}{n\,\pi} \,.
\end{eqnarray*}

Tenemos entonces que
\[
f(t)= at=\frac{a_0}{2} + \sum_{n=1}^{\infty} b_n \mathrm{sen}\left(\frac{2 \pi n t}{T} \right)= 
\frac{aT}{2} - \frac{aT}{\pi} \sum_{n=1}^{\infty} \frac{\mathrm{sen}\left(\omega n t \right)}{n} \,
 \,,\quad \text{para } 0 \leq t \leq  T \,.
\]

\begin{figure}[t]
\begin{center}
\includegraphics[width=6.0in]{VOLUMEN_2/02_Series/Figuras/VariosFourier}
\caption{Un par de funciones, definidas con un período $T$, a ser expresadas en como expansiones en Series de Fourier. En los cuadrantes I y II, encontramos una onda cuadrada. La primera (cuadrante I) definida en un intervalo $\left( -\frac{T}{2}, \frac{T}{2} \right)$  y en el cuadrante II la misma función definida en un intervalo $( 0, T )$. El cuadrante III ilustra las aproximaciones de la serie de Fourier para $n=3,7,20$, mientras que el espectro de potencia se presenta en el cuadrante IV. La onda ``diente de sierra'', definida en un intervalo $\left( 0, T \right)$, se presenta en el cuadrante V. Sus aproximaciones en series de Fourier para $n=3,7,10$ se pueden observar en el cuadrante VI, mientras que el espectro de potencia en el cuadrante VII. }
\label{FigSeriesFourier}
\end{center}
\end{figure}


En el caso particular de hacer $a =3$ y $T = 2 \rightarrow \omega_{n} = n \pi $, entonces:
\[
f(t)= 3t= 3 - \frac{6}{\pi} \sum_{n=1}^{\infty} \frac{\mathrm{sen}\left(n \pi t \right)}{n} = 
3- {\frac {6\mathrm{sen} \left( \pi \,t \right) }{\pi }} - {\frac {3 \mathrm{sen} \left( 2\,\pi \,t \right) }{\pi }} - {\frac {2\mathrm{sen} \left( 3\,\pi \,t \right) }{\pi }} - {\frac {3\mathrm{sen} \left( 4\,\pi \,t \right) }{2\pi }}
- {\frac {6 \mathrm{sen} \left( 5\,\pi \,t \right) }{5 \pi }} + \cdots
\]

La figura \ref{FigSeriesFourier} (cuadrantes V y VI) muestra la construcción de esta función y su representación en Series de Fourier. 

A partir de esta función podemos hacer unas variaciones. Por ejemplo considérese la función 
\[
f(t)= a t  \quad \text{si } \frac{-T}{2} \leq t \leq  \frac{T}{2} \,, \quad \text{con } a \text{ constante }
\quad \Rightarrow 
\left\{
\begin{array}{cll}
   a_{0} =  & \frac{2}{T}\int_{-T/2}^{T/2}a t \mathrm{d}t  & = 0 \\   \\
   a_{n} =  & \frac{2}{T}\int_{-T/2}^{T/2}a t \cos\left(\frac{2\pi nt}{T}\right) \mathrm{d}t & = 0\\  \\
   b_{n} =  & \frac{2}{T}\int_{-T/2}^{T/2}a t\,\mathrm{sen}\left(\frac{2\pi nt}{T}\right)\mathrm{d}t & = 
   -\frac {a T(-1)^{n}}{n\,\pi}\,.
\end{array}
\right. 
\] 

Claramente es una función impar $f(-x) = -f(x)$ y así lo refleja su expansión en series de Fourier. Si hacemos $ a =3$ y $T = 2 \rightarrow \omega_{n} = n \pi $ tendremos que la expresión para de la serie es 
\[
f(t)= 3 t = {\frac {6\mathrm{sen} \left( \pi \,t \right) }{\pi }} -{\frac {3 \mathrm{sen} \left( 2\,\pi \,t \right) }{\pi }} + {\frac {2 \mathrm{sen} \left( 3\,\pi \,t \right) }{\pi }} - {\frac{3 \mathrm{sen} \left( 4\,\pi \,t \right) }{2\pi }} + {\frac {6\mathrm{sen} \left( 5\,\pi \,t \right) }{5\pi }} + \cdots
 \quad \text{con } \frac{-T}{2} \leq t \leq  \frac{T}{2} \,,
\] 
la cual, si bien es parecida no es igual a la anterior, debido que estamos expandiendo otra función.

Otra variación posible de la función ``diente de sierra'' puede ser la versión completamente par  del ``diente'', $f(-x) = f(x)$. Esta es
\[
f(t)= 
\left\{
\begin{array}{l}
   -a t  \quad \text{si } \frac{-T}{2} \leq t \leq  0  \\   \\
  a t  \quad \text{si }          0 \leq t \leq  \frac{T}{2}  
\end{array}
\right. 
\]

El cálculo de los coeficientes resulta en: 
\begin{eqnarray*}
a_{0} &=& \frac{2}{T}\int_{-T/2}^{0}(-a t) \mathrm{d}t+\frac{2}{T} \int_{0}^{T/2}a t\mathrm{d}t = 
\frac{a T}{2} \,,\\
a_{n} &=& \frac{2}{T} \int_{-T/2}^{0}(-a t) \cos \left( \frac{2 \pi nt}{T}\right) \mathrm{d}t+
\frac{2}{T} \int_{0}^{T/2}a t \cos \left( \frac{2 \pi nt}{T}\right) \mathrm{d}t  = 
{\frac {a\,T}{{\pi }^{2}{n}^{2}}} \left[ \left( -1 \right)^{n}-1 \right]\,,\\
b_{n} &=& \frac{2}{T} \int_{-T/2}^{0}(-a t)\,\mathrm{sen}\left(\frac{2\pi nt}{T}\right)\mathrm{d}t+
\frac{2}{T} \int_{0}^{T/2}a t\,\mathrm{sen}\left(\frac{2\pi nt}{T}\right)\mathrm{d}t  = 0 \,.
\end{eqnarray*}

En este caso son los  coeficiente $b_n$ los que se anulan. 

Adicionalmente, nótese que para $n$ par, los coeficientes  $a_n$ también se anulan, Otra vez, si hacemos $ a =3$ y $T = 2 \rightarrow \omega_{n} = n \pi $ tendremos la serie:
\[
f(t) =  \frac32 - {\frac {12 \cos \left( \pi \,t \right) }{{\pi }^{2}}}-{ \frac{4\cos \left( 3\,\pi \,t \right) }{{3\pi }^{2}}} -{\frac {12}{25}}\,{\frac {\cos \left( 5\,\pi \,t \right) }{{\pi }^{2}}} + \cdots
 \quad \text{con } \frac{-T}{2} \leq t \leq  \frac{T}{2} 
\]

\item{\bf Función cuadrática}

Otro caso, complementario al anterior por sus propiedades de simetría, es la expansión en series de Fourier de la función $f(x)= x^{2}$ para $-\pi <  x  < \pi$. Entonces los coeficientes se la expansión serán 
\[
f(x)= x^{2} \,\, \Rightarrow \,\, 
\left\{ 
\begin{array}{ll}
  a_{0} = \frac{1}{\pi} \int_{-\pi }^{\pi}  \ x^{2} \mathrm{d}x & = \frac{2 \pi^{2}}{3}    \\ 
    	&  	\\
  a_{n} = \frac{2}{\pi} \int_{0}^{\pi} \ x^{2} \cos(nx) \mathrm{d}x  & =   \frac{4(-1)^{n}}{n^{2}}   
\end{array}
\right.
\] 
ya que los coeficientes correspondientes a los términos impares $b_{n}$ se anulan. Con lo cual
\[
x^{2} = \frac{ \pi^{2}}{3} + 4 \sum_{n=1}^{\infty} \frac{(-1)^n \cos(nx) }{n^{2}} \,.
\]



Nótese que como un resultado particular, al evaluar en $x= \pi$, se tiene la función zeta de Riemann $\zeta(2)$
\[
\pi^{2} = \frac{\pi^{2}}{3} + 4\sum_{n=1}^{\infty} \frac{1}{n^{2}} \,\, \Rightarrow \,\,
\zeta(2) \equiv \sum_{n=1}^{\infty} \frac{1}{n^{2}} =  \frac{ \pi^{2}}{6} \,.
\]

Pero este caso se presta también para considerar funciones no periódicas. Supongamos que queremos desarrollar la expansión de Fourier para $f(t)= t^{2}$ pero en este caso con $0 <  t  < 2 $. Si este fuera el caso, empezamos por suponer que la función tienen un período, digamos $T =4$. Esto es $-2 \leq t \leq 2$. Con lo cual
\begin{eqnarray}
 a_{0} & = & \frac{2}{4} \int_{-2 }^{2} t^{2}\mathrm{d}t =
 \frac{4}{4} \int_{0}^{2} t^{2} \mathrm{d}t =\frac{8}{3}  \nonumber \\
a_{n} & = &  \frac{2}{4} \int_{-2}^{2} t^{2} \cos \left( \frac{2 \pi nt}{4} \right) \mathrm{d}t  =  
 \frac{4}{4} \int_{0}^{2}  t^{2} \cos \left( \frac{ \pi nt}{2} \right)\mathrm{d}t   =  \frac{16}{\pi^{2} n^{2}} \cos(n\pi) = \frac{16}{ \pi^{2} n^{2}}(-1)^{n}   \,.
 \nonumber 
\end{eqnarray}

Con lo cual tendremos que 
\[
t^{2}= \frac{4}{3} + 16 \sum_{n=1}^{\infty} \frac{(-1)^n }{\pi^{2} n^{2}} \cos \left( \frac{ \pi nx}{2}  \right) \quad 
\text{para } 0 < t \leq 2 \,.
\]



\end{enumerate}




\subsection{{\color{red}Practicando con Maxima}}

\subsection{{\color{OliveGreen}Ejercicios}}


\begin{thebibliography}{9}

\bibitem[Aleksandrov Kolmogorov y Lavrentiev 1999]{AleksandrovKolmogorovLavrentiev1999}A. D. Aleksandrov, A. N. Kolmogorov y M. A. Lavrentiev (1999) \textbf{Mathematics: Its Content, Methods and Meaning.} (\textit{Dover Publications, New York}) Existe traducción por Editorial Alianza Universidad.

\bibitem[Arfken, Weber y Weber 2000]{ArfkenWeberWeber2000}Arfken, G. B., Weber, H., y Weber, H.J. (2000)
\textbf{Mathematical Methods for Physicists} 5ta Edición (\textit{Academic Press, Nueva York})

\bibitem{ByronFuller1970}Byron, F.W. y Fuller W.F. (1970) \textbf{Mathematics of Classical and Quantum Physics } (\textit{Dover Publications, New York})

\bibitem[Cushing 1975]{Cushing1975}Cushing, J. (1975)\textbf{ Applied Analytical Mathematics for Physical Sciences } (\textit{John Wiley \& Sons, New York})

\bibitem[Hamming 1973]{Hamming1973} Hamming R.W. (1973) \textbf{Numerical Methods For Scientist and Engineers, 2nd ed. } (Dover, New York.)

\bibitem[Hassani 1991]{Hassani1991}  Hassani, S. (1991) \textbf{Foundations of Mathematical Physics} 
(\textit{Prentice Hall, International Edition, London:})

\bibitem[Lebedev 1972]{Lebedev1972} Lebedev, N.N. (1972) \textbf{Special Functions \& Their Applications} (\textit{Dover Publications, New York})

\bibitem[math-atlas.org URL]{math-atlas.org} \textbf{The Mathematical Atlas} \url{http://www.math-atlas.org/welcome.html}

\bibitem[Richards 2002]{Richards2002} Richards, D. (2002) \textbf{Advanced Mathematical Methods with MAPLE} (\textit{Cambridge University Press Cambridge})

\bibitem[Riley Hobson y Bence 2002]{RileyHobsonBence2002}  Riley, K.F., Hobson, M.P. y Bence, S.J. (2002)
\textbf{Mathematical Methods for Physics and Engineering } (\textit{Cambridge University Press Cambridge})

\bibitem[Weisstein URL]{WeissteinURL} Weisstein, E. W.,  \textbf{MathWorld} \url{http://mathworld.wolfram.com/}

\end{thebibliography}


  