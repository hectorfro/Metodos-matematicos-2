\documentclass[spanish,titlepage,letterpaper]{article} % para articulo en castellano
\usepackage[ansinew]{inputenc} % Acepta caracteres en castellano
\usepackage[spanish]{babel} % silabea palabras castellanas
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[colorlinks=true,urlcolor=blue,linkcolor=blue]{hyperref} % navega por el doc
\usepackage{graphicx}
\usepackage{geometry}      % See geometry.pdf to learn the layout options.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{epstopdf}
\usepackage{fancyhdr} % encabezados y pies de pg

\pagestyle{fancy} 
\chead{\bfseries Formulario de Métodos Matemáticos 1} 
\lhead{} % si se omite coloca el nombre de la seccion
\rhead{} 
\lfoot{\it Luis A. Núñez  } 
\cfoot{ Universidad de Los Andes, Mérida, Venezuela} 
\rfoot{\thepage} 

\voffset = -0.25in 
\textheight = 8.0in 
\textwidth = 6.5in
\oddsidemargin = 0.in
\headheight = 20pt 
\headwidth = 6.5in
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0,5pt}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\begin{document}
\title{\textbf{Series de Series...\footnote{\textbf{ADVERTENCIA: El presente documento constituye una guía para los estudiantes de Métodos Matemáticos de la Física de la Universidad de Los Andes. Es, en el mejor de los casos, un FORMULARIO y de ninguna manera sustituye a los líbros de texto del curso. La bibliografía de la cual han surgido estas notas se presenta al final de ellas y debe ser consultada
por los estudiantes.}}}}
\author{\textbf{L. A. Núñez\thanks{e-mail: \texttt{nunez@ula.ve }\qquad Web:
\url{http://webdelprofesor.ula.ve/ciencias/nunez/}}} \\ 
\textit{Centro de Física Fundamental,} \\ 
\textit{Departamento de Física, Facultad de Ciencias, }\\
\textit{Universidad de Los Andes, Mérida 5101, Venezuela} y \\ 
\textit{Centro Nacional de Cálculo Científico, Universidad de Los Andes,}\\ 
\textsc{(CeCalCULA),}  \\
\textit{Corporación Parque Tecnológico de Mérida, Mérida 5101, Venezuela}}
\date{Versión $\beta$ 1.0 Mayo 2006}
\maketitle
\tableofcontents

\section{Series por todos lados}
\label{IntroSeries}
Las series o sucesiones se nos presentan casi por todos lados en Física. Cuando no sabemos resolver un problema analíticamente, lo más cercano serán las soluciones por series. Pero más que eso, las series nos ayudarán a definir funciones y a estudiar su continuidad o derivabilidad. 

Representaremos unas serie como
\[
S_{i}=\sum_{n=1}^{i}a_{n} \hspace{1cm} 
\left\{
\begin{array}
[l]{l}
	i = N  \qquad \Longrightarrow \quad \text{la serie es finita con}\ N \ \text{elementos} \\
	i \rightarrow \infty \qquad \Longrightarrow \quad \text{la serie es infinita}%
\end{array}
\right.
\]
Nos van a interesar las series infinitas. Una serie infinita $S_{\infty}$ la podremos separar en sumas parciales finitas $S_{i}$, y si la suma parcial converge a un número finito $S$ cuando $i \rightarrow \infty$ diremos que la serie \textit{converge}. Si no, diremos que \textit{diverge}.  Se dirá que la serie \textit{diverge} si el valor de la sumatoria aumenta indeteniblemente, pero también puede oscilar, con lo cual tampoco converge.
\[
S_{i}=\sum_{n=1}^{i} ( -1 )^{n}= 1-1+1-1+ \cdots + (-1 )^{i}+ \cdots 
\] 
Esto se puede formalizar un poco diciendo que la condición para la existencia de un límite $S$ es que para cada $\epsilon > 0$ existe un número $N=N(\epsilon)$ tal que
\[
\| S - S_{i} \| < \epsilon \quad \text{para} \ i > N
\]
Esta afirmación se puede derivar del llamado \textbf{criterio de Cauchy} \footnote{Augustin Louis Cauchy Paris, 1789 - 1857, matemático francés pionero en los estudios de análisis (real y complejo) y de la Teoría de los Grupos de Permutación. Cauchy hizo aportes importantes en los criterios de convergencia y divergencia de series infinitas, así como tambien, en ecuaciones diferenciales, determinantes, probabilidades y Física Matemática } sobre la convergencia de las series parciales. Esto es que la condicioón necesaria y suficiente para que una suma parcial $S_{i}$ converja es, que
\[
\| S_{j}- S_{i} \| < \epsilon \quad \text{para, todo } \ i,j > N
\]
que quiere decir que las sumas parciales convergen a medida que avanzamos en los términos de la serie.

\subsection{La Suma de la Serie}
 De las series nos intereserá conocer cuanto suman. Es decir cuál es el valor de $S_{i}$ para una serie finita cuando $i = N$ Pero también estaremos interesados en conocer cuanto suma una serie infinita. Empecemos con las finitas. 
\subsubsection{Las Series de Siempre}
 De siempre hemos conocido algunas series emblemáticas

 \paragraph{Serie aritmétrica}
 Desde siempe hemos oído hablar de progresiones aritméticas. Ellas son, sencillamente
  \[
  S_{N}=\sum_{n=0}^{N-1} (a+nd)= a+(a+d)+(a+2d)+(a+3d)+(a+4d)+\cdots+  \left[a +(N-1)d \right].
  \]
  Es fácil comprobar que al desarrollar la serie en orden inverso y sumar ambas
  \[
 \begin{array}{rcccccc}
S_{N}= & a & +(a+d) & +(a+2d) & +(a+3d)  & +\cdots &+ \left[a +(N-1)d \right] \\
S_{N}= & \left[a +(N-1)d \right] & + \left[a +(N-2)d \right] & + \left[a +(N-3)d \right] & + \left[a +(N-4)d \right] & +\cdots &+ a
 \end{array} 
  \]  
  Con lo cual
  \[
   S_{N}=\frac{N}{2}\left[a +a + (N-1 d) \right] \rightarrow S_{N}= \frac{N}{a}\left[\text{Primer Término} + \text{Ultimo Término} \right]
\] 
obviamente, si $N \rightarrow \infty$ la serie diverge.
 
\paragraph{Serie Geométrica}
De ésta también sabemos desde siempre....
  \[
  S_{N}=a +ar +ar^{2}+ar^{3}+\cdots+ar^{N-1}=\sum_{i=0}^{N}ar^{i} 
  \]  y si restamos
 \begin{equation}
 \begin{array}{rllllll}
S_{N}= & a & +ar & +ar^{2} & +ar^{3}  & +\cdots &+ ar^{N-1} \\
rS_{N}= & ar & + ar^{2} & + ar^{3} & +ar^{4} &+ \cdots &+ ar^{N}
 \end{array} \nonumber
\end{equation} 
también es inmediato comprobar que si $\|r\|<1$
\[ 
S_{N}=\frac{a(1-r^{N})}{1-r} \quad \text{con lo cual tendremos que la suma de la serie será} S = \lim_{N \rightarrow \infty}S_{N} =   \frac{a}{1-r}
\]
y,  divergerá (u oscilará) si $\|r\| \geqslant 1$ 
\paragraph{ Series \textbf{Aritmético-geométricas}}
 Estas series, un poco más exóticas y como su nombre lo sugiere son una combinación de las anteriores. Estos es
  \[
   S_{N}= a+(a+d)r+(a+2d)r^{2}+(a+3d)r^{3}+(a+4d)r^{4}+\cdots+  \left[a +(N-1)d \right]r^{N} =\sum_{n=0}^{N-1} (a+nd)r^{n}
  \] y con la misma estrategia de las geométricas se llega a encontrar el valor de su, nada intuitiva, suma 
  \[
   S_{N}=\frac{a -\left[ a + (N-1)d \right]r^{N}}{1-r} + \frac{ rd(1 -r^{N-1} )}{(1-r)^{2}}  
  \] Otra vez, si si $\|r\|<1$ entonces cuando $N \rightarrow \infty$ 
  \[
  S=\frac{a}{1-r} +\frac{rd}{(1-r)^{2}}
  \] 

Algunos ejercicios (respectivos) de las situaciones anteriores lo constituyen 
\begin{itemize}
  \item[\textbf{Ejercicio}] Encuentre la suma de los 100 primeros enteros
  \item[\textbf{Ejercicio}] Encuentre la distancia total que recorre una pelota que rebota verticalmente y que en cada rebote pierde 2/3 de su energía cinética
  \item[\textbf{Ejercicio}] Encuentre la suma de la serie $S=2+\frac{5}{2} + \frac{8}{4} +  \frac{11}{8} +\cdots $
\end{itemize}

\paragraph{Serie Harmónica} Quizá no la conocíamos con este nombre (y menos por sus propiedades) pero seguro nos la hemos tropezado
\[
1+ \frac{1}{2} +\frac{1}{3} +\frac{1}{4} +\frac{1}{5} +\cdots\frac{1}{n} +\cdots = \sum_{n=1}^{\infty} \frac{1}{n}
\]
Esta serie es engañosa, en apariencia parece converger, pero no es así. Si analizamos con más cuidado, veremos que hay sutilezas
\[
\sum_{n=1}^{\infty} \frac{1}{n} = 1  + \underbrace{\frac{1}{2}}_{s_{0}} 
+ \underbrace{ \left( \frac{1}{3} +\frac{1}{4} \right) }_{s_{1}} 
+ \underbrace{\left( \frac{1}{5} +\frac{1}{6}  +\frac{1}{7}  +\frac{1}{8}   \right)}_{s_{2}} 
+ \underbrace{\left( \frac{1}{9} +\frac{1}{10}  + \cdots  +\frac{1}{16}   \right)}_{s_{3}} + \cdots 
\]
y puede ser reescrita como
\[
1 
+  \underbrace{ \frac{1}{1+1} }_{s_{0}} 
+  \underbrace{ \frac{1}{2+1} + \frac{1}{2+2}}_{s_{1}} 
+  \underbrace{ \frac{1}{4+1} + \frac{1}{4+2} + \frac{1}{4+3} + \frac{1}{4+4}}_{s_{2}} 
+  \underbrace{ \frac{1}{8+1} + \frac{1}{8+2} + \cdots +\frac{1}{8+8} }_{s_{3}} +\cdots
+ \sum_{j=1}^{2^{n}} \frac{1}{2^{n}+j} +\cdots
\]
con lo cual
\[
s_{0}=\frac{1}{2}; \quad s_{1}=\frac{7}{12} > \frac{1}{2}; \quad s_{2}= \frac{533}{840}> \frac{1}{2}; \quad s_{3}=\frac{95549}{144144} >\frac{1}{2}; 
\]
y claramente diverge ya que 
\[
1+ s_{0} +s_{1} +s_{2} + s_{3} +\cdots > 1 +\frac{1}{2}  +\frac{1}{2}  +\frac{1}{2}  +\frac{1}{2} +\cdots
\]
Esta prueba aparentemente se le debe a Nicole dóresme\footnote{\textbf{Nicole dóresme} (1323-1382) Matemático francés que inventó la geometría coordenada antes de Descartes. Más detalles en \url{http://www-history.mcs.st-and.ac.uk} y más detalles sobre la serie harmónica en \url{http://mathworld.wolfram.com/HarmonicSeries.html}}. Una de las generalizaciones de la serie harmónica es la función Zeta de Riemann\footnote{\textbf{Georg Friedrich Bernhard Riemann} 1826 Hanover, Alemania - 1866 Selasca, Italia, Matemático alemán cuyas ideas sobre las geometría del espacio han tenido un profundo impacto en el desarrollo de la Física Teórica. Igualmente clarificó la noción de integral al introducir el concepto de lo que hoy se conoce como \textit{integral de Riemann}. \\ Más detalles en  \url{http://www-history.mcs.st-and.ac.uk} } $\zeta(p) = \sum_{n=1}^{\infty} n^{p}$, la cual analizaremos más adelante en la sección \ref{IntegralMaclaurin}.


\subsubsection{El método de la diferencia}
A veces para una serie $S_{N}=\sum_{n=1}^{N}a_{n}$ uno encuentra que para el término nésimo $a_{n}= f(n) -f(n-1)$ para alguna función. En ese caso es inmediato demostrar
\[
S_{N}=\sum_{n=1}^{N}a_{n}=f(N) -f(0)
\] más aún, se puede ir más allá. Si identificamos que el término nésimo tiene la forma de $a_{n}= f(n) -f(n-m)$ es fácilmente demostrable que la suma de la serie se puede escribir como 
\[
S_{N}=\sum_{n=1}^{N}a_{n}=\sum_{i=1}^{m} f(N -k +1) -\sum_{i=1}^{m}f(1 - k)
\] Hay que hacer notar que el argumento $n-m$ puede ser positivo o negativo. Con lo cual el método de la diferencia resulta versátil y muy útil cuando se requiere encontrar la suma de series de variada dificultad
\begin{itemize}
  \item  Así la suma de la serie
  	\[
  	S_{N}=\sum_{n=1}^{N} \frac{1}{n(n+1)} \rightarrow  a_{n}=\frac{1}{n(n+1)}  = \left( \frac{1}{n+1} -\frac{1}{n} \right) \rightarrow f(n)= \frac{-1}{n+1}
	\] se podrá expresar como 
	\[
	S_{N}= f(N)-f(0)= \frac{-1}{N + 1} +1=\frac{N}{N + 1}
	\]
  \item También siguiendo la estrategia de la expansión en fracciones simples se puede encontrar que
  \[
  S_{N}=\sum_{n=1}^{N} \frac{1}{n(n+2)} \rightarrow  a_{n}=\frac{1}{n(n+2)}  = -\left( \frac{1}{2(n+2)} -\frac{1}{2n} \right) \rightarrow f(n)= \frac{-1}{2(n+2)}
  \] de forma y manera que
  \[
  S_{N}= f(N) + f(N-1) - f(0) - f(-1) = \frac{3}{4} - \frac{1}{2} \left( \frac{1}{N + 2} + \frac{1}{N+1} \right)
  \]
\end{itemize}
Con alguna frecuencia surgen las series de números naturales. La más simple es 
\[
  S_{N}= 1+2+3+\cdots+N=\sum_{n=1}^{N}n=\frac{N(N+1)}{2} \quad \text{una serie aritmétrica de razón }d=1
\]
o también más interesante puede ser la serie de cuadrados de números enteros
\[
  S_{N}= 1+2^{2}+3^{2}+\cdots+N^{2}=\sum_{n=1}^{N}n^{2}= \frac{N(N+1)(2N+1)}{6}
\]
Este resultado, nada intuitivo, surge de la aplicación ingeniosa del método de la diferencia. Tal y como hemos dicho, se trata de encontrar que el elemento genérico de la serie $a_{n}= f(n) -f(n-1)=n^{2}$ para alguna función. Suponga una función del tipo
\[
f(n)=n(n+1)(2n+1) \quad \Rightarrow  f(n-1)=(n-1)n(2n-1)\quad \Rightarrow  f(n) -f(n-1)=6n^{2}
\]
con lo cual 
\[
a_{n}=n^{2}=\frac{N(N+1)(2N+1)}{6}\quad \Rightarrow  S_{N} = \frac{f(N) -f(0)}{6} = \frac{N(N+1)(2N+1)}{6}
\]
\begin{itemize}
  \item[\textbf{Ejercicio}] Muestre que $ S_{N}= 1+2^{3}+3^{3}+\cdots+N^{3}=\sum_{n=1}^{N}n^{3} = \left( \sum_{n=1}^{N} n \right)^{2} =\frac{N^{2}(N+1)^{2}}{4}$ 
\end{itemize}

\subsubsection{Sumando por analogía}
Como siempre, intentaremos proceder por analogía. La intención es expresar una serie complicada como sumas de series conocidas. Considere el siguiente ejemplo
\[
S_{N} = \sum_{n=1}^{N}(n+1)(n+3) = \sum_{n=1}^{N}\left(n^2+4n+3\right)=\left(\sum_{n=1}^{N}n^2 \right)+\left(\sum_{n=1}^{N}4n \right)+\left(\sum_{n=1}^{N}3 \right)
\] con lo cual
\[
S_{N}= \left(\frac{N(N+1)(2N+1)}{6} \right) +\left( \frac{N(N+1)}{2} \right) +\left( 3N\right) = \frac{N(2N^{2} +15N +31)}{6}
\]

\subsection{Algebra Elemental de Series}
\label{AlgebraElementalSeries}
Las series se suman, se igualan y se multipilican. Para ello es importante que tengamos cuidado con los índices y sus valores. Consideremos un par de series infinitas $S_{\infty}=\sum_{n=0}^{\infty}a_{n}$ y $\tilde{S}_{\infty}=\sum_{n=0}^{\infty}b_{n}$ con lo cual la suma de esas series será
\[
S_{\infty} + \tilde{S}_{\infty} = \sum_{n=0}^{\infty}a_{n} + \sum_{n=0}^{\infty}b_{n} = \sum_{n=0}^{\infty}\left(  a_{n}+b_{n}\right) 
\] 
Los índices son mudos y se acomodan para ser sumados. Para sumar series es imperioso que los índices de cada serie comiencen con el mismo valor esto es
\[
\left.
\begin{array}
[l]{l}
S_{\infty} = \sum_{n=0}^{\infty}a_{n} 	\\ \\
\tilde{S}_{\infty}=\sum_{n=1}^{\infty}b_{n}
\end{array}
\right\} \Rightarrow  \sum_{n=0}^{\infty}a_{n} + \sum_{j=1}^{\infty}b_{j} = \sum_{n=1}^{\infty}\left(  a_{n-1}+b_{n}\right) = a_{0} + \sum_{n=1}^{\infty}\left(  a_{n}+b_{n}\right) 
\]
nótese que hemos hecho $j=n$ y  $n=n-1$.

Algo parecido ocurre cuando las series se igualan
\[
\sum_{n=0}^{\infty}b_{n}  =\sum_{n=1}^{\infty}na_{n} \quad \Rightarrow 
\sum_{n=0}^{\infty}b_{n} = \sum_{k=0}^{\infty} (k+1)a_{k+1}\ \quad \Longleftrightarrow
\quad  \sum_{n=0}^{\infty}\left( (n+1)a_{n+1}+b_{n}\right) =0 
\]
Para finalizar se puede comprobar que las series y también se pueden multiplicar  
\[
\left[S_{\infty} \right] \left[\tilde{S}_{\infty}\right] =\left[  \sum_{n=0}^{\infty}a_{n}\right]  \left[
\sum_{n=0}^{\infty}b_{n}\right]  = \sum_{n=0}^{\infty}c_{n} \quad \text{donde}\quad
c_{n}=a_{0}b_{n}+a_{1}b_{n-1}+\cdots+a_{j}b_{n-j}+\cdots
+a_{n-2}b_{2}+a_{n-1}b_{1}+a_{n}b_{0}
\]


\subsection{Criterios de Convergencia}
Sólo podremos calcular la suma de algunas series, en la mayoría nos será imposible y nos tendremos que conformar con saber si convergen o no, o peor aún, si una suma parcial converge sin poder calcular el valor de esa suma. Los términos de una serie pueden ser positivos, negativos o números complejos y las series pueden converger (decrecer o crecer hacia un valor finito) diverger (incrementar o decrecer indefinidamente) u oscilar, Existen una serie de criterios y teoremas de aplicación general que expondremos a continuación. 

\subsubsection{Convergencia Absoluta o Condicional}
 Para estudiar la convergencia de una serie dada i.e. $\sum_{n=1}^{i}a_{i} $ siempre podremos asociarle otra de la forma $\sum_{n=1}^{i} \left\| a_{i}  \right\| $, es decir la serie de valores absolutos, con lo cual garantizamos la positividad (y que sean números reales) de los términos de la serie.  Si la serie de los valores absolutos $\sum_{n=1}^{i} \left\| a_{i}  \right\| $ converge, entonces también covergerá la serie original $\sum_{n=1}^{i}a_{i} $ y diremos que esa serie es \textit{absolutamente convergente}. Sin embargo si la serie de valores absolutos diverge, no podremos decir que $\sum_{n=1}^{i}a_{i} $ siempre converja. De hecho si converge diremos que es \textit{condicionalmente convergente} y, con un rearreglo de sus términos podrá converger, diverger u oscilar.
 
 Para una serie de términos positivos el criterio de convergencia más intuitivo (necesario pero no suficiente) es que en límite cuando $n \rightarrow \infty$ el término nésimo tienda a cero, i.e. $\lim_{n \rightarrow \infty}a_{n}= 0$. Con lo cual tenemos que si esta condición no se satisface, la serie diverge. 

\subsubsection{Criteterio de Comparación}
En segundo lugar de simplicidad está el criterio de comparación entre un par de series de términos positivos. Si conocemos el comportamiento de una de ellas comparamos el de la otra. Esto es, suponga que consideremos dos serie, una de prueba $S_{\infty} = \sum_{n=0}^{\infty}a_{n}$  y una serie conocida y convergente (o divergente) $\tilde{S}_{\infty} = \sum_{n=0}^{\infty}a_{n}$, entonces
\[
\text{Si} \ \tilde{S}_{\infty} = \sum_{n=0}^{\infty} \tilde{a_{n}} \ \text{converge y} \ \forall n \ \text{se tiene que} \  \tilde{a_{n}} \geqslant  a_{n} \quad \Rightarrow \sum_{n=0}^{\infty} \tilde{a_{n}} \geqslant \sum_{n=0}^{\infty}a_{n} \quad \Rightarrow \tilde{S}_{\infty} \ \text{converge} 
\]
Por otro lado
\[
\text{Si} \ \tilde{S}_{\infty} = \sum_{n=0}^{\infty} \tilde{a_{n}} \ \text{diverge y} \ \forall n \ \text{se tiene que} \ 0 \leqslant \tilde{a_{n}} \leqslant  a_{n} \quad \Rightarrow \sum_{n=0}^{\infty} \tilde{a_{n}} \leqslant \sum_{n=0}^{\infty} a_{n} \quad \Rightarrow \tilde{S}_{\infty} \ \text{diverge} 
\]
Para ilustrar esta estrategia consideremos las siguientes series
\[
S_{\infty} = \frac{1}{2} +  \frac{1}{3} +  \frac{1}{7} +  \frac{1}{25} + \cdots=\sum_{n=1}^{\infty}\frac{1}{n! +1}  
\]
En ese caso compararmos con con una serie conocida
\[
\sum_{n=1}^{\infty}\frac{1}{n!}=\frac{1}{0!} +\frac{1}{1!} +\frac{1}{2!} +\frac{1}{3!} +\cdots =
		1 + \underbrace{1  +\frac{1}{2!} +\frac{1}{3!} +\cdots}_{e} = 1+ e
\] y es claro que la serie indicada no es otra cosa que $e$, con lo cual la serie claramente converge y su suma es $1+e$.

\subsubsection{Criterio de la Raíz}
Dada una serie de términos positivos $S_{\infty} = \sum_{n=0}^{\infty}a_{n} $, el criterio de la raíz  (o también de la raíz de Cauchy) puede resumirse en el siguiente par de afirmaciones. Si 
\[
\begin{array}{lccl}
 \left(a_{n} \right)^{ \frac{1}{n} } \leqslant \rho < 1 & \text{para un valor de } n \text{ suficientemente grande y } \rho \text{ independiente de }n  & \Longrightarrow  & \text{converge}  \\ \\
 \left(a_{n} \right)^{\frac{1}{n}}  > 1 & \text{para un valor de } n \text{ suficientemente grande y } \rho \text{ independiente de }n  & \Longrightarrow  &  \text{diverge} \\ \\
 \left(a_{n} \right)^{\frac{1}{n}}  = 1 & \text{para un valor de } n \text{ suficientemente grande y } \rho \text{ independiente de }n  & \Longrightarrow  &  \text{diverge o converge}
\end{array}
\]
Otra forma, más compacta de expresarlo sería
\[
\text{Si } \rho = \lim_{n \rightarrow \infty}  \left(a_{n} \right)^{ \frac{1}{n} }  \quad \text{entonces, si} \quad
\left\{
\begin{array}{lcl}
 \rho < 1  & \Longrightarrow  & \text{converge}   \\ \\
  \rho > 1 & \Longrightarrow  & \text{diverge}   \\ \\
  \rho = 1 & \Longrightarrow  & \text{converge o diverge}
\end{array}
\right.
\]
Es fácil ver que si utilizamos el criterio de comparación, entonces
\[
 \left(a_{n} \right)^{ \frac{1}{n} } \leqslant \rho \qquad \Rightarrow  a_{n}  \leqslant \rho^{n} 
 \qquad \Rightarrow \left\{
 \begin{array}{ll }
     \text{cuando } \rho < 1    		& \text{la serie converge}  \\ \\
     \text{cuando } \rho \geqslant 1& \text{la serie diverge}    
\end{array} 
\right.
\]

\subsubsection{Criterio de Dálembert}
Dada una serie de términos positivos $S_{\infty} = \sum_{n=0}^{\infty}a_{n} $, el criterio de Dálembert\footnote{\textbf{Jean Le Rond Dálembert} París, Francia 1717 - 1783 Matemático francés pionero en el estudio de las ecuaciones diferenciales y su utilización en la Física, en particular en el estudio de los fluídos \\ Más detalles en  \url{http://www-history.mcs.st-and.ac.uk} } o también llamado criterio del cociente,  compara el valor relativo de un término de la serie con el que le precede. Este criterio se resume también fácilmente
\[
\text{Si } \rho = \lim_{n \rightarrow \infty}  \left( \frac{ a_{n+1} }{a_{n} } \right)   \quad \text{entonces, si} \quad
\left\{
\begin{array}{lcl}
 \rho < 1  & \Longrightarrow  & \text{converge}   \\ \\
 \rho > 1 & \Longrightarrow  & \text{diverge}   \\ \\
 \rho = 1 & \Longrightarrow  & \text{indeterminado}
\end{array}
\right.
\]
Nótese que si 
\[ 
\rho < 1 \quad \Rightarrow \rho < r < 1 \quad \Rightarrow \frac{a_{n+1}}{a_{n}} < r \quad \Rightarrow  a_{n+1} = a_{n} r
\]
Entonces para un $N <n$ pero también suficientemente grande, tendremos que los términos de la serie a partir de ese $N$ serán
\[
a_{N} +a_{N+1} +a_{N+2} +a_{N+3} \cdots = a_{N} +ra_{N} +r^{2}a_{N+2} +r^{3}a_{N+3}  \cdots = a_{N} \left( 1 + r +r^{2} +r^{3} +r^{4} \cdots \right) 
\]
y que no es otra cosa que una serie geométrica con razón $r<1$ y por consiguiente converge. Es claro que un argumento similar se puede utilizar para probar la divergencia.

Un ejemplo inmediato lo constituye la serie
\[
\frac{1}{2} + \frac{1}{2} + \frac{3}{8} + \frac{1}{4} +\frac{5}{32} +\cdots = \sum_{n=1}^{\infty} \frac{n}{2^{n}} \qquad \Rightarrow \frac{ \left( \frac{n+1}{2^{n+1}} \right) }{ \left( \frac{n}{2^{n}}  \right)} = \frac{1}{2} \cdot \frac{n+1}{n} 
\qquad \Rightarrow \rho = \lim_{n \rightarrow \infty} \frac{ \left( \frac{n+1}{2^{n+1}} \right) }{ \left( \frac{n}{2^{n}}  \right)} = \frac{1}{2} < 1
\]
con lo cual tiene converger.

\subsubsection{Criterio de la Integral de Maclaurin}
\label{IntegralMaclaurin}
El criterio de la Integral de Maclaurin\footnote{\textbf{Colin Maclaurin} 1698, Argyllshire, Escocia - 1746  Edinburgo, Escocia. Matemático escocés quien escribió el  \textit{Tratado de los Fluxiones} el primer tratado que expuso de una manera sistemática y rigurosa el cálculo diferencial ideado por Newton. Este tratado fue como respuesta a la crítica de Berkeley sobre la falta de rigurosidad de los métodos Newton } es otro criterio de comparación, pero esta vez se compara la serie con una integral.   Así supondremos que existe una funciíon $f(x)$ contínua y monótonamente decreciente para un valor de $x \geqslant x_{0}$ y que, adicionalmente, se cumple que para algún valor entero $x=n$ el valor de la función es igual a un término de la serie. Esto es $f(n)=a_{n}$. Entonces se tendrá que si el límite de $\lim_{N \rightarrow \infty} \int^{N} \text{d}x\ f(x)$ existe y es finito, entonces $\sum_{n=1}^{\infty} a_{n}$ converge. Por el contrario si el límite no existe o es infinito, entonces diverge.

La idea de este criterio es comparar la integral de $f(x)$ (es decir, el área bajo la curva) con la suma de rectángulos que representa la serie. Entoces, la suma parcial
\[
s_{i}=\sum_{n=1}^{i} a_{n} \equiv \sum_{n=1}^{i} f(n) \quad \text{Pero,  } 
\left\{ 
\begin{array}{l}
   s_{i} > \int_{1}^{i+1}  \text{d}x\ f(x)       \\ \\
   s_{i} - a_{1} < \int_{1}^{i}  \text{d}x\ f(x)         
\end{array}
\right\} \quad \Rightarrow \int_{1}^{i+1}  \text{d}x\ f(x) \leq  s_{i} \leq  \int_{1}^{i}  \text{d}x\ f(x)  +a_{1}
\]
con lo cual, al hacer $i \rightarrow \infty$ tendremos que si el límite de la integral existe, entonces la serie $\sum_{n=1}^{\infty} a_{n}$ converge. 
\[
\int_{1}^{\infty}  \text{d}x\ f(x) \leq \sum_{n=1}^{\infty} a_{n} \leq  \int_{1}^{\infty}  \text{d}x\ f(x)  +a_{1}
\]

Un ejemplo inmediato podría ser determinar si la siguiente serie converge
\[
\sum_{n=1}^{\infty} \frac{1}{\left( n - \frac{3}{2} \right)^2} \qquad \Rightarrow f(x)=\frac{1}{\left( x - \frac{3}{2} \right)^2} \qquad \Rightarrow \lim_{N \rightarrow \infty} \int^{N}  \text{d}x\  \frac{1}{\left( x - \frac{3}{2} \right)^2}=  \lim_{N \rightarrow \infty} \left( \frac{-1}{N -\frac{3}{2}} \right)=0
\]
con lo cual claramente converge

Este criterio es muy útil para acotar (entre un ínfimo y un supremo) el residuo de una determinada serie. Vale decir
\[
\sum_{n=1}^{\infty} a_{n}=\sum_{n=1}^{N} a_{n} + \underbrace{\sum_{n=N+1}^{\infty} a_{n}}_{\text{Residuo}} \qquad \Rightarrow 
\int_{N+1}^{\infty}  \text{d}x\ f(x) \leq \sum_{n=N+1}^{\infty} a_{n} \leq  \int_{N+1}^{\infty}  \text{d}x\ f(x)  +a_{N+1}
\]

El otro ejemplo, más elaborado es comprobar que la función Zeta de Riemann, $\zeta(p) = \sum_{n=1}^{\infty} n^{p}$, efectivamente converge. En este caso $f(x)=x^{-p}$, entonces
\[
\zeta(p) = \sum_{n=1}^{\infty} n^{-p} \qquad \Rightarrow \int_{1}^{\infty}  \text{d}x\ x^{-p} =
\left\{ 
\begin{array}{lc}
\left. \frac{x^{-p+1}}{-p+1} \right|_{1}^{\infty}      & \text{ Para } p \neq 1  \\ \\
   \left. \ln x \right|_{1}^{\infty}     &   \text{ Para } p = 1 
\end{array}
\right.
\]
y es claro que para $p>1$ el límite existe y es finito, por lo tanto, la función Zeta de Riemann, $\zeta(p) = \sum_{n=1}^{\infty} n^{-p}$, converge para $p>1$

\subsubsection{Series alternantes y convergencia condicional}
Hasta ahora todos los criterios que analizamos eran para una serie de términos positivos $S_{\infty} = \sum_{n=0}^{\infty}a_{n} $ por lo cual todos esos criterios nos llevaban al concepto de series absolutamente convergente. Esto es, si  $\sum_{n=0}^{\infty} \|a_{n} \|$  converge, entonces $ \sum_{n=0}^{\infty}  a_{n}$   también converge. Sin embargo, muchas veces nos tendremos que conformar con que una serie sea simplemente convergente y no requerir que sea absolutamente convergente. Este es el caso de las series alternantes. Series en las cuales se alternas términos positivos y negativos. Son series del tipo 
\[
a_{1} -a_{2} +a_{3} -a_{4} +a_{5} -a_{6} +\cdots+a_{2n-1} -a_{2n} +\cdots =\sum_{n=1}^{\infty} (-1)^{n+1} \left(  a_{n} \right) \qquad \text{ con } a_{n} \geq 0 
\]
Entonces 
\[
\sum_{n=1}^{\infty} (-1)^{n+1} \left(  a_{n} \right) \text{ converge, si } \quad 
\left\{
\begin{array}{lc}
a_{n} \rightarrow 0      & \text{cuando } n \rightarrow \infty \\ \\ & \wedge  \\ \\
 a_{n} < a_{n-1}     &   \forall \quad n >N
\end{array}
\right. 
\]
De otro modo la serie oscilará

Estas condiciones son fáciles de ver si reorganizamos la serie de los primeros $2m$ términos, a partir de un determinado $N$ par y  $ N > n $, entonces
\[
s_{2m}=(a_{N}-a_{N-1})+(a_{N-2}-a_{N-3})+\cdots+(a_{N+2m-2}-a_{N+2m-1}) 
\]
donde todos los paréntesis son positivos, con lo cual $s_{2m} > 0$ y se incrementa al incrementar $m$. Ahora bien, si rearreglamos la serie tendremos que 
\[
s_{2m}=a_{N}-(a_{N-1}-a_{N-2})- (a_{N-3}-a_{N-3})+\cdots-(a_{N+2m-1}-a_{N+2m-2})- a_{N+2m-1} 
\]
y, otra vez los paréntesis son positivos y es inmediato comprobar que entonces $s_{2m} < a_{n}$ para todo $m$. Como  $a_{n} \rightarrow 0 $   cuando  $n \rightarrow \infty$, la serie alternante necesariamente converge.

\section{Series de potencias}
El siguiente paso en este estudio, será el ampliar la idea de serie al permitir que sus términos sean función de alguna variable (una o varias), esto es $a_{n}=a_{n}(x)$. Esta extensión del concepto se serie, trae como consecuencia que ahora las sumas parciales dependen de $x$
\[
s_{n}=s_{n}(x)=\sum_{k=1}^{n} a_{k}(x) = a_{0}(x) +a_{1}(x) +a_{2}(x) + \cdots \qquad \text {con lo cual, si } 
\quad \lim_{n \rightarrow \infty} s_{n}(x) = S(x)= \sum_{k=1}^{\infty} a_{k}(x) 
\]
Entonces, el comportamiento de las serie también dependerá de la variable. Ahora la convergencia de la serie podrá ser posible para algunos valores de $x$ y no para otros. El punto central con las en las series de funciones $f(x)$(complicadas) es la tratar de construir funciones como una serie de funciones, $ a_{k}(x)$, más simples. Así, esas sumas parciales $f_{n}(x)$ constituirán la función deseada
\[
f_{n}(x) = \sum_{k=1}^{n} a_{k}(x) \quad \Rightarrow f(x)= \sum_{k=1}^{\infty} a_{k}(x) = \lim_{n \rightarrow \infty} \sum_{k=1}^{n} a_{k}(x)
\]
Es decir estaremos interesados en aquellas funciones a las cuales converjan las sumas parciales de una serie.

Si bien más adelante abordaremos este concepto haciéndolo extensivo a cualquier función, para fijar conceptos, comenzaremos por las series de funciones más comunes: Las series de potencias.  Esto es, asimilaremos una serie de potencias $a_{n}=c_{n} x^{n}$ a un polinomio de grado infinito.
\[
P(x)=c_{0}+ c_{1}x+ c_{2}x^{2}+ c_{3}x^{3}+ c_{4}x^{4}+\cdots =\sum_{n=0}^{\infty} c_{n} x^{n} \qquad \text{o también} \quad P(x-x_{0})=\sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right) ^{n}
\]
Esta asociación tiene la ventaja de permitirnos intuir algunos comportamientos de la serie para algunos valores de $x$. Los coeficientes $ c_{n}$ son números independientes de $x$. Pero, más aún, estas series pueden ser series de potencias de número complejos. Vale decir, $\sum_{n=0}^{\infty} c_{n} z^{n}$ con $z = x +iy$


\subsection{Convergencia de una serie de potencias}
Claramente, podremos utilizar todos los criterios que hemos desarrollado anteriormente. Así  una serie de potencias $\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right) ^{n}$ converge en un punto $x_{0}$ si el
$ \lim_{m\rightarrow\infty}\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)  ^{n}$
existe, para $x=x_{0};$ para todo $x$ o para algunos $x$

Una serie de potencias $\sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right)^{n}$ convergerá absolutamente sí el
\[ \lim_{n \rightarrow \infty} \sum_{j=0}^{n}\left\|  c_{j}\left(x-x_{0}\right)  ^{j}\right\|~=~l\]
existe. También se cumplirá el criterio de convergencia absoluta. Esto era, si $\sum_{n=0}^{\infty}\left\|  c_{n}\left(  x-x_{0}\right)^{n}\right\|  $ converge $\Rightarrow \sum_{n=0}^{\infty} c_{n}\left( x-x_{0} \right)^{n}$ converge, pero el inverso no es siempre verdad.

Los criterios más populares para evaluar la convergencia, se seguirán cumpliendo. Así el criterio de Dálembert y el de la raíz de Cauchy se podrán reescribir como:
\[
\left.
\begin{array}[c]{c}
\lim_{n\rightarrow\infty}\left\|  \dfrac{c_{n+1}\left(  x-x_{0}\right)^{n+1}}{c_{n}\left(  x-x_{0}\right)  ^{n}}\right\|  =l \\ \\
\lim_{n\rightarrow\infty}\sqrt[n]{c_{n}\left(  x-x_{0}\right) ^{n}}=l(x)
\end{array}
\right\}  \qquad \Rightarrow \quad \left\{
\begin{array}[c]{c}
l(x)<1 \qquad \Rightarrow \quad \text{converge} \\ \\
l(x)>1 \qquad \Rightarrow \quad \text{diverge}
\end{array}
\right.
\]
Sólo que ahora es bueno enfatizar que $l=l(x)$, es decir que el límite dependerá de la variable. Llamaremos, de ahora en adelante a este límite el \textit{radio o entorno de convergencia} y lo denotaremos por $l \equiv \rho=\rho (x)$. El cual delimitará los valores de $x$ para que la serie de potencias converja. Vemos el siguiente ejemplo en el cual considermos la siguiente serie
\[
s_{n}(s)= 1 + x+\frac{x^{2}}{2} + \frac{x^{3}}{6} +\cdots +\frac{x^{n}}{n!} +\cdots=\sum_{n=1}^{\infty} \frac{x^{n}}{n!} \quad \Rightarrow \lim_{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}} \equiv 
 \lim_{n \rightarrow \infty} \left\| \dfrac{  \dfrac{x^{n+1} }{ (n+1)! } }{ \dfrac{x^{n} }{ n!}} \right\| = 
  \lim_{n \rightarrow \infty} \left\| \dfrac{ x }{n +1 } \right\|  =0 
\]
es decir, $\rho=\rho (x)= \lim_{n \rightarrow \infty} \frac{a_{n+1}}{a_{n}} =0$ con lo cual la serie converge para todo valor de $x$. Otro caso ocurre cuando consideramos la siguiente serie de potencias:
\begin{align*}
\sum_{n=1}^{\infty} \left(  -1\right)  ^{n+1}\ n\ \left(  x-2\right)^{n} \quad &  \Rightarrow \quad
\rho = \lim_{n \rightarrow \infty} 
\left\|
\frac{\left(  -1\right)  ^{n+2}\ \left(  n+1\right)  \ \left(  x-2\right)^{n+1}}{\left(  -1\right)^{n+1}\ n\ \left(  x-2\right)^{n}}\right\|
= \left\|  x-2\right\|  \lim_{n\rightarrow\infty}\left\|  \frac{n+1}{n}\right\|  \\
& \\
\rho = \left\|  x-2\right\|  \lim_{n\rightarrow\infty} \left\|  \frac{n+1}{n} \right\|   &  = \left\|  x-2\right\|  \quad\Rightarrow \quad \left\{
\begin{array}[c]{l}
\text{converge si }\left\|  x-2\right\|  <1 \Rightarrow \quad 1<x<3\\
\\
\text{diverge si }\left\|  x-2\right\|  >1
\end{array}
\right.
\end{align*}
Es decir, la serie $ \sum_{n=1}^{\infty} \left(  -1\right)  ^{n+1}\ n\ \left(  x-2 \right)^{n} $ convergerá únicamente para $1 < x < 3 $. Para otros valores de $x$, diverge.

Para puntualizar 
\begin{itemize}
\item  Si una serie converge en $x=x_{1}$, convergerá absolutamente para $\left\|  x-x_{0}\right\|  <\left\|  x_{1}-x_{0}\right\|  $ y divergerá para $\left\|  x-x_{0}\right\|  >\left\|  x_{1}-x_{0}\right\|  $

\item  Se llama radio de convergencia, $\rho =\rho(x)$ a aquella cantidad tal que la serie $\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)  ^{n}$ converge para $\left\|  x-x_{0}\right\|  < \rho$ y diverge para $\left\|  x-x_{0}\right\| >\rho.$ Una serie $\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)  ^{n}$ que converge únicamente para $x=x_{0}$ tendrá un radio de convergencia $\rho=0,$ mientras que una que converja para todo $x$ tendrá un radio de convergencia $\rho=\infty$
\end{itemize}

\subsection{Covergencia uniforme}
En definitiva se puede refrasear el criterio de convergencia de Cauchy que vimos al comenzar este capitulo \ref{IntroSeries}. Para cualquier valor de $\epsilon > 0$, tan pequeño como uno quiera, siempre existirá un número $N$ \textbf{independiente de } $x$, con $a \leq x \leq b$ tal que 
\[
\text {Si } S(x) = \lim_{n \rightarrow \infty} s_{n}(x)  = \sum_{n=1}^{\infty} a_{n}(x) \quad \Rightarrow \left\| S(x) - s_{n}(x)  \right\| < \epsilon \qquad \forall \; x \in \left[a,b\right] \: \wedge \; n \geq N.
\]
Con ello es inmediato indentificar el error que se comete cuando se corta la serie en un $N$ suficientemente grande
\[
S(x) =\underbrace{ \sum_{n=1}^{N} a_{n}(x)}_{s_{n}(x)} + \underbrace{\sum_{n=N+1}^{\infty} a_{n}(x)}_{\approx \epsilon} 
\]

Para el caso de series de funciones, consideraremos existen un par de criterios que identifican la convergencia uniforme y tienen, también  cierta popularidad. El criterio Mayorante de Weierstrass\footnote{\textbf{Karl Theodor Wilhelm Weierstrass} Westphalia 1815 - Berlin 1897 Matemático Alemán con importantes contribuciones al análisis complejo mediante la utilización de series} y el criterio de Abel. A continuación los resumiremos.

Hay que resaltar el punto que las suma de funciones contínuas $a_{n}(x)$ no necesariamente habrá de ser contínua, el concepto de convergencia uniforme busca garantizar que esa suma de funciones contínuas también será contínua. Así, recordamos la idea de continuidad de una función. Una función será contínua si sus límites por la derecha y por izquierda coinciden
\[
\lim_{t \rightarrow x^{\pm}} f(t) = f(x) \quad \Rightarrow \lim_{t \rightarrow x^{\pm}} \lim_{n \rightarrow \infty} f_{n}(x) \stackrel{?}{=} 
 \lim_{n \rightarrow \infty} \lim_{t \rightarrow x^{\pm}} f_{n}(x)
\]
Es decir, al suponer que la suma de términos contínuos tiende a una función contínua estamos suponiendo que podemos intercambiar los ímites. pero eso no es simpre cierto. Considere el caso (extremo)
\[
f_{n}= n^2 x \left(1 -x^2 \right)^n  \text{con } 0 \leq x \leq 1 \; n=1,2,3, \dots \quad \left\{
\begin{array}{l l}
     \lim_{n \rightarrow \infty} f_{n}= 0  \Rightarrow &  \int_{0}^{1} \mathrm{d}x \; \left( \lim_{n \rightarrow \infty} f_{n}(x) \right) =0  \\ \\
      \int_{0}^{1} \mathrm{d}x \; f_{n}(x) =\frac{n^2}{2(n +1)}  \Rightarrow &   \lim_{n \rightarrow \infty}  \int_{0}^{1} \mathrm{d}x \;  f_{n} \rightarrow \infty
\end{array} \right.
\]
Claramente no se pueden intercambiar los límites.

\subsubsection{Criterio Mayorante de Weierstrass}
Si encontramos una serie  convergente  de números positivos
\[
\mathcal{M} = \sum_{j=1}^{\infty} M_{j} \quad \text{con } M_{i} \geq \|a_{i}(x) \| \quad \forall \; x \in \left[ a,b \right] \quad \text{entonces la serie }  \sum_{n=1}^{\infty} a_{n}(x) \text{ es \textbf{uniformemente} convergente}
\]
La demostración se obtiene a partir de la definición misma de convergencia. Si $\sum_{j=1}^{\infty} M_{j}$ converge, entonces para $n+1 \geq N$ se tiene
\[
\sum_{j=n+1}^{\infty} M_{j} < \epsilon \quad \text{y como }  \|a_{i}(x) \| \leq M_{i} \quad \Rightarrow \sum_{j=n+1}^{\infty}  \|a_{i}(x) \| < \epsilon \quad \Rightarrow \left\| S(x) - s_{n}(x)  \right\| \equiv \sum_{j=n+1}^{\infty}  \|a_{i}(x) \| < \epsilon
\]
con la cual la serie $\sum_{n=1}^{\infty} a_{n}(x)$ será uniformemente convergente para todo $ x \in \left[ a,b \right] $. Ahora bien, como consieramos los $M_{i} \geq 0$. La serie en cuestión también será absolutamente convergente. Otra vez, los criterios de convergencia absoluta y, en este caso, de convergencia uniforme, no son consecuencia uno del otro, ni están relacionados. Las series 
\[
\sum_{n=1}^{\infty} \frac{(-1)^n}{n+x^2} \quad \text{para } -\infty < x < \infty \qquad \wedge \qquad 
\sum_{n=1}^{\infty} (-1)^{n-1} \frac{x^{n}}{n} \quad \text{para } 0 \leq x \leq 1
\]
convergen uniformemente pero NO absolutamente. Sin embargo, en el intervalo $0 \leq x \leq 1$ la serie $\sum_{j=0}^{\infty}~(1~-~x)x^{j}$ converge absolutamente pero no uniformemente, por cuanto tiene una discontinuidad. Se puede demostrar que 
\[
\sum_{j=0}^{\infty}~(1~-~x)x^{j} = \left\{ \begin{array}{lc}
 = 1     & 0 \leq x < 1   \\
 = 0    &  x =1 
\end{array}
\right.
\]
con lo cual se puede concluir que una serie arbitraria $f(x)= \sum_{j=1}^{\infty}  a_{i}(x) $ no podrá converger uniformemente en intervalos en los cuales la función $f(x)$ sea discontínua.

\subsubsection{Criterio de Abel}
El criterio de Abel se puede resumir de la siguiente forma: dada una serie de la forma 
\[
\sum_{i=1}^{\infty}  a_{i}(x) \; \wedge \;  a_{i}(x)=c_{n}f_{i}(x) \quad \Rightarrow \lim_{n \rightarrow \infty}  \sum_{j=1}^{n}  a_{j}(x) =S
\]
y por lo tanto la serie \textit{converge uniformemente} en $\left[a,b \right]$. Para que se cumpla el criterio de Abel, $f_{n}(x)$ tiene que estar acotada, $0 \leq f_{n} \leq M \; \forall \: n)$, tiene que ser monótonamente decreciente en el intervalo en el cual esté definida, $f_{n+1}(x) \leq f_{n}(x)$ con $x \in \left[a,b \right]$


\subsection{Algebra y convergencia de series de potencias}

El álgebrá elemental de series que mencionamos en la sección \ref{AlgebraElementalSeries} se puede reconsiderar a la luz de las series de potencias. De esta forma recordamos que  los índices en las series son mudos%
\[
\sum_{n=1}^{\infty}a_{n}\ n\ \left(  x-x_{0}\right)  ^{n-1}=\sum_{j=1}^{\infty}a_{j}\ j\ \left(  x-x_{0}\right)  ^{j-1}=\sum_{k=0}^{\infty}
a_{k+1}\ \left(  k+1\right)  \ \left(  x-x_{0}\right)  ^{k}%
\]
en la última sumatoria hemos hecho $k=j-1,$ por lo cual $j=k+1.$

Las series se igualan%
\begin{align*}
\sum_{n=0}^{\infty}b_{n}\left(  x-x_{0}\right)  ^{n}  &  =\sum_{n=1}^{\infty}a_{n}\ n\ \left(  x-x_{0}\right)  ^{n-1} \\
\sum_{n=0}^{\infty}b_{n}\left(  x-x_{0}\right)  ^{n}  &  =\sum_{k=0}^{\infty}a_{k+1}\ \left(  k+1\right)  \ \left(  x-x_{0}\right)  ^{k}=\sum_{n=0}^{\infty}a_{n+1}\ \left(  n+1\right)  \ \left(  x-x_{0}\right)  ^{n}
\end{align*}
por lo cual
\[
b_{n}=a_{n+1}\ \left(  n+1\right)  \
\]
si la igualdad hubiera sido%
\[
\sum_{n=0}^{\infty}{}_{n}a_{n}\left(  x-x_{0}\right)  ^{n}=\sum_{n=1}^{\infty}a_{n}\ n\ \left(  x-x_{0}\right)  ^{n-1}=
\sum_{n=0}^{\infty}a_{n+1}\ \left(n+1\right)  \ \left(  x-x_{0}\right)  ^{n} 
\quad \Longrightarrow \quad a_{n+1}=\frac{a_{n}}{\left(  n+1\right)  }%
\]

Las series se suman%
\[
\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)^{n} + \sum_{k=2}^{\infty}b_{k}\left(  x-x_{0}\right)^{k} 
	= a_{0} + a_{1}\left(  x-x_{0}\right) + \sum_{n=2}^{\infty}\left(  a_{n}+b_{n}\right)  \left(  x-x_{0}\right)  ^{n}
\]	 
o también 
\[	
\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)^{n} + \sum_{k=0}^{\infty}b_{k}\left(  x-x_{0}\right)^{k +2} = 
a_{0} + a_{1}\left(  x-x_{0}\right) + \sum_{n=2}^{\infty}\left(  a_{n}+b_{n-2}\right)  \left(  x-x_{0}\right)^{n} =
	 \sum_{n=0}^{\infty}\left(  a_{n}+b_{n-2}\right)  \left(  x-x_{0}\right)  ^{n} 
\]
y en este último caso $b_{-2} = b_{-1}=0$. Nótese como en los dos ejemplos anteriores hemos hecho coincidir los el comienzo de los dós índices de la sumatoria. 

La series también se multiplican, esto es
\[
\left[  \sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)^{n} \right]  \left[ \sum_{n=0}^{\infty}b_{n}\left(  x-x_{0}\right)^{n}\right]  =
\sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right)^{n}
\]
con
\[
c_{n}=a_{0}b_{n}+a_{1}b_{n-1}+a_{2}b_{n-2}+\cdots+a_{j}b_{n-j}+\cdots
+a_{n-2}b_{2}+a_{n-1}b_{1}+a_{n}b_{0}%
\]
Si alguna de las series de potencias es absolutamente convergente, entonces su multiplicación con otra, será absolutamente convergente.

Pero también las series de potencias se  ¡ i\textbf{nvierten} ! y para ello utilizamos todo lo visto anteriormente veamos. Supongamos que se tiene una serie del tipo 
\[
y -y_0 =a_{0} +a_{1}\left(  x-x_{0}\right) +a_{2}\left(  x-x_{0}\right)^{2} +\cdots +a_{n}\left(  x-x_{0}\right)^{n}+\cdots
=\sum_{n=0}^{\infty}a_{n}\left(  x-x_{0}\right)^{n} 
\]
Es decir tenemos $y -y_0$ expresado en términos de una serie de potencias de $\left(  x-x_{0}\right)^n$ entonces, igual podremos plantearnos invertir el proceso, vale decir, expresar $\left(  x-x_{0}\right)$ en términos de potencias $\left(  y - y_{0}\right)^n$ Esto es
\[
\left(  x-x_{0} \right) = \sum_{n=0}^{\infty} b_{n} \left(  y-y_{0} \right)^{n} \quad \Rightarrow  \left(  x-x_{0} \right) = \sum_{k=0}^{\infty} b_{k}\left [ \sum_{j=0}^{\infty}a_{j}\left(  x-x_{0}\right)^{j}  \right]^{k} 
\]
y a lo bestia al igualar términos con la misma potencia, despejamos los coeficientes $b_n$ en términos de los $a_n$, de forma que
\begin{align*}
  b_1 =  & \frac{1}{a_1}  \\
  b_2 =  & -\frac{a_2}{(a_1)^3}  \\
  b_3 =  & \frac{2(a_2)^2 -a_1 a_3}{(a_1)^5}  \\
  b_4 =  & \frac{5a_1 a_2 a_3 -a_1^2 a_4 -5a_2^3}{(a_1)^7}  \\
  \vdots = &  \vdots
\end{align*}

Igualmente, si una serie $f(x) = \sum_{n=0}^{\infty} a_{n}(x -x_{0}) = \sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right)^{n} $ converge para un entorno $-R \leq x \leq R$ entonces por el criterio de Mayorante de Weierstrass, entonces convergerá absoluta y uniformemente para $-S \leq x \leq S$ con $0 \leq S \leq R$. Más aún, el criterio de Abel nos garantiza las siguientes propiedades 
\begin{itemize}
  \item  podemos extender la idea de continuidad de una serie, dado que todos los términos  $a_{n}(x) =c_{n}\left(  x-x_{0}\right)^{n}$ son funciones contínuas de $x$ y $f(x)  = \sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right)^{n} $ converge uniformemente para un entorno  $-S \leq x \leq S$, entonces la función $f(x)$ es contínua en el intervalo de convergencia.
  \item Si los términos $a_{n}(x) =c_{n}\left(  x-x_{0}\right)^{n}$ son funciones contínuas de $x$, entonces la serie puede ser derivada término a término 
  \[
\frac{\mathrm{d}\left[  \sum_{n=0}^{\infty}c_{n}\left(  x-x_{0}\right)^{n}\right]  }{\mathrm{d}x} = 
\sum_{n=1}^{\infty}c_{n}\ n\ \left(x-x_{0}\right)^{n-1}
\]
(nótese como cambia el comienzo de la serie) y convergerá a 
\[
\sum_{n=1}^{\infty}c_{n}\ n\ \left(x-x_{0}\right)^{n-1} \rightarrow \frac{\mathrm{d} f(x) }{\mathrm{d}x} \quad 
a_{n}(x) \; \wedge \frac{\mathrm{d} a_{n}(x) }{\mathrm{d}x} \quad \text{contínuas} \; \wedge \; \sum_{n=0}^{\infty} a_{n}(x) \quad \text{converge uniformemente en } \left[a,b \right] 
\]
  \item De igual manera las series pueden ser integradas término a término
  \[
  \int_{a}^{b} \mathrm{d}x \; f(x) =  \int_{a}^{b} \mathrm{d}x \; \sum_{n=0}^{\infty}c_{n}\left(  x-x_{0} \right)^{n} 
  =  \sum_{n=0}^{\infty} \int_{a}^{b} \mathrm{d}x \; c_{n}\left(  x-x_{0} \right)^{n}
  = \sum_{n=0}^{\infty}\frac{c_{n}}{n+1}\left(  x-x_{0} \right)^{n+1}    
  \]
\end{itemize}

\subsection{Series de Taylor}
Para los físicos el uso apropiado (y frecuente) de la serie Taylor facilita la vida y muchos cálculos. La idea detrás de este tipo de series es la de la aproximación de una determinada función por una serie de potencias en donde existe una forma sistemática de construir los coeficientes y, dependiendo de el número de términos que utilicemos en la serie, tendremos idea de cuan aproximada es la serie y cuanto es el error que cometemos al desarrollar la serie hasta un determinado término. Así supodremos que $f=f(x)$ es una función contínua y contínuamente diferenciable. Con lo cual, si denotamos  $\frac{\mathrm{d} f(x) }{\mathrm{d}x} = f'(x)$, entonces supondremos que $f'(x),f''(x),f'''(x),\cdots,f^{(n)}(x)$ están definidas en el intervalo $\left[ a,b\right]$. Entonces, conocemos desde siempre que 
\[
\int^{a+h}_{a} \mathrm{d}x \; f'(x)= f(a+h) - f(a) \quad \Rightarrow f(a+h) =   f(a) +  \int^{a+h}_{a} \mathrm{d}x \; f'(x) 
\quad \Rightarrow  f(a+h) \approx  f(a) + hf'(a)
\] 
donde hemos supuesto que en intervalo $\left[a,a + h\right]$ la función $f'(x)$ es constante y tiene como valor $f'(a)$. Ahora bien, esto vale todo $x$ y para cualquier función, por lo tanto se cumple que 
\[
\begin{array}{ r l}
  f(x)  \approx & f(a) + (x- a)f'(a) \\ \\
  f'(x)  \approx & f'(a) + (x- a)f''(a) \\ \\
 f''(x)  \approx & f''(a) + (x- a)f'''(a) \\ \\
\vdots    &  \vdots \\ \\
 f^{(n-1)}(x)  \approx & f^{(n-1)}(a) + (x- a)f^{(n)}(a) 
\end{array}
\]
Con lo cual podemos construir
\[
 f(a+h) =   f(a) +  \int^{a+h}_{a} \mathrm{d}x \; f'(x) \approx  f(a) +  \int^{a+h}_{a} \mathrm{d}x \left[ f'(a) + (x- a)f''(a) \right]
  \approx  f(a) +h f'(a) +\frac{h^2}{2}f''(a)
\]
que no es otra cosa que una aproximación de segundo orden a $f(a+h)$. En general podemos construir

\begin{eqnarray}
 f(a+h) & = & f(a) +  \int^{a+h}_{a} \mathrm{d}x \; f'(x) =   f(a) +  \int^{a+h}_{a} \mathrm{d}x \; \left[  f'(a) +  \int^{a+h}_{a} \mathrm{d}x \; f''(x)\right] \nonumber \\ 
{ } & = & f(a) +h f'(a) +  \int^{a+h}_{a} \mathrm{d}v \; \left[  \int^{a+h}_{a} \mathrm{d}x \; f''(x)\right] \nonumber \\ 
     & = & f(a) +h f'(a) +  \int^{a+h}_{a} \mathrm{d}u \; \left(  \int^{a+h}_{a} \mathrm{d}v \; \left[  f''(a) +  \int^{a+h}_{a} \mathrm{d}x \; f'''(x) \right] \right) \nonumber \\
     & = &  f(a) +h f'(a) +\frac{h^2}{2} f''(a) + \int^{a+h}_{a} \mathrm{d}u \; \left(  \int^{a+h}_{a} \mathrm{d}v \; \left[  \int^{a+h}_{a} \mathrm{d}x \; f'''(x) \right] \right)
\nonumber
\end{eqnarray}
y si repetimos ese procedimiento $n$ veces, suponiendo que las derivadas de $f(x)$ existan, tendremos la aproximación $n-1$ a la función. Esto es
\[
  f(a+h) =  f(a) +h f'(a) +\frac{h^2}{2!}f''(a) +\frac{h^3}{3!}f'''(a)+ \cdots+\frac{h^{n-1}}{(n-1)!}f^{n-1}(a) + \mathcal{R}_{n}
\]
y también es fácil convencerse por inspección que el residuo o el error que cometemos en la aproximación $n-1$ viene dado por la integración enésima de la derivada enésima, vale decir 
\[
\mathcal{R}_{n} =  \int^{a+h}_{a} \mathrm{d}u \;  \int^{a+h}_{a} \mathrm{d}v \; \underbrace{\cdots \; \; \cdots}_{n \text{ veces} } \int^{a+h}_{a} \mathrm{d}x \; f'''(x) 
\]
y por el Teorema del Valor medio
\[
\int^{a+h}_{a} \mathrm{d}\tau g(\tau) = hg(\xi) \quad \Rightarrow \mathcal{R}_{n} =\frac{h^n}{n!}f^{(n)}(\xi) \quad \text{con } a \leq \xi \leq a + h
\]
Ahora bien, una elección astuta del parámetro $h = x-a$ nos lleva a la conocida expresión de la serie de Taylor para una función de una variable
\[
 f(x) =  f(a) +(x-a) f'(a) +\frac{(x-a)^2}{2!}f''(a) +\frac{(x-a)^3}{3!}f'''(a)+ \cdots+\frac{(x-a)^{n-1}}{(n-1)!}f^{n-1}(a) + \mathcal{R}_{n}
\]
y el error vendrá dado por 
\[
 \mathcal{R}_{n} =\frac{(x-a)^n}{n!}f^{(n)}(\xi) \quad \text{con } a \leq \xi \leq a + h
\]
así la expansión de Taylor especifica el valor de una función en un punto $x$ en términos de el valor de la función y sus derivadas en un punto de referencia $a$. La expansión se hace en términos de potencias de la diferencia, $(x-a)$, entre el punto que se evalúa y el punto de referencia. 

Algunas otras formas de expresar la serie de Taylor, serían
\[
f(x +h) = \sum^{\infty}_{n=0}  \frac{h^n}{n!}f^{n}(x) =   \sum^{\infty}_{n=0} \frac{h^n \frac{\mathrm{d}^n}{\mathrm{d}x^n}}{n!}f(x) =
 \sum^{\infty}_{n=0} \frac{h^n \mathbb{D}^{n} }{n!}f(x) = e^{h\mathbb{D}}f(x) \quad \text{donde,  } \mathbb{D} \equiv \frac{\mathrm{d}}{\mathrm{d}x}
\] 

Si el punto de referencia es $a=0$ tendremos la serie de Maclaurin 
\[
 f(x) =  f(0) + x f'(a) +\frac{x^2}{2!}f''(a) +\frac{x^3}{3!}f'''(a)+ \cdots+\frac{x^{n-1}}{(n-1)!}f^{n-1}(a) + \mathcal{R}_{n} 
\]
\subsubsection{Algunas Series de Taylor}
Un listado incompleto de las series de Taylor más utilizadas es
\begin{eqnarray}
  e^x   & = &  1 + x +\frac{x^2}{2} +\frac{x^3}{3!}+\frac{x^4}{4!}+\cdots + \frac{x^n}{n!} + \cdots   \qquad \text{para }  -\infty < x < \infty  \nonumber \\  
 \mathrm{sen} \: x  & = &  x - \frac{x^3}{3!} +\frac{x^5}{5!} - \frac{x^7}{7!} +\cdots + (-1)^{n+1} \frac{x^{2n-1}}{(2n-1)!} + \cdots \qquad    \text{para }  -\infty < x < \infty  \nonumber \\ 
 \cos x &	=& 1 - \frac{x^2}{2} +\frac{x^4}{4!} -\frac{x^6}{6!} +\cdots + (-1)^{n+1} \frac{x^{2n-2}}{(2n-2)!} + \cdots     \qquad \text{para }  -\infty < x < \infty  \nonumber  \\   
 \arctan x	& = &  x - \frac{x^3}{3} +\frac{x^5}{5} - \frac{x^7}{7} +\cdots + (-1)^{n+1} \frac{x^{2n-1}}{(2n-1)} + \cdots  \qquad    \text{para }  -1 < x < 1  \nonumber \\ 
 \ln (1 +x)	& = & x -\frac{x^2}{2} +\frac{x^3}{3} - \frac{x^4}{4} +\cdots + (-1)^{n+1} \frac{x^{n}}{n} + \cdots     \qquad \text{para }  -1 < x < 1  \nonumber \\
 (1 + x)^m	& = & 1 +mx + m(m-1) \frac{x^2}{2} + m(m-1)(m-2) \frac{x^3}{3!} +\cdots + \frac{m!}{n!(m -n)!}x^n + \cdots	\quad  \text{para }  -\infty < x < \infty  \nonumber   
 \end{eqnarray}
 \subsubsection{La expansión binomial}
Por lo frecuente y su uso, consideremos el caso de la expansión binomial 
\[
(1 + x)^m	 =  1 +mx + m(m-1) \frac{x^2}{2} + m(m-1)(m-2) \frac{x^3}{3!} +\cdots = \sum_{n=0}^{\infty} \frac{m!}{n!(m -n)!}x^n =
 \sum_{n=0}^{\infty} \left(\begin{array}{c}m \\ n \end{array}\right) x^n
\]
donde el término $\left(\begin{array}{c}m \\ n \end{array}\right)$ se denomina el coeficiente binomial y la serie termina cuando $m=n$. Ahora bien, escrito de la formas compactas de la derecha se sugiere que el exponente $m$ tendría que ser entero y positivo. Pero no es así. La serie explícita de la izquierda no se restringe a valores enteros y positivos de $m$. Por ello la forma compacta pero exacta de la expansión binomial. 
\[
\left(1 + \frac{x}{a}\right)^m =  1 +m\left(\frac{x}{a} \right) + \frac{ m(m-1)}{2}\left(\frac{x}{a} \right)^2 +  \frac{m(m-1)(m-2)}{3!}\left(\frac{x}{a} \right)^3 +\cdots = \sum_{n=0}^{\infty} \frac{\Gamma(1+m)}{\Gamma(1+n)\Gamma(1+m - n)}\left(\frac{x}{a} \right)^n 
\]
Donde hemos utilizado la función $\Gamma(x) $ como la generalización del factorial para valores que no se restringen a enteros positivos. Cuando $m$ es un entero positivo tendremos $\Gamma(1 +m)=m!$.
 Nótese también que si el exponente es negativo, $\left(1 + \frac{x}{a}\right)^m$ tiene una singularidad o un polo en $x = -a$

\subsubsection{Taylor en varias variables}
Sólo por razones de completitud, y para reforzar los conceptos de que es un desarrollo en series para una función alrededor de un determinado punto, escribiremos el desarrollo en series de Taylor para una funcionón de dos variables $f=f(x,y)$. Esta es
\begin{eqnarray*}
f(x,y) & = & f(a,b) + ( x -a )  \left. f_x \right|_{ ab } +( y -b )  \left. f_y \right|_{ ab } + \frac{1}{2!} \left[   \left. ( x -a )^2 f_{xx}  \right|_{ ab } + 2( x -a )( y -a )   \left.  f_{xy} \right|_{ab } +( y -a )^2   \left. f_{yy} \right|_{ ab } \right] \\
 	&  & + \frac{1}{3!} \left[ ( x -a )^3  \left. f_{xxx}  \right|_{ ab } + 3( x -a )^2 ( y -a )   \left. f_{xxy} \right|_{ ab } + 3( x -a )( y -a )^2  \left. f_{xyy} \right|_{ ab } +( y -a )^3  \left. f_{yyy}  \right|_{ ab } \right] + \cdots  
\end{eqnarray*}
O más compacto
\[
 f\left( x^j + x_0^j\right)  =  \sum_{n=0}^{\infty} \frac{1}{n!} \left. \left( x^k \partial_{k} \right)^n  f \left( x^m \right)  \right|_{x^m = x^m_0}  \quad \Rightarrow 
f\left( \vec{r} + \vec{a} \right) =  \sum_{n=0}^{\infty}  \frac{1}{n!} \left. \left( \vec{r} \cdot \vec{\nabla} \right)^n  f\left( x^m \right)  \right|_{\vec{r} = \vec{a}}  
\]
Dónde hemos utilizado la siguiente convención
\[
f_{x} = \frac{\partial}{\partial x } = \partial_{x}; \quad f_{y} = \frac{\partial }{ \partial y} = \partial_{y};  \quad f_{xx} = \frac{\partial^2}{\partial x^2 } = \partial_{xx}; \quad f_{xy} = \frac{\partial^2}{\partial x \partial y} = \partial_{xy};  \quad f_{yy} = \frac{\partial^2}{\partial y^2} = \partial_{yy};  \quad \cdots
\]

\section{Series y Espacios de Hilbert}
Hemos dejado ``sueltos'' algunos conceptos para los espacios de Hilbert \textit{infito-}dimensional. El primero de estos conceptos es que un vector $ \left| a \right> \in E^{\infty} $ surge la combinación lineal de elementos de una base infinita $\{ \left| e_{i} \right> \}$, ( de una serie) que converge al vector $ \left| a \right> $ para un espacio donde también la norma del vector converge a un valor finito $ \|a\|^{2} = \left< a \right. \left| a \right> $. El segundo concepto fue la posibilidad de expresar un determinado vector (una función ) como combinación lineal de una base (de dimensión infinita) de un espacio vectorial $E^{\infty} $. Efectivamente, esa combinación lineal (de dimensión infinita) habrá de converger a el valor de la función en ese punto. En su momento expresamos estos conceptos intuitivos y fácilmente demostrables para $E^n$ (un espacio vectorial Euclideano de dimensión finita, $n-$dimensional) y sin mayores justificaciónes hicimos el  ``salto '' a $E^{\infty}$ (un espacio Euclideano \textit{infinito}-dimensional). Ahora, equipados con los conceptos de convergencia uniforme estamos en capacidad de explorar esas razones que antes eludimos. Ambos conceptos tienen que ver con la palabra \textit{completitud}, la cual, como veremos, no tiene el mismo significado en cada una de las situaciones antes mencionadas, pero será complementario. En el primer caso la completitud de $E^{\infty} $ se logra al poder expresar un vector como una combinación lineal de una base infinita que converja al valor del vector. En el segundo caso diremos que la base  $\{ \left| e_{i} \right> \}$ para $E^{\infty} $ será completa si expande la totalidad de los vectores de $E^{\infty} $.

\subsection{Completitud de $E^{\infty}$}
La primera idea de completitud de un Espacio de Hilbert $E^{\infty}$ tiene que ver con el hecho que, en en ese espacio, donde la norma de un vector es finita $ \|a\|^{2}=\left< a \right. \left| a \right>  < \infty$, la combinación lineal de los elementos de una base infinita, $\{ \left| e_{i} \right> \}$, converja al vector  $\left| a \right>$. Esto es $ a^{i}  \left| e_{i} \right>  \stackrel{n \rightarrow \infty }{\longrightarrow} \left| a \right>$. 

Para el caso de $E^{n}$ es inmediato que, dada una base (ortonormal, por ejemplo)
\[
 \left| a \right> = a^{i}  \left| e_{i} \right>  \quad \Rightarrow  \| a \|^{2} = \left< a \right. \left| a \right> = a^{i} a_{i} < \infty \qquad \text{con } i=1,2,3, \cdots n
\]  
La norma es finita, por cuanto es la suma de términos finitos (las componentes del vector $ (a^{1}, a^{2}, a^{3}, \cdots a^{n}) $). Sin embargo, para el caso de $E^\infty$  las componentes del vector serán función de las sumas parciales, esto es hasta dónde desarrollemos la serie y debemos demostrar que si
\[
\left| a_{n} \right>  \Leftrightarrow (a^{1}_{n}, a^{2}_{n}, a^{3}_{n}, a^{4}_{n},\cdots a^{n}_{n}) \quad \stackrel{n \rightarrow \infty }{\longrightarrow} \left| a_{\infty} \right>  \Leftrightarrow (a^{1}_{\infty}, a^{2}_{\infty}, a^{3}_\infty,\cdots a^{j}_\infty,\cdots) \quad \Rightarrow \| \left| a_{\infty} \right> - \left| a_{n} \right> \| < \epsilon
\]
Es decir que, efectivamente, componente a componente el vector $\left| a_{n} \right>$ converja al vector $\left| a \right>$. El criterio de convergencia de Cauchy, en este caso significa que: dadas dos sumas parciales (desarrollos parciales en una determinada base infinita $\{ \left| e_{i} \right> \}$)  $ \left| a_{n} \right> = a^{i}  \left| e_{i} \right>$ con $ i=1,2,\cdots n$ y $\left| a_{m} \right> = a^{j}  \left| e_{j} \right>$ con  $j=1,2,\cdots m$ entonces 
\[
 \| \left| a_{m} \right> - \left| a_{n} \right> \| = \|   \left| a_{m} \right> -\left| a \right>  - \left| a_{n} \right> + \left| a \right>  \| \leq \| \left| a \right>  - \left| a_{n} \right> \| + \| \left| a \right>  - \left| a_{m} \right> \| < \epsilon' + \epsilon''  \equiv \epsilon
\]
con lo cual las diferencias en las sumas parciales serán siempre menor que un $0 < \epsilon < 1$. Nótese que hemos utilizado la desigualdad triangular $\| x + y\| \leq \|x\| + \| y \| $, y esa misma desigualdad triangular nos garantiza que
\[
| a^{j}_{n} -a^{j}_{m} |^{2} \leq \sum_{j =1}^\infty | a^j_n -a^j_m |^2 \equiv   \| \left| a_m \right> - \left| a_n \right> \|^2 < \epsilon
\]
vale decir, hemos demostrado que el término $j-$esimo (y con ello todas las componentes del vector) de una suma parcial, converge al término correspondiente de la serie límite. Esto es  $a^j_n \stackrel{n \rightarrow \infty }{\longrightarrow}  a^j_m \stackrel{n \rightarrow \infty }{\longrightarrow}  a^j $ por lo tanto que las combinación lineal converge al vector y nos queda por demostrar si su norma es finita, o lo que es lo mismo, $ \left< a \right. \left| a \right> = a^i a_i < \infty$  con  $i=1,2,3, \cdots \infty$. Es claro que
\[
\sum_{j=1}^{M} | a^{j}_{n} -a^{j}_{m} |^{2} \leq \sum_{j =1}^\infty | a^{j}_{n} -a^j_m |^2 \equiv   \| \left| a_m \right> - \left| a_n \right> \|^2 < \epsilon
\]
con lo cual si $m \rightarrow \infty$ tendremos que $\sum_{j=1}^M | a^j_n -a^j |^2 < \epsilon$ y si ahora hacemos  
\[
M \rightarrow \infty \quad \Rightarrow  \sum_{j=1}^{\infty} | a^{j}_n -a^{j} |^2 < \epsilon \quad \Rightarrow  \left< a \right. \left| a \right> 
= \sum_{j=1}^\infty |a^{j}|^2 \equiv  \sum_{j=1}^\infty |a^{j} +a^{j}_n -a^{j}_{n}|^2 
\]
Ahora bien, para $\alpha$ y $\beta$ complejos, se cumple que
\[
\left(| \alpha | - | \beta | \right)^{2} \equiv | \alpha |^{2} + | \beta |^{2} -2 | \alpha |  | \beta | \geq 0  \Rightarrow 2 | \alpha |  | \beta | \leq \alpha |^2 + | \beta |^2 \Rightarrow | \alpha + \beta |^2 \leq | |\alpha| + |\beta| |^2 =  | \alpha |^2 + | \beta |^2 + 2| \alpha |  | \beta |
\]
para que finalmente, tengamos que 
\[
\left( | \alpha | - | \beta | \right)^{2} \leq 2 \left(  | \alpha |^{2} + | \beta |^{2} \right)
\]
Finalmente, podemos aplicarlo al caso que nos compete
\[
 \left< a \right. \left| a \right>  \equiv  \sum_{j=1}^{\infty} | a^{j} +a^{j}_{n} - a^{j}_{n} |^{2}  \leq 2 \left(  \sum_{j=1}^{\infty} | a^{j} -a^{j}_{n} |^{2} + \sum_{j=1}^{\infty} | a^{j}_{n} |^{2}  \right) < \infty
\]

\subsection{Conjunto completo de funciones}
El segundo sentido de completitud con que el conjunto (funciones) de vectores base expandan la totalidad del espacio vectorial (de funciones). Esto es, si $\{ \left| \textbf{u}_{i} \right> \} \Leftrightarrow \{ u_{i}(x) \}$ es una base ortonormal para $E^{\infty} $
\[
 \left| \mathbf{a} \right> = a^{i}  \left| \mathbf{u}_{i} \right>  \quad \Rightarrow  \| \left|\mathbf{ a} \right>  \|^{2} = \left< \mathbf{a} \right. \left|\mathbf{ a} \right> = a^{i} a_{i} = \sum_{k=1}^{\infty} | a_{k} |^{2} \qquad \text{con } i=1,2,3, \cdots \infty
\]
Es, otra vez, la misma afirmación que consideramos en el caso de un espacio finito dimensional, $E^n$ en el cual demostramos que una base $\{ \left| \textbf{u}_{i} \right> \}$ con $i=1,2,3,\cdots,n$ expandía todo el espacio.

 Si adicionalmente existe una función \textit{cuadrado integrable}, $\mathcal{L}^{2}_{\left[ a, b \right]}$ definidas en el intervalo $\left[ a, b \right]$, la cual pueda ser aproximada por la base 
 \[
\| \left| \mathbf{f} \right>   \|^{2}  \equiv \left<  \mathbf{f} \right. \left| \mathbf{f} \right> < \infty  \Rightarrow 
 \left| \mathbf{f} \right> = \sum_{i=0}^{\infty} c^{i} \left| \mathbf{u}_{i} \right> \sim  \sum_{i=0}^{N} c^{i} \left| \mathbf{u}_{i} \right>  \Leftrightarrow \| f(x) \|^{2} \equiv \int_{a}^{b} \mathrm{d}x | f(x) |^{2} \Rightarrow 
 f(x) \sim \sum_{j=0}^{N} c^{j} \ u_{j}(x) 
\]
Nótese que hemos supuesto la existencia de un producto interno y si las bases son ortonormales tendremos que
\[
\left<  \mathbf{g} \right. \left| \mathbf{f} \right>  \equiv \int_{a}^{b} \mathrm{d}x \ g^{\ast}(x) f(x) \quad \Rightarrow 
\left<  \mathbf{u}^{k} \right. \left| \mathbf{u}_{l} \right> \equiv \int_{a}^{b} \mathrm{d}x \ u^{\ast  k}(x) u_{l}(x) =\delta^{k}_{l} \quad \Rightarrow \| f(x) \|^{2} \equiv \int_{a}^{b} \mathrm{d}x | f(x) |^{2} = \sum_{j=0}^{\infty} | c^{j} |^{2}
\]
donde 
\[ c^{k} = \int_{a}^{b} \mathrm{d}x \ u^{\ast  k}(x)  f(x) \]

Para demostrar que $E^{\infty} $ es completo, comenzamos por demostrar la llamada \textit{Desigualdad de Bessel}. Esta es: dada una base ortonormal infinita, $\{ \left| \textbf{u}_{i} \right> \} \Leftrightarrow \{ u_{i}(x) \}$ para un espacio vectorial de Hilbert, $E^{\infty}$, de funciones cuadrado integrable $ f(x) \in \mathcal{L}^{2}_{\left[ a, b \right]} $, con un producto interno definido por $ \left<  \mathbf{g} \right. \left| \mathbf{f} \right>  \equiv \int_{a}^{b} \mathrm{d}x \ g^{\ast}(x) f(x) $,  entonces se cumple que 
\[
\| f(x) \|^{2} \geq  \sum_{k=1}^{\infty} | c_{k} |^{2} \quad \text{con }  c^{k} = \left< \mathbf{u}^{k} \right.  \left| \mathbf{f} \right>  = \int_{a}^{b} \mathrm{d}x \ u^{\ast  k}(x)  f(x) \quad \wedge \quad \left<  \mathbf{g} \right. \left| \mathbf{f} \right>  \equiv \int_{a}^{b} \mathrm{d}x \ g^{\ast}(x) f(x) 
\]
Para demostrar la desigualdad de Bessel, partimos de una afirmación obvia en espacios finito dimensionales
\[
0 \leq \| \left| \mathbf{f} \right>  -  c^{i} \left| \mathbf{u}_{i} \right> \|^{2} \equiv 
\left[ \left< \mathbf{f} \right| - c^{\ast}_{k} \left< \mathbf{u}^{k} \right| \right] \left[ \left| \mathbf{f} \right>  -  c^{i} \left| \mathbf{u}_{i} \right>  \right] =  \|  \left| \mathbf{f} \right>  \|^{2} - c^{\ast}_{k} \underbrace{ \left< \mathbf{u}^{k} \right.  \left| \mathbf{f} \right> }_{c^{k}} -  c^{i} \underbrace{\left< \mathbf{f} \right. \left| \mathbf{u}_{i} \right>}_{c^{\ast}_{i}} + c^{\ast}_{k} c^{i}\underbrace{ \left< \mathbf{u}^{k} \right.  \left| \mathbf{u}_{i} \right>}_{\delta_{i}^{k}}
\]
donde $k, i=1,2,3, \cdots n$ Entonces, queda demostrada la desigualdad de Bessel al tomar el límite $n \rightarrow \infty$
\[
0 \leq \|  \left| \mathbf{f} \right>  \|^{2} - \sum_{k =1}^{n}  | c_{k} |^{2} \quad \stackrel{n \rightarrow \infty}{\Longrightarrow} \| \left| \mathbf{f} \right>  \|^{2} \geq  \sum_{k =1}^{n}  | c_{k} |^{2}
\]

Si definimos el error, $M_{n}$, que se comete al aproximar una función con su expansión hasta un término $n-$simo como $M_{n}(b-a) \equiv \| \left| \mathbf{f} \right>  -  \alpha^{i} \left| \mathbf{u}_{i} \right> \|^{2} $ demostraremos que $M_{n}$ es mínima si $ \alpha^{i} = c^{i} = \left< \mathbf{u}^{i} \right.  \left| \mathbf{f} \right> $
Para ello procedemos como es costumbre, partiendo de la definición que acabamos de hacer y nos concentramos en el caso finito dimiensional 
\[
0 \leq M_{n}(b-a) \equiv \| \left| \mathbf{f} \right>  -  \alpha^{i} \left| \mathbf{u}_{i} \right> \|^{2} = 
 \| \left| \mathbf{f} \right>  -  (\alpha^{i} -c^{i}) \left| \mathbf{u}_{i} \right>  - c^{k} \left| \mathbf{u}_{k} \right>  \|^{2}
 \]
Desarrollando
\begin{align*}
M_{n}(b-a) =     &   \left[ \left< \mathbf{f} \right| - (\alpha^{\ast}_{k} - c^{\ast}_{k} ) \left< \mathbf{u}^{k} \right| \right.  - c^{\ast}_{k} \left< \mathbf{u}^{k}  \right] 
\left[ \left| \mathbf{f} \right> - (\alpha^{i} -  c^{i}) \left| \mathbf{u}_{i} \right>  -  c^{i} \left| \mathbf{u}_{i} \right>  \right]  \\
  =  & \|  \left| \mathbf{f} \right>  \|^{2} - c^{\ast}_{i}(\alpha^{i} -  c^{i})  -  2c^{\ast}_{i}c^{i} -  (\alpha^{\ast}_{k} -       c^{\ast}_{k} ) c^{k} + \sum_{j=1}^{n} \| \alpha^{j} -  c^{j} \|^2 +  (\alpha^{\ast}_{k} - c^{\ast}_{k} ) c^{k} + c^{\ast}_{i}(\alpha^{i} -  c^{i}) +c^{\ast}_{i}c^{i}
  \\
  = & \left[ \| \left| \mathbf{f} \right>  \|^{2} - \sum_{i=1}^{n} \| c^{i} \|^2 \right] +  \sum_{j=1}^{n} \| \alpha^{j} -  c^{j} \|^2
\end{align*}
Pero la desigualdad de Bessel garantiza que la cantidad entre corchetes es positiva, por lo tanto $M_{n}$ es mínima (y la denotaremos $\tilde{M}_{n}$ ) cuando seleccionamos $\alpha^{j} = c^{j}$. Más aún $\tilde{M}_{n}$ decrece cuando $n \rightarrow \infty$, vale decir
\[
\tilde{M}_{n}(b-a) =  \| \left| \mathbf{f} \right>  \|^{2} - \sum_{i=1}^{n} \| c^{i} \|^2 \quad \stackrel{n \rightarrow \infty}{\Longrightarrow} \quad \tilde{M}_{\infty}(b-a) =  \| \left| \mathbf{f} \right>  \|^{2} - \sum_{i=1}^{\infty} \| c^{i} \|^2
\]
y si, adicionalemente tenemos que $\tilde{M}_{n} \rightarrow 0$ cuando $n \rightarrow \infty$ entonces es claro que 
\[
 \| \left| \mathbf{f} \right>  \|^{2} = \sum_{i=1}^{\infty} \| c^{i} \|^{2} \quad \Longrightarrow \quad  \{ \left| \textbf{u}_{i} \right> \} \Leftrightarrow \{ u_{i}(x) \} \qquad \text{es completa} 
\]
Este noción de convergencia se denomina como \textit{convergencia al promedio}

Si adicionalmente exigimos que la serie $ c^{i} \left| \mathbf{u}_{i} \right> $ converja uniformemente para 
$x \in \left[a,b \right]$ entonces es claro que 
\[
\int_{a}^{b} \mathrm{d}x \  \|  f(x) - c^{i} \left| \mathbf{u}_{i} \right> \|^{2} =0 \quad \Longrightarrow \left| \mathbf{f} \right>  = c^{i} \left| \mathbf{u}_{i} \right> \quad ( \text{con } i=1,2,3\cdots , \infty) \quad
 \Leftrightarrow  f(x) = \sum_{i=1}^{\infty} c^{i} u_{i}(x) 
\]
Con lo cual enumeraremos las condiciones para la cual exigiremos que una función pueda ser expresada en términos de una base completa de funciones. 
\begin{itemize}
  \item Que $f(x)$ sea cuadrado integrable $ f(x) \in \mathcal{L}^{2}_{\left[ a, b \right]} $
  \item Que la base sea completa,  $\{ \left| \textbf{u}_{i} \right> \} \Leftrightarrow \{ u_{i}(x) \}$ i.e. $ \| \left| \mathbf{f} \right>  \|^{2} = \sum_{i=1}^{\infty} \| c^{i} \|^{2}$
  \item Que la serie $c^{i} \left| \mathbf{u}_{i} \right> \Leftrightarrow \sum_{i=1}^{\infty} c^{i} u_{i}(x) $ converja uniformemente, para $x \in \left[a,b \right]$
\end{itemize}
\section{Series de Polinomios Ortogonales}
Enunciaremos un teorema debido a Weierstrass el cual garantiza que una función contínua en un intervalo $[a,b]$ puede ser aproximada uniformemente por una serie de polinomios. Por lo tanto, cualquier función contínua podrá ser aproximada por combinaciones lineales de potencias.

El Teorema de aproximación polinómica de Weiernstrass queda enunciado como sigue. Cualquier función contínua $f(x)$ en un intervalo cerrado $x \in \left[a,b \right]$ podrá ser aproximada uniformente por polinomios en ese mismo intervalo si, para un $n$ suficientemente grande y un $\epsilon$ suficientemente pequeño siempre se tiene que 
\[
\| \mathcal{P}_{n}(x) - f(x) \| < \epsilon \qquad \forall \ x \in [a,b]
\]
Para la demostración de este teorema puede consultar \cite{ByronFuller1970,Cushing1975}. Sin embargo la aceptación de este teorema nos permitirá desarrollar las secciones que siguientes...
\subsection{Polinomios de Legendre}
El primero de los ejemplos de una base ortonormal de polinomios en la cual podremos expresar cualquier función contínua en el intervalo cerrado  $x \in \left[-1,1 \right]$ serán los \textit{Polinomios de Legendre}. Estos vienen construidos a partir de la Fórmula de Rodrígues
\[
P_{n}(x)=\frac1{n!2^{n}}\frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}(x^{2}-1)^{n}, \qquad n=0,1,2,.....
\]
con $P_{0}(x)=1.$

Es decir
\begin{align*}
 P_{0}(x) = 1   						 &  & P_{1}(x) =  x  				\\
 P_{2}(x)=1-3x^{2}  					 &  & P_{3}(x)  =  x-\frac{5}{3}x^{3}	 \\
 P_{4}(x)= \frac{3}{8}+{\frac {35}{8}}\,{x}^{4}-{\frac {15}{4}}\,{x}^{2} &  &	 P_{3}(x)  = {\frac{63}{8}} \,{x}^{5} - {\frac{35}{4}} \, {x}^{3} + {\frac{15}{8}}\,x
 		\\
	       \vdots 					         &  &	  	       \vdots 	
\end{align*}

\subsubsection{Generalidades de los Polinomios de Legendre}
Es fácil comprobar que los polinomios de Legendre $| \mathbf{P}_{\alpha} \rangle \leftrightarrow P_{\alpha}(x)$ son mutuamente ortogonales para un producto interno definido como
\[
\langle\mathbf{P}_{n}|\mathbf{P}_{m}\rangle=\int_{-1}^{1}P_{n}(x)P_{m}(x)\mathrm{d}x=\frac{2}{2n+1}\delta_{nm}%
\]
con norma definida por%
\[
\left\|  \mathbf{P}_{n}\right\|  ^{2}=\langle\mathbf{P}_{n}|\mathbf{P}_{n}\rangle=\int_{-1}^{1}P_{n}^{2}(x)\mathrm{d}x\check{=}\frac{2}{2n+1}%
\]
nótese que los polinomios de Legendre, calculados a partir de la Fórmula de Rodrigues no están normalizados.

Al ser los Polinomios de Legendre un conjunto completo de funciones, ellos expanden el espacio de funciones contínuas en el intervalo cerrado $x \in \left[-1,1 \right]$. Por ello cualquier función en el intervalo $\left[  -1,1\right]  $ puede ser expresada en esa base.
\[
f(x)=|\mathbf{F}\rangle=\sum_{k=0}^{\infty}a_{k}\ |\mathbf{P}_{k}\rangle = \sum_{k=0}^{\infty}\frac{\langle\mathbf{P}_{k}|\mathbf{F}\rangle}{\langle\mathbf{P}_{k}|\mathbf{P}_{k}\rangle}\ |\mathbf{P}_{k}\rangle
\]
Varios ejemplos ilustrarán esta aplicación

Si $f(x)$ es un polinomio
\[
f(x)=\sum_{n=0}^{m}b_{n}x^{n}=\sum_{k=0}^{\infty}a_{k}\ |\mathbf{P}_{k}\rangle=\sum_{n=0}^{\infty}a_{n}P_{n}(x)
\]
no se requiere hacer ninguna integral por cuanto los coeficientes $a_{n}$ se determinan a través de un sistema de ecuaciones algebraicas. Para el caso de $f(x)=x^{2}$ tendremos
\begin{align*}
f(x)  & =x^{2}=a_{0}P_{0}(x)+a_{1}P_{1}(x)+a_{2}P_{2}(x)\\
f(x)  & =x^{2}=a_{0}+a_{1}x+\frac12a_{2}(3x^{2}-1)\\
f(x)  & =x^{2}=\frac13P_{0}(x)+\frac23P_{2}(x)
\end{align*}
En el caso de una función mas complicada
\[
f(x)=\sqrt{\frac{1-x}2}=\sum_{k=0}^{\infty}\frac{\langle\mathbf{P}_{k}|\mathbf{F}\rangle}{\langle\mathbf{P}_{k}|\mathbf{P}_{k}\rangle}\ |\mathbf{P}_{k}\rangle
\]
\[
\langle\mathbf{P}_{k}|\mathbf{F}\rangle=\int_{-1}^{1}f(x)P_{k}(x)\mathrm{d}x=\int_{-1}^{1}\sqrt{\frac{1-x}2}P_{k}(x)\mathrm{d}x
\]
la expansión en series de Legendre quedaría cómo
\[
\sqrt{\frac{1-x}2}=\frac23P_{0}(x)-2\sum_{n=1}^{\infty}\frac{P_{n}(x)}{\left(2n-1\right)  \left(  2n+3\right)  }
\]

Antes de entrar en el detalle de las propiedades de estos polinomios, hay que enfatizar que los Polinomios de Legendre constituyen la única base ortogonal para un espacio de Hilbert con un producto interno definido como el producto simple de funciones en el intervalo cerrado. Al ortonormalizar mediante Gram Schmidt la base  $\left\{1,x,x^{2},x^{3},\cdots,x^{n}, \cdots \right\}  $ del espacio de polinomios, $\mathcal{P}^{n},$ de grado $n$ en el intervalo $\left[  -1,1\right]  $, con el producto interno definido
por $\left\langle \mathbf{f}\right|  \left.  \mathbf{g}\right\rangle =\int_{-1}^{1}\mathrm{d}x\ f\left(  x\right)  \ g\left(  x\right)  .$ se obtienen los polinomios de Legendre.

Los polinomios de Legendre surgen, originalmente, como soluciones a la ecuación diferencial ordinaria del tipo 
\[
(1-x^{2})\ \frac{\mathrm{d}^2 P_{n}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} P_{n}(x)}{\mathrm{d} x} + n(n+1)\ P_{n}(x) =0
\]
Vale decir
\begin{center}
\begin{tabular}
[c]{lcc}\hline\hline
$n$ & Ecuación de Legendre & Solución\\\hline\hline
$0$ & \multicolumn{1}{r}{$(1-x^{2})\ \frac{\mathrm{d}^2 P_{0}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} P_{0}(x)}{\mathrm{d} x}=0$} & \multicolumn{1}{l}{$P_{0}(x)=1$}\\ \\
$1$ & \multicolumn{1}{r}{$(1-x^{2})\ \frac{\mathrm{d}^2 P_{1}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} P_{1}(x)}{\mathrm{d} x}+2\ P_{1}(x)=0$} & \multicolumn{1}{l}{$P_{1}(x)=x$}\\ \\
$2$ & \multicolumn{1}{r}{$(1-x^{2})\ \frac{\mathrm{d}^2 P_{2}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} P_{2}(x)}{\mathrm{d} x} +6\ P_{2}(x)=0$} & \multicolumn{1}{l}{$P_{2}(x)=1-3x^{2}$}\\ \\
$3$ & \multicolumn{1}{r}{$(1-x^{2})\ \frac{\mathrm{d}^2 P_{3}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} P_{3}(x)}{\mathrm{d} x} +12\ P_{3}(x) = 0$} & \multicolumn{1}{l}{$P_{3}(x)=x-\frac{5}{3}x^{3}$}\\ \\
$4$ & \multicolumn{1}{r}{$(1-x^{2})\ \frac{\mathrm{d}^2 P_{4}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} P_{4}(x)}{\mathrm{d} x} +20\ P_{4}(x)=0$} & \multicolumn{1}{l}{$P_{4}(x)=1-10x^{2}+\frac{35}{3}x^{4}$}\\ \hline\hline
\end{tabular}
\end{center}

\subsubsection{Ortogonalidad de los Polinomios de Legendre}

Como los polinomios de Legendre son soluciones de su ecuaciones
\begin{align*}
(1-x^{2})\ P_{\alpha}(x)^{\prime\prime}-2x\ \ P_{\alpha}(x)^{\prime} +\alpha(\alpha+1)\ P_{\alpha}(x)  & =0 \\
(1-x^{2})\ P_{\beta}(x)^{\prime\prime}-2x\ \ P_{\beta}(x)^{\prime}+\beta (\beta+1)\ P_{\beta}(x)  & =0
\end{align*}
Nótese que hemos cambiado de notación del operador diferencial
\[
 P_{\alpha}(x)^{\prime\prime} \leftrightarrow \frac{\mathrm{d}^2 P_{\alpha}(x)}{\mathrm{d} x^2} \qquad P_{\beta}(x)^{\prime}  \leftrightarrow \frac{\mathrm{d} P_{\beta}(x)}{\mathrm{d} x}
\]
Acomodando y restando ambas ecuaciones
\[
(1-x^{2})\left\{  \ P_{\beta}(x)P_{\alpha}(x)^{\prime\prime}-\ P_{\alpha}(x)P_{\beta}(x)^{\prime\prime}\right\}  
-2x\ \left\{  P_{\beta}(x)P_{\alpha}(x)^{\prime}-P_{\alpha}(x)P_{\beta}(x)^{\prime}\right\}  
+\left\{  \alpha(\alpha+1)-\beta(\beta+1)\right\}  P_{\beta}(x)P_{\alpha}(x) =0
\]
el primer término de la ecuación puede interpretarse una la derivada
\[
\left[  (1-x^{2})\left\{  \ P_{\beta}(x)P_{\alpha}(x)^{\prime}-\ P_{\alpha}(x)P_{\beta}(x)^{\prime}\right\}  \right]  ^{\prime}%
\]
por lo tanto al integrar
\[
\left.  (1-x^{2})\left\{  \ P_{\beta}(x)P_{\alpha}(x)^{\prime}-\ P_{\alpha}(x)P_{\beta}(x)^{\prime}\right\}  \right|  _{-1}^{1}  +
\left\{  \alpha(\alpha+1)-\beta(\beta+1)\right\}  \int_{-1}^{1}P_{\alpha}(x)P_{\beta}(x)\mathrm{d}x   =0
\]
El primer término de la ecuación se anula en los extremos y es fácil comprobar que los polinomios de Legendre $|\mathbf{P}_{\alpha}\rangle=P_{\alpha}(x)$ son mutuamente ortogonales con un producto interno definido como
\[
\langle\mathbf{P}_{\alpha}|\mathbf{P}_{\beta}\rangle=\int_{-1}^{1}P_{\alpha
}(x)P_{\beta}(x)\mathrm{d}x\propto\delta_{\alpha\beta}%
\]
\subsubsection{Relación de Recurrencia}
Conocido esto se puede generar una relación de recurrencia. Supongamos que
conocemos todos los polinomios de Legendre hasta $P_{n}(x)$ y queremos generar
el próximo. Obviamente el ese polinomio será de grado $n+1$ y nos
plantemos generarlo a partir de $xP_{n}(x)$ así como los estos polinomios
son base del espacio de funciones, entonces
\[
xP_{n}(x)=|x\mathbf{P}_{n}\rangle=\sum_{k=0}^{n+1}\frac{\langle\mathbf{P}%
_{k}|x\mathbf{P}_{n}\rangle}{\langle\mathbf{P}_{k}|\mathbf{P}_{k}\rangle
}\ |\mathbf{P}_{k}\rangle
\]
en donde
\[
\langle\mathbf{P}_{k}|x\mathbf{P}_{n}\rangle=\langle x\mathbf{P}%
_{k}|\mathbf{P}_{n}\rangle=\int_{-1}^{1}P_{n}(x)xP_{k}(x)\mathrm{d}x=0
\]
para $k<n-1.$ Sobreviven entonces tres términos
\[
|x\mathbf{P}_{n}\rangle=xP_{n}(x)=\frac{\langle\mathbf{P}_{n-1}|x\mathbf{P}%
_{n}\rangle}{\langle\mathbf{P}_{n-1}|\mathbf{P}_{n-1}\rangle}\ |\mathbf{P}%
_{n-1}\rangle+\frac{\langle\mathbf{P}_{n}|x\mathbf{P}_{n}\rangle}%
{\langle\mathbf{P}_{n}|\mathbf{P}_{n}\rangle}\ |\mathbf{P}_{n}\rangle
+\frac{\langle\mathbf{P}_{n+1}|x\mathbf{P}_{n}\rangle}{\langle\mathbf{P}%
_{n+1}|\mathbf{P}_{n+1}\rangle}\ |\mathbf{P}_{n+1}\rangle
\]
y dado que
\[
\langle\mathbf{P}_{n}|x\mathbf{P}_{n}\rangle=\int_{-1}^{1}P_{n}(x)xP_{n}%
(x)\mathrm{d}x=\int_{-1}^{1}xP_{n}^{2}(x)\mathrm{d}x\quad,
\]
es una función impar, entonces $\langle\mathbf{P}_{n}|x\mathbf{P}%
_{n}\rangle=0.$ Entonces
\[
|x\mathbf{P}_{n}\rangle=xP_{n}(x)=\frac{\langle\mathbf{P}_{n-1}|x\mathbf{P}%
_{n}\rangle}{\langle\mathbf{P}_{n-1}|\mathbf{P}_{n-1}\rangle}\ |\mathbf{P}%
_{n-1}\rangle+\frac{\langle\mathbf{P}_{n+1}|x\mathbf{P}_{n}\rangle}%
{\langle\mathbf{P}_{n+1}|\mathbf{P}_{n+1}\rangle}\ |\mathbf{P}_{n+1}\rangle
\]
Es decir
\[
xP_{n}(x)=AP_{n+1}(x)+BP_{n-1}(x)
\]
desarrollando con la fórmula de Rodríguez el coeficiente de orden $k$
del lado izquierdo es
\[
\frac{1}{2^{k}k!}2k(2k-1)\cdots\left[  2k-(k-1)\right]  =\frac{(2k)!}%
{2^{k}(k!)^{2}}%
\]
mientras que el primer término del lado izquierdo, hasta orden $k-2$ queda
como
\[
\frac{(2k-2)!}{2^{k}(k-2)!(k-1)}%
\]
por lo cual
\[
A=\frac{n+1}{2n+1}%
\]
De igual forma se determina $B$ igualando coeficientes a orden $n-1$ y queda
la relación de recurrencia:
\[
\left(  n+1\right)  P_{n+1}(x)=\ \left(  2n+1\right)  xP_{n}(x)-nP_{n-1}(x)
\]
\subsubsection{Norma de los Polinomios de Legendre}
Conociendo que la
ortogonalidad de los polinomios de Legendre y la relación de recurrencia,
procedemos encontrar el valor de su norma
\[
\left\|  \mathbf{P}_{n}\right\|  ^{2}=\langle\mathbf{P}_{n}|\mathbf{P}%
_{n}\rangle=\int_{-1}^{1}P_{n}^{2}(x)\mathrm{d}x\check{=}\frac2{2n+1}
\]
De la relación de recurrencia
\begin{align*}
\left(  2n+1\right)  P_{n}(x)nP_{n}(x)  & =\left(  2n+1\right)  P_{n}%
(x)\left[  \left(  2n-1\right)  \ xP_{n-1}(x)-\left(  n-1\right)
P_{n-2}(x)\right] \\
\left(  2n-1\right)  P_{n-1}(x)\left(  n+1\right)  P_{n+1}(x)  & =\left(
2n-1\right)  P_{n-1}(x)\ \left[  \left(  2n+1\right)  xP_{n}(x)-nP_{n-1}%
(x)\right]
\end{align*}
restando miembro a miembro obtenemos:
\begin{align*}
\left(  2n+1\right)  P_{n}(x)nP_{n}(x)+\left(  2n+1\right)  \left(
n-1\right)  P_{n}(x)P_{n-2}(x)-\  & \\
-\left(  n+1\right)  \left(  2n-1\right)  P_{n-1}(x)P_{n+1}(x)-\left(
2n-1\right)  nP_{n-1}^{2}(x)  & =0
\end{align*}
integrando y considerando la ortogonalidad
\begin{align*}
\int_{-1}^{1}P_{n}^{2}(x)\mathrm{d}x  & =\frac{2n-1}{2n+1}\int_{-1}^{1}%
P_{n-1}^{2}(x)\mathrm{d}x\\
\int_{-1}^{1}P_{n}^{2}(x)\mathrm{d}x  & =\left(  \frac{2n-1}{2n+1}\right)
\left(  \frac{2n-3}{2n-1}\right)  \int_{-1}^{1}P_{n-2}^{2}(x)\mathrm{d}x\\
\int_{-1}^{1}P_{n}^{2}(x)\mathrm{d}x  & =\left(  \frac{2n-1}{2n+1}\right)
\left(  \frac{2n-3}{2n-1}\right)  \left(  \frac{2n-5}{2n-3}\right)  \int
_{-1}^{1}P_{n-3}^{2}(x)\mathrm{d}x\\
\vdots\qquad & =\qquad\vdots\\
\int_{-1}^{1}P_{n}^{2}(x)\mathrm{d}x  & \check{=}\frac3{2n+1}\int_{-1}%
^{1}P_{1}^{2}(x)\mathrm{d}x\\
\int_{-1}^{1}P_{n}^{2}(x)\mathrm{d}x  & \check{=}\frac2{2n+1}%
\end{align*}

\subsubsection{Función Generatriz de los Polinomios de Legendre}
Se puede
encontrar una función generatriz $\mathcal{P}(t,x)$ de los polinomios de
Legendre:
\[
\mathcal{P}(t,x)=\frac1{\sqrt{1-2xt+t^{2}}}=P_{0}(x)\ +P_{1}(x)\ t+P_{2}%
(x)\ t^{2}+\cdots=\sum_{n=0}^{\infty}P_{n}(x)\ t^{n}
\]
para la cual los $P_{n}(x)$ son los coeficientes de su desarrollo en series de
potencias. Esta serie converge para $\left\|  2xt+t^{2}\right\|  <1.$ Para
demostrar que el desarrollo en serie de la función $\mathcal{G}(t,x)$
tiene como coeficientes a los $P_{n}(x)$ partimos de que:
\[
\mathcal{P}(t,x)=\frac1{\sqrt{1-2xt+t^{2}}}\quad\Rightarrow\quad\frac
{\partial\mathcal{P}(t,x)}{\partial t}=\frac{t-x}{\left(  1-2xt+t^{2}\right)
^{3/2}}\quad
\]
por lo cual
\[
\left(  t-x\right)  \mathcal{P}(t,x)+\left(  1-2xt+t^{2}\right)
\frac{\partial\mathcal{P}(t,x)}{\partial t}=0
\]
y, consecuentemente
\[
\left(  t-x\right)  \sum_{n=0}^{\infty}P_{n}(x)\ t^{n}+\left(  1-2xt+t^{2}%
\right)  \sum_{n=0}^{\infty}nP_{n}(x)\ t^{n-1}=0\ .
\]
Multiplicando y acomodando queda
\[
-x\ P_{0}(x)+\ P_{0}(x)\ t\ +\sum_{n=0}^{\infty}\left(  n+1\right) P_{n+1}(x) t^{n}   -\sum_{n=1}^{\infty}\left(  2n+1\right)  \ x\ P_{n}(x)\ t^{n}- \sum_{n=2}^{\infty}nP_{n-1}(x)\ t^{n}   =0
\]
por lo tanto
\[
\left[  \underbrace{P_{1}(x)-x\ P_{0}(x)}_{=0}\right]  
+\ \left[ \underbrace{2P_{2}(x)-3xP_{1}(x)+P_{0}(x)}_{=0}\right]  t
+\sum_{n=1}^{\infty}\left[  \underbrace{\left(  n+1\right)  P_{n+1}(x)-\left(2n+1\right)  \ xP_{n}(x)+nP_{n-1}(x)}_{=0}\right]   t^{n}   =0
\]
El primero de los términos se cumple siempre por cuanto $P_{0}(x)=1$ y $P_{1}(x)=x.$ El tercer término conforma la relación de recurrencia para los polinomios de Legendre. Con esto queda demostrado que el desarrollo en series de potencias de la función generatriz, tiene como coeficientes a los polinomios de Legendre.

La función generatriz muestra su utilidad en la expansión
\[
f(x)=\sqrt{\frac{1-x}2}=\sum_{k=0}^{\infty}\frac{\langle\mathbf{P}_{k}|\mathbf{F}\rangle}{\langle\mathbf{P}_{k}|\mathbf{P}_{k}\rangle}\ |\mathbf{P}_{k}\rangle 
\]
Así, al considerar la definición del producto interno
\[
\langle\mathbf{P}_{k}|\mathbf{F}\rangle=\int_{-1}^{1}f(x)P_{k}(x)\mathrm{d}%
x=\int_{-1}^{1}\sqrt{\frac{1-x}2}P_{k}(x)\mathrm{d}x
\] e integrando
\begin{align*}
{\displaystyle \int_{-1}^{1}} \sqrt{\frac{1-x}2}\left[  \frac1{\sqrt{1-2xt+t^{2}}}\right]  \mathrm{d}x  &
=\sum_{n=0}^{\infty}t^{n}
{\displaystyle\int_{-1}^{1}} \sqrt{\frac{1-x}2}P_{n}(x)\mathrm{d}x\\
\frac1{2t}\left[  1+t-\frac{\left(  1-t\right)  ^{2}}{2\sqrt{t}}
\ln \left(\frac{1+\sqrt{t}}{1-\sqrt{t}}\right)  \right]   & =\sum_{n=0}^{\infty}t^{n}{\displaystyle\int_{-1}^{1}}\sqrt{\frac{1-x}2}P_{n}(x)\mathrm{d}x
\end{align*}
Expandiendo el lado izquierdo en series de potencias de $t$%
\[
\frac43-4\sum_{n=1}^{\infty}\frac{t^{n}}{\left(  4n^{2}-1\right)  \left(
2n+3\right)  }=\sum_{n=0}^{\infty}t^{n}%
%TCIMACRO{\dint _{-1}^{1}}%
%BeginExpansion
{\displaystyle \int_{-1}^{1}}
%EndExpansion
\sqrt{\frac{1-x}2}P_{n}(x)\mathrm{d}x
\]
lo cual nos conduce, al igualar coeficientes a
\begin{align*}
\frac43  & =%
%TCIMACRO{\dint _{-1}^{1}}%
%BeginExpansion
{\displaystyle\int_{-1}^{1}}
%EndExpansion
\sqrt{\frac{1-x}2}P_{0}(x)\mathrm{d}x\\
\frac{-4}{\left(  4n^{2}-1\right)  \left(  2n+3\right)  }  & =%
%TCIMACRO{\dint _{-1}^{1}}%
%BeginExpansion
{\displaystyle\int_{-1}^{1}}
%EndExpansion
\sqrt{\frac{1-x}2}P_{n}(x)\mathrm{d}x
\end{align*}
y finalmente a la forma de la expansión en series
\[
\sqrt{\frac{1-x}2}=\frac23P_{0}(x)-2\sum_{n=1}^{\infty}\frac{P_{n}(x)}{\left(
2n-1\right)  \left(  2n+3\right)  }
\]

\subsubsection{Otras propiedades de los polinomios de Legendre}
\begin{itemize}
\item $P_{n}(1)=1$ y $P_{n}(-1)=(-1)^{n}$ para todo $n.$

\item $P_{n}(x)$ tiene $n$ raíces en el intervalo $\left(  -1,1\right)  $
Esta propiedad puede apreciarse para los primeros 5 polinomios en la figura \ref{poligendre}
\end{itemize}

\begin{center}%
\begin{figure}
[ptb]
\begin{center}
\includegraphics[
natheight=7.489300in,
natwidth=9.989500in,
height=3.3987in,
width=4.5238in
]%
{legendre1.jpg}%
\caption{Polinomios de Lengendre}%
\label{poligendre}%
\end{center}
\end{figure}
\end{center}

\begin{itemize}
\item  Tienen una representación integral de la forma
\[
P_{n}(x)=\frac1{2\pi}\int_{0}^{\pi}\left[  x+\sqrt{x^{2}-1}\cos\varphi\right]
^{n}\mathrm{d}\varphi
\]

\item  Cambios de variables inmediatos conllevan a ecuaciones diferenciales equivalentes

\begin{itemize}
\item  Forma autoadjunta
\[
\left[  (1-x^{2})\ y^{\prime}\right]  ^{\prime}+\lambda(\lambda+1)\ y=0
\]

\item  En coordenadas esféricas con $u=P_{n}(\cos\theta)$%
\[
\frac1{\operatorname*{sen}\theta}\frac{\mathrm{d}}{\mathrm{d}\theta}\left(
\operatorname*{sen}\theta\ \frac{\mathrm{d}u}{\mathrm{d}\theta}\right)
+\lambda(\lambda+1)u=0
\]

\item  En coordenadas esféricas con $u=\sqrt{\operatorname*{sen}\theta
}P_{n}(\cos\theta)$%
\[
\frac{\mathrm{d}^{2}u}{\mathrm{d}\theta^{2}}+\left[  \left(  \lambda
+\frac12\right)  ^{2}+\frac1{4\operatorname*{sen}^{2}\theta}\right]  u=0
\]
\end{itemize}
\end{itemize}
\subsubsection{Resumen de Propiedades Polinomios Legendre}
\begin{tabular}
[c]{|l|l|}\hline
\multicolumn{2}{|c|}{\textbf{Polinomios de Legendre}}\\\hline
Definición & $P_{n}(x)=\dfrac1{n!2^{n}}\dfrac{\mathrm{d}^{n}}%
{\mathrm{d}x^{n}}(x^{2}-1)^{n},\qquad n=0,1,2,.....$\\\hline
Ejemplos & $%
\begin{array}
[c]{c}%
P_{-1}\equiv0;\quad P_{0}\equiv1;\quad P_{1}=x\\
P_{2}=\frac12(3x^{2}-1);\quad P_{3}=\frac12(5x^{3}-3x)
\end{array}
$\\\hline
Relación de Recurrencia & $\left(  n+1\right)  P_{n+1}(x)=\ \left(
2n+1\right)  xP_{n}(x)-nP_{n-1}(x)$\\\hline
Ecuaciones Diferenciales & $%
\begin{array}
[c]{c}%
(1-x^{2})\ y^{\prime\prime}-2x\ y^{\prime}+\lambda(\lambda+1)\ y=0\\
\dfrac1{\operatorname*{sen}\theta}\dfrac{\mathrm{d}}{\mathrm{d}\theta}\left(
\operatorname*{sen}\theta\ \dfrac{\mathrm{d}u}{\mathrm{d}\theta}\right)
+n(n+1)u=0;\quad u=P_{n}(\cos\theta)
\end{array}
$\\\hline
Función Generatriz & $\mathcal{P}(t,x)=\dfrac1{\sqrt{1-2xt+t^{2}}}=%
%TCIMACRO{\dsum _{n=0}^{\infty}}%
%BeginExpansion
{\displaystyle\sum_{n=0}^{\infty}}
%EndExpansion
P_{n}(x)\ t^{n}$\\\hline
Representación Integral & $P_{n}(x)=\dfrac1{2\pi}%
%TCIMACRO{\dint _{0}^{\pi}}%
%BeginExpansion
{\displaystyle\int_{0}^{\pi}}
%EndExpansion
\left[  x+\sqrt{x^{2}-1}\cos\varphi\right]  ^{n}\mathrm{d}\varphi$\\\hline
Ortogonalidad & $\langle\mathbf{P}_{\alpha}|\mathbf{P}_{\beta}\rangle=%
%TCIMACRO{\dint _{-1}^{1}}%
%BeginExpansion
{\displaystyle\int_{-1}^{1}}
%EndExpansion
P_{\alpha}(x)P_{\beta}(x)\mathrm{d}x=\delta_{\alpha\beta}\dfrac2{2\alpha+1}%
$\\\hline
\end{tabular}%

\subsubsection{Potencial Electrostático de un Dipolo Eláctrico}
En Física el ejemplo claro es el cálculo del potencial electrostático producido por dos cargas $q_{1}=+q$ y $q_{2}=-q$ separadas por una distancia $2d$ en un punto $P$ cualquiera de un plano $\left(  x,y\right)  $. El potencial en ese punto
genérico viene dado por
\[
V=q\left(  \frac{1}{R^{\prime}}-\frac{1}{R}\right)
\]
\begin{figure}
[ptb]
\begin{center}
\includegraphics[
natheight=7.489300in,
natwidth=9.989500in,
height=3.3987in,
width=4.5238in
]
{potencial.jpg}%
\label{potencial1}%
\caption{Potencial Electrostático de un Dipolo Eláctrico}
\end{center}
\end{figure}
%EndExpansion

Tal y como puede apreciarse de la figura \ref{potencial1}
\begin{align*}
\left(  R^{\prime}\right)  ^{2}  & =r^{2}+d^{2}-2r\ d\cos\theta\\
R^{2}  & =r^{2}+d^{2}-2r\ d\cos\left(  \pi-\theta\right)
\end{align*}
por lo cual
\begin{align*}
\frac{1}{R^{\prime}}  & =\frac{1}{r}\left[  1-\left(  2\frac{d}{r}\ \cos
\theta-\left\{  \frac{d}{r}\right\}  ^{2}\right)  \right]  ^{-1/2}\\
\frac{1}{R}  & =\frac{1}{r}\left[  1-\left(  2\frac{d}{r}\ \cos\left(
\pi-\theta\right)  -\left\{  \frac{d}{r}\right\}  ^{2}\right)  \right]
^{-1/2}%
\end{align*}
y consecuentemente
\begin{align*}
\frac{1}{R^{\prime}}  & =\frac{1}{r}\sum_{n=0}^{\infty}P_{n}(\cos
\theta)\ \left\{  \frac{d}{r}\right\}  ^{n}\\
\frac{1}{R}  & =\frac{1}{r}\sum_{n=0}^{\infty}P_{n}\left(  \cos\left(
\pi-\theta\right)  \right)  \ \left\{  \frac{d}{r}\right\}  ^{n}=\frac{1}%
{r}\sum_{n=0}^{\infty}P_{n}(-\cos\theta)\ \left\{  \frac{d}{r}\right\}  ^{n}%
\end{align*}
El potencial será
\[
V=\frac{q}{r}\left(  \sum_{n=0}^{\infty}\left[  P_{n}(\cos\theta)-P_{n}%
(-\cos\theta)\ \right]  \ \left\{  \frac{d}{r}\right\}  ^{n}\right)
\]
donde todos los términos pares de $P_{n}(\cos\theta)$ se anula y finalmente tendremos la expresión del potencial para cualquier punto del plano
\[
V=\frac{2q}{r}\left(  \sum_{n=0}^{\infty}P_{2n+1}(\cos\theta)\ \left\{
\frac{d}{r}\right\}  ^{2n+1}\right)
\]
Entonces nos quedamos con el primer término de la serie, si
\[
\frac{d}{r}\ll1\quad\Rightarrow\quad V\approx\frac{q}{r^{2}}\ 2d\cos\theta
\]

\subsection{Polinomios de Hermite}
Los polinomios de Hermite a diferencia de los de Legendre (y Tchevychev), vienen definidos en toda la recta real, vale decir, $x \in (-\infty, \infty)$, por lo cual la función peso $w(x)$ en el producto interno deberá decrecer más rápido que $|x|^{n}$, para garantizar que la norma de los vectores en este espacio vectorial sea finita. La función más simple que cumple estos requisitos es $w(x)= e^{-x^{2}} $ (tambián algunos autores utilizan $w(x)= e^{-x^{2}/2})$ Esto es, el producto interno entre los polinomios de Hermite vendrá definido como
\[
\langle \mathbf{f}|\mathbf{g} \rangle =  \int_{-\infty}^{\infty}\mathrm{d}x \ w(x) f(x)g(x)=  \int_{-\infty}^{\infty}\mathrm{d}x \ e^{-x^{2}} f(x)g(x)
\]
Otra vez, para este producto interno, si ortogonalizamos con Gram-Schmidt se obtienen los polinomios de Hermite. Al igual que el resto de los polinomios ortogonales, existe una fórmula de Rodrigues para los polinomios de Hermite
\[
H_{n}(x) = (-1)^{n} \ e^{x^{2}} \frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}
\]
con lo cual se obtienen
\begin{align*}
 H_{0}(x) = 1,   						 &  & H_{1}(x) =  2x  				\\
 H_{2}(x) = 4x^{2} - 2,  					 &  & H_{3}(x)  =  8x^{3} -12x,	 \\
 P_{4}(x)= 16x^{4} -48 x^{2} +12	         &  &	 	H_{5}(x) = 32x^{5} -160x^{3} +120 x.     		\\
	       \vdots 					         &  &	  	       \vdots 	
\end{align*}

\subsubsection{Generalidades de los Polinomios de Hemite}
Los polinomios de Hermite serán ortogonales, pero no normales
\[
\langle\mathbf{H}_{\alpha}|\mathbf{H}_{\beta}\rangle=2^{n}n!\sqrt{\pi}\ \delta_{\alpha\beta}=\int_{-\infty}^{\infty}e^{-x^{2}}H_{\beta}(x)H_{\alpha}(x)\mathrm{d}x
\quad \Leftrightarrow \quad 
\langle\mathbf{H}_{\alpha}|\mathbf{H}_{\alpha}\rangle=\left\|  \mathbf{H}%
_{\alpha}\right\|  ^{2}=\int_{-\infty}^{\infty}e^{-x^{2}}H_{\alpha}%
^{2}(x)\mathrm{d}x=2^{n}n!\sqrt{\pi}
\]
Donde la función delta de Kronecker es $\delta_{\alpha\beta}=0$ si $\alpha\neq\beta$; y $\delta_{\beta\beta}=1.$  

Antes de desarrollar funciones en
tárminos de los polinomios de Hermite, expondremos un par de teoremas sin
demostración.

\textbf{Teorema 1}\newline Sean $|\ \mathbf{f\ }\rangle$ y $|\ \mathbf{g\ }%
\rangle$ dos funciones arbitrarias, cuando menos continuas a trozos en
$\left(  -\infty,\infty\right)  $ y que cumplen con
\[
\int_{-\infty}^{\infty}e^{-x^{2}}f^{2}(x)\mathrm{d}x<\infty\qquad\wedge
\qquad\int_{-\infty}^{\infty}e^{-x^{2}}g^{2}(x)\mathrm{d}x<\infty
\]
Entonces el conjunto de estas funciones forman un espacio vectorial Euclideano
$\mathcal{I}_{2}^{w}$ con un producto interno definido por
\[
\langle\ \mathbf{g}\ |\ \mathbf{f\ }\rangle=\int_{-\infty}^{\infty}e^{-x^{2}%
}f(x)g(x)\mathrm{d}x
\]
Las funciones $f(x)$ y $g(x)$ se denominan cuadrado-integrables respecto al
peso $w$. Es por ello que denotamos el espacio de funciones como
$\mathcal{I}_{2}^{w}$

\textbf{Teorema 2}\newline Si $f(x)$ es una función continua arbitraria en
$\mathcal{I}_{2}^{w}$ entonces puede ser aproximada por un polinomio en ese
mismo espacio. Es decir
\[
\lim_{n\rightarrow\infty}\left\|  f(x)-p_{n}(x)\right\|  =\lim_{n\rightarrow
\infty}\left(  \int_{-\infty}^{\infty}e^{-x^{2}}\left[  f(x)-p_{n}(x)\right]
^{2}\mathrm{d}x\right)  ^{1/2}=0
\]
As\'{\i}, la expresión de una función arbitraria en la base de los
polinomio de Hermite se reduce a
\[
f(x)=|\ \mathbf{f\ }\rangle=\sum_{k=0}^{\infty}a_{k}\ |\mathbf{H}_{k}%
\rangle=\sum_{k=0}^{\infty}\frac{\langle\mathbf{H}_{k}|\mathbf{f}\rangle
}{\langle\mathbf{H}_{k}|\mathbf{H}_{k}\rangle}\ |\mathbf{H}_{k}\rangle
\]
donde
\[
a_{k}=\frac{\langle\mathbf{H}_{k}|\ \mathbf{f\ }\rangle}{\langle\mathbf{H}%
_{k}|\mathbf{H}_{k}\rangle}\ =\frac{\int_{-\infty}^{\infty}e^{-x^{2}}%
f(x)H_{k}(x)\mathrm{d}x}{\int_{-\infty}^{\infty}e^{-x^{2}}H_{k}^{2}%
(x)\mathrm{d}x}=\frac1{2^{k}k!\sqrt{\pi}}\int_{-\infty}^{\infty}e^{-x^{2}%
}f(x)H_{k}(x)\mathrm{d}x
\]
Si $f(x)=x^{2p}$ con $p=1,2,3,\cdots$
\[
f(x)=x^{2p}=\sum_{k=0}^{p}a_{2k}\ H_{2k}(x)
\]
entonces
\begin{align}
a_{2k}  & =\frac1{2^{2k}(2k)!\sqrt{\pi}}\int_{-\infty}^{\infty}e^{-x^{2}%
}x^{2p}H_{2k}(x)\mathrm{d}x\\[0.03in]
& =\frac1{2^{2k}(2k)!\sqrt{\pi}}\int_{-\infty}^{\infty}x^{2p}\frac
{\mathrm{d}^{2k}}{\mathrm{d}x^{2k}}e^{-x^{2}}\mathrm{d}x\nonumber
\end{align}
Una integración por partes estratágica muestra que:
\[
a_{2k}=\frac1{2^{2k}(2k)!\sqrt{\pi}}\left\{  \left.  x^{2p}\frac
{\mathrm{d}^{2k-1}}{\mathrm{d}x^{2k-1}}e^{-x^{2}}\right|  _{-\infty}^{\infty
}-\int_{-\infty}^{\infty}2px^{2p-1}\frac{\mathrm{d}^{2k-1}}{\mathrm{d}%
x^{2k-1}}e^{-x^{2}}\mathrm{d}x\right\}
\]
El primer tármico de la resta se anula siempre debido a la defición de
los polinomios de Hermite
\[
\left.  x^{2p}\frac{\mathrm{d}^{2k-1}}{\mathrm{d}x^{2k-1}}e^{-x^{2}}\right|
_{-\infty}^{\infty}=\left.  x^{2p}(-1)^{2k-1}e^{-x^{2}}H_{2k-1}(x)\right|
_{-\infty}^{\infty}%
\]
Repitiendo el proceso $2k$ veces, tendremos
\[
a_{2k}=\frac1{2^{2k}(2k)!\sqrt{\pi}}\frac{\left(  2p\right)  !}{\left(
2p-2k\right)  !}\int_{-\infty}^{\infty}x^{2p-2k}\ e^{-x^{2}}\mathrm{d}x
\]
ahora si en la integralhacemos $x=\sqrt{t}$ obtenemos
\begin{align*}
a_{2k}  & =\frac1{2^{2k}(2k)!\sqrt{\pi}}\frac{\left(  2p\right)  !}{\left(
2p-2k\right)  !}\int_{-\infty}^{\infty}t^{p-k}\ e^{-t}\frac{\mathrm{d}%
t}{2\sqrt{t}}\\
& =\frac1{2^{2k+1}(2k)!\sqrt{\pi}}\frac{\left(  2p\right)  !}{\left(
2p-2k\right)  !}\int_{-\infty}^{\infty}t^{p-k-\frac12}\ e^{-t}\mathrm{d}t
\end{align*}
y utilizando la definición $\Gamma\left(  z\right)  \equiv\int_{0}%
^{\infty}e^{-t}t^{z-1}\mathrm{d}t\equiv\left(  z-1\right)  !$ , queda como
\[
a_{2k}=\frac1{2^{2k+1}(2k)!\sqrt{\pi}}\frac{\left(  2p\right)  !}{\left(
2p-2k\right)  !}\Gamma\left(  p-k+\frac12\right)
\]
Ahora, recurrimos a la propiedad de ``duplicación'' de la Función
Gamma, i.e.
\[
2^{2z-1}\Gamma\left(  z\right)  \Gamma\left(  z+\frac12\right)  =\sqrt{\pi
}\Gamma\left(  2z\right)
\]
tenemos que
\[
2^{2p-2k}\Gamma\left(  p-k+\frac12\right)  \left(  p-k\right)  !=\sqrt{\pi
}\left(  2p-2k\right)  !
\]
quedan entonces los coeficientes determinados como
\[
a_{2k}=\frac{\left(  2p\right)  !}{2^{2p+1}(2k)!\left(  p-k\right)  !}
\]
y, por lo tanto el desarrollo en la base de los polinomios de Hermite
\[
f(x)=x^{2p}=\frac{\left(  2p\right)  !}{2^{2p+1}}\sum_{k=0}^{p}\ \frac
{H_{2k}(x)}{(2k)!\left(  p-k\right)  !}\qquad-\infty<x<\infty
\]
Muestre que del mismo modo se puede encontrar
\[
f(x)=x^{2p+1}=\frac{\left(  2p-1\right)  !}{2^{2p-1}}\sum_{k=0}^{p}%
\ \frac{H_{2k+1}(x)}{(2k+1)!\left(  p-k\right)  !}\qquad-\infty<x<\infty
\]
Si $f(x)=e^{-a^{2}x^{2}}$ con $\operatorname{Re}a^{2}>-1.$ Otra vez
\[
f(x)=e^{-a^{2}x^{2}}=\sum_{k=0}^{\infty}a_{2k}\ H_{2k}(x)
\]
entonces
\begin{equation}
a_{2k}=\frac1{2^{2k}(2k)!\sqrt{\pi}}\int_{-\infty}^{\infty}e^{-(a^{2}+1)x^{2}%
}H_{2k}(x)\mathrm{d}x\nonumber
\end{equation}
Sustituyendo $H_{2k}(x)$ por su expresión integral tendremos
\begin{align*}
a_{2k}  & =\frac1{2^{2k}(2k)!\sqrt{\pi}}\int_{-\infty}^{\infty}e^{-(a^{2}%
+1)x^{2}}\left[  \frac{2^{2k+1}(-1)^{k}e^{x^{2}}}{\sqrt{\pi}}\int_{0}^{\infty
}e^{-t^{2}}t^{2k}\cos2xt\ \mathrm{d}t\right]  \mathrm{d}x\\
& =\frac{2(-1)^{k}}{\pi(2k)!}\int_{-\infty}^{\infty}e^{-a^{2}x^{2}}\left[
\int_{0}^{\infty}e^{-t^{2}}t^{2k}\cos2xt\ \mathrm{d}t\right]  \mathrm{d}x\\
& \equiv\frac{2(-1)^{k}}{\pi(2k)!}\int_{0}^{\infty}e^{-t^{2}}t^{2k}\left[
\int_{-\infty}^{\infty}e^{-a^{2}x^{2}}\cos2xt\ \mathrm{d}x\right]
\ \mathrm{d}t\\
& =\frac{2(-1)^{k}}{\pi(2k)!}\int_{0}^{\infty}e^{-t^{2}}t^{2k}\left[
\sqrt{\frac\pi{a^{2}}}\ e^{-t^{2}/a^{2}}\right]  \ \mathrm{d}t=\\
& =\frac{2(-1)^{k}}{\sqrt{\pi}(2k)!a}\int_{0}^{\infty}e^{-t^{2}(1+a^{-2}%
)}\ t^{2k}\ \mathrm{d}t\\
& =\frac{(-1)^{k}}{\sqrt{\pi}(2k)!}\ \frac{a^{2k}}{\left(  1+a^{2}\right)
^{k+1/2}}\int_{0}^{\infty}e^{-s}\ s^{k-\frac12}\ \mathrm{d}s\qquad\leftarrow
t^{2}(1+a^{-2})=s\\
& =\frac{(-1)^{k}}{\sqrt{\pi}(2k)!}\ \frac{a^{2k}}{\left(  1+a^{2}\right)
^{k+1/2}}\Gamma\left(  k+\frac12\right)
\end{align*}
y ahora usando, otra vez la propiedad de ``duplicación'' de la función
gamma,
\[
2^{2k}\Gamma\left(  k+\frac12\right)  k!=\sqrt{\pi}\left(  2k\right)  !
\]
obtenemos
\[
a_{2k}=\frac{(-1)^{k}a^{2k}}{2^{2k}\ k!\left(  1+a^{2}\right)  ^{k+1/2}}\
\]
por lo tanto
\[
f(x)=e^{-a^{2}x^{2}}=\sum_{k=0}^{\infty}\frac{(-1)^{k}a^{2k}}{2^{2k}%
\ k!\left(  1+a^{2}\right)  ^{k+1/2}}\ H_{2k}(x)
\]
Al igual que los polinomios de Legendre, los de Hermite, surgen tambián en sus orígenes como soluciones a la ecuación diferencial ordinaria del tipo 
\[
 \frac{\mathrm{d}^2 H_{n}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} H_{n}(x)}{\mathrm{d} x} + nH_{n}(x) =0
\]
Vale decir
\begin{center}
\begin{tabular}
[c]{lcc}\hline\hline
$n$ & Ecuación de Hermite & Solución\\\hline\hline
$0$ & \multicolumn{1}{r}{$ \frac{\mathrm{d}^2 H_{0}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} H_{0}(x)}{\mathrm{d} x}=0$} & \multicolumn{1}{l}{$H_{0}(x)=1$}\\ \\
$1$ & \multicolumn{1}{r}{$ \frac{\mathrm{d}^2 H_{1}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} H_{1}(x)}{\mathrm{d} x}+ 2 H_{1}(x)=0$} & \multicolumn{1}{l}{$H_{1}(x) = 2x $}\\ \\
$2$ & \multicolumn{1}{r}{$ \frac{\mathrm{d}^2 H_{2}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} H_{2}(x)}{\mathrm{d} x} + 4 H_{2}(x)=0$} & \multicolumn{1}{l}{$H_{2}(x)=4x^{2} -2$}\\ \\
$3$ & \multicolumn{1}{r}{$ \frac{\mathrm{d}^2 H_{3}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} H_{3}(x)}{\mathrm{d} x} + 6 H_{3}(x) = 0$} & \multicolumn{1}{l}{$H_{3}(x)= 8x^{3} -12x $}\\ \\
$4$ & \multicolumn{1}{r}{$ \frac{\mathrm{d}^2 H_{4}(x)}{\mathrm{d} x^2} -2x \ \frac{\mathrm{d} H_{4}(x)}{\mathrm{d} x} + 8 H_{4}(x)=0$} & \multicolumn{1}{l}{$H_{4}(x)=16 x^{4} -48x^{2} +12$ }  \\ \hline\hline
\end{tabular}
\end{center}

\subsubsection{Función Generatriz de los Polinomios de Hermite}
Se puede
encontrar una función generatriz $\mathcal{H}(t,x)$ de los polinomios de
Hermite:
\[
\mathcal{H}(t,x)=e^{2xt-t^{2}}=H_{0}(x)\ +H_{1}(x)\ t+\frac{H_{2}(x)}%
2\ t^{2}+\frac{H_{3}(x)}{3!}\ t^{2}+\cdots=\sum_{n=0}^{\infty}\frac{H_{n}%
(x)}{n!}\ t^{n}
\]
para la cual los $H_{n}(x)$ son los coeficientes de su desarrollo en series de
potencias. Es fácil darse cuenta que esta expresión proviene del
desarrollo en Serie de Taylor
\[
\mathcal{H}(t,x)=e^{2xt-t^{2}}=\sum_{n=0}^{\infty}\frac1{n!}\left[
\frac{\partial^{n}\mathcal{H}(t,x)}{\partial t^{n}}\right]  _{t=0}%
\ t^{n}\qquad\left\|  t\right\|  <\infty
\]
para lo cual
\[
\left[  \frac{\partial^{n}\mathcal{H}(t,x)}{\partial t^{n}}\right]
_{t=0}=e^{x^{2}}\left[  \frac{\partial^{n}}{\partial t^{n}}e^{-\left(
x-t\right)  ^{2}}\right]  _{t=0}=\left(  -1\right)  ^{n}e^{x^{2}}\left[
\frac{\mathrm{d}^{n}}{\mathrm{d}u^{n}}e^{-\left(  u\right)  ^{2}}\right]
_{u=x}=H_{n}(x)
\]
\subsubsection{Relación de Recurrencia}
A partir de la función
generatriz se puede construir la siguiente identidad
\[
\frac{\partial\mathcal{H}(t,x)}{\partial t}=\left(  2x-2t\right)  \mathcal{H}
\]
y utilizando el desarrollo en series de potencias en $t$ tendremos,
\begin{align*}
\sum_{n=1}^{\infty}\frac{H_{n}(x)}{n!}\ nt^{n-1}-2x\sum_{n=0}^{\infty}%
\frac{H_{n}(x)}{n!}\ t^{n}+\sum_{n=0}^{\infty}\frac{H_{n}(x)}{n!}\ t^{n+1}  &
=0\\
\sum_{n=0}^{\infty}\ \frac1{n!}\left[  \underbrace{H_{n+1}(x)-2xH_{n}%
(x)+2nH_{n-1}(x)}_{=0}\right]  t^{n}  & =0
\end{align*}
As\'{\i} la relación de recurrencia será
\[
H_{n+1}(x)-2xH_{n}(x)+2nH_{n-1}(x)=0
\]
De igual modo, podemos partir de otra identidad
\[
\frac{\partial\mathcal{H}(t,x)}{\partial x}=2t\ \mathcal{H}\Rightarrow
\sum_{n=0}^{\infty}\frac{H_{n}^{\prime}(x)}{n!}\ t^{n}-2\sum_{n=0}^{\infty
}\frac{H_{n}(x)}{n!}\ t^{n+1}
\]
y encontrar una relación para generar las derivadas de los polinomios de Hermite en tármino de ellos mismos:
\[
H_{n}^{\prime}(x)=2n\ H_{n-1}(x),\qquad n=1,2,3,\cdots
\]
Finalmente, utilizando la ecuación anterior en la relación de
recurrencia y derivando esa expresión una vez más, queda como:
\begin{align*}
H_{n+1}(x)-2xH_{n}(x)+H_{n}^{\prime}(x)  & =0\\
H_{n}^{\prime\prime}(x)-2xH_{n}^{\prime}(x)+2n\ H_{n}(x)  & =0
\end{align*}
con lo cual queda demostrado que los polinomios de Hermite son una solución particular de esa ecuación diferencial.
\[
y^{\prime\prime}-2xy^{\prime}+2ny=0,
\]
Donde hemos hecho $y=H_{n}(x)$ Adicionalmente, haciendo un cambio cosmático podremos demostrar que $y=e^{-x^{2}/2}H_{n}(x)$ es solución de la ecuación diferencial autoadjunta
\[
y^{\prime\prime}+\left(  2n+1-x^{2}\right)  y=0
\]
\subsubsection{Ortogonalidad y Norma de los Polinomios de Hermite}
En general
estos polinomios cumplen con
\[
\langle\mathbf{H}_{\alpha}|\mathbf{H}_{\beta}\rangle=2^{n}n!\sqrt{\pi}%
\ \delta_{\alpha\beta}=\int_{-\infty}^{\infty}e^{-x^{2}}H_{\beta}(x)H_{\alpha
}(x)\mathrm{d}x
\]
Donde la función delta de Kronecker es $\delta_{\alpha\beta}=0$ si
$\alpha\neq\beta$; y $\delta_{\beta\beta}=1.$

Para demostrar el caso $\alpha\neq\beta$ partimos de
\begin{align*}
u_{\beta}\left[  u_{\alpha}^{\prime\prime}+\left(  2\alpha+1-x^{2}\right)
u_{\alpha}\right]   & =0\\
u_{\alpha}\left[  u_{\beta}^{\prime\prime}+\left(  2\beta+1-x^{2}\right)
u_{\beta}\right]   & =0
\end{align*}
restando miembro a miembro e integrando se tiene que:
\begin{align*}
\left[  u_{\alpha}^{\prime}u_{\beta}-u_{\beta}^{\prime}u_{\alpha}\right]
^{\prime}+2\left(  \alpha-\beta\right)  u_{\alpha}u_{\beta} & =0\\
\left(  \alpha-\beta\right)  \int_{-\infty}^{\infty}e^{-x^{2}}H_{\beta
}(x)H_{\alpha}(x)\mathrm{d}x  & =0\\
\int_{-\infty}^{\infty}e^{-x^{2}}H_{\beta}(x)H_{\alpha}(x)\mathrm{d}x  &
=0\qquad\alpha\neq\beta;
\end{align*}
ya que
\[
\left.  e^{-x^{2}/2}\left(  2\alpha\ H_{\alpha-1}(x)H_{\beta}(x)-2\beta
\ H_{\beta-1}(x)H_{\alpha}(x)\right)  \right|  _{-\infty}^{\infty}=0
\]
Para encontrar el valor de la norma, procedemos a partir de la relación de
recurrencia
\begin{align*}
H_{n}(x)\left(  H_{n}(x)-2xH_{n-1}(x)+2(n-1)H_{n-2}(x)\right)   & =0\\
H_{n-1}(x)\left(  H_{n+1}(x)-2xH_{n}(x)+2nH_{n-1}(x)\right)   & =0
\end{align*}
restando miembro a miembro, multiplicando por $e^{-x^{2}}$ e integrando entre
$(-\infty,\infty)$ se obtiene
\[
\int_{-\infty}^{\infty}e^{-x^{2}}H_{\alpha}^{2}(x)\mathrm{d}x=2\alpha
\int_{-\infty}^{\infty}e^{-x^{2}}H_{\alpha-1}^{2}(x)\mathrm{d}x
\]
repitiendo la operación y recordando que al final queda
\[
\int_{-\infty}^{\infty}e^{-x^{2}}x^{2}\mathrm{d}x=2\sqrt{\pi}
\]
Obtenemos
\[
\langle\mathbf{H}_{\alpha}|\mathbf{H}_{\alpha}\rangle=\left\|  \mathbf{H}%
_{\alpha}\right\|  ^{2}=\int_{-\infty}^{\infty}e^{-x^{2}}H_{\alpha}%
^{2}(x)\mathrm{d}x=2^{n}n!\sqrt{\pi}
\]
\subsubsection{Representación Integral de los Polinomios de Hermite}
 Los
polinomios de Hermite pueden ser representados como
\[
H_{n}(x)=\frac{2^{n}(-i)^{n}e^{x^{2}}}{\sqrt{\pi}}\int_{-\infty}^{\infty
}e^{-t^{2}+2itx}t^{n}\mathrm{d}t
\]
que puede ser separada como
\[
H_{2n}(x)=\frac{2^{2n+1}(-1)^{n}e^{x^{2}}}{\sqrt{\pi}}\int_{0}^{\infty
}e^{-t^{2}}t^{2n}\cos2xt\ \mathrm{d}t\qquad n=1,2,3,\cdots
\]
y paralos tárminos impares
\[
H_{2n+1}(x)=\frac{2^{2n+2}(-1)^{n}e^{x^{2}}}{\sqrt{\pi}}\int_{0}^{\infty
}e^{-t^{2}}t^{2n+1}\operatorname*{sen}2xt\ \mathrm{d}t\qquad n=1,2,3,\cdots
\]
La forma de llegar a cualquiera de estas últimas fórmulas se parte de
las conocidas integrales desarrolladas en el plano complejo
\[
e^{-x^{2}}=\frac2{\sqrt{\pi}}\int_{-\infty}^{\infty}e^{-t^{2}}\cos
2xt\ \mathrm{d}t
\]
se deriva $2n$ veces a ambos miembros se utiliza la definición de los
polinomios de Hermite.

\subsubsection{Resumen de Propiedades Polinomios Hermite}
\begin{center}
\begin{tabular}[c]{|l|l|}\hline
\multicolumn{2}{|c|}{\textbf{Polinomios de Hermite}}\\\hline
Definición & $%
\begin{array}[c]{c}%
H_{n}(x)=(-1)^{n}e^{x^{2}}\dfrac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}e^{-x^{2}%
},\qquad n=0,1,2,....\\
H_{n}(x)={\displaystyle\sum_{k=0}^{n/2}} \dfrac{(-1)^{k}n!}{k!\left(  n-2k\right)  !}\left(  2x\right)  ^{n-2k}%
\qquad\qquad
\end{array}
$\\\hline
Ejemplos & $%
\begin{array}
[c]{c}%
H_{0}(x)=1;\qquad H_{1}(x)=2x;\qquad H_{2}(x)=4x^{2}-2;\\
H_{3}(x)=8x^{3}-12x\qquad H_{4}(x)=16x^{5}-48x^{2}+12
\end{array}
$\\\hline
Relaciones de Recurrencia & $%
\begin{array}
[c]{c}%
H_{n+1}(x)-2xH_{n}(x)+2nH_{n-1}(x)=0\\
H_{n}^{\prime}(x)=2n\ H_{n-1}(x),\qquad n=1,2,3,\cdots
\end{array}
$\\\hline
Ecuaciones Diferenciales & $%
\begin{array}
[c]{c}%
y^{\prime\prime}-2xy^{\prime}+2ny=0\\
u^{\prime\prime}+\left(  2n+1-x^{2}\right)  u=0;\quad u(x)=e^{-x^{2}/2}%
H_{n}(x)
\end{array}
$\\\hline
Función Generatriz & $\mathcal{H}(t,x)=\mathbf{e}^{2xt-t^{2}}=%
%TCIMACRO{\dsum _{n=0}^{\infty}}%
%BeginExpansion
{\displaystyle\sum_{n=0}^{\infty}}
%EndExpansion
\dfrac{H_{n}(x)}{n!}\ t^{n}$\\\hline
Representación Integral & $%
\begin{array}
[c]{c}%
H_{2n}(x)=\dfrac{2^{2n+1}(-1)^{n}e^{x^{2}}}{\sqrt{\pi}}%
%TCIMACRO{\dint _{0}^{\infty}}%
%BeginExpansion
{\displaystyle\int_{0}^{\infty}}
%EndExpansion
e^{-t^{2}}t^{2n}\cos2xt\ \mathrm{d}t\\
H_{2n+1}(x)=\dfrac{2^{2n+2}(-1)^{n}e^{x^{2}}}{\sqrt{\pi}}%
%TCIMACRO{\dint _{0}^{\infty}}%
%BeginExpansion
{\displaystyle\int_{0}^{\infty}}
%EndExpansion
e^{-t^{2}}t^{2n+1}\operatorname*{sen}2xt\ \mathrm{d}t
\end{array}
$\\\hline
Ortogonalidad & $\langle\mathbf{H}_{\alpha}|\mathbf{H}_{\beta}\rangle
=2^{n}n!\sqrt{\pi}\ \delta_{\alpha\beta}=%
%TCIMACRO{\dint _{-\infty}^{\infty}}%
%BeginExpansion
{\displaystyle\int_{-\infty}^{\infty}}
%EndExpansion
e^{-x^{2}}H_{\beta}(x)H_{\alpha}(x)\mathrm{d}x$\\\hline
\end{tabular}%
\end{center}

\subsubsection{El Oscilador armónico, independiente del Tiempo, en Mecánica
Cuántica.}
La Ecuación de Schr\"{o}dinger independiente del tiempo y en una
dimensión es
\[
\frac{\mathrm{d}^{2}}{\mathrm{d}x^{2}}\psi(x)+\frac{2\mu}{\hbar^{2}}\left[
E-\mathcal{U}(x)\right]  \psi(x)=0
\]
con $\mu$ la ``masa'' de la partícula;$\ E$ los niveles de energía y
$\mathcal{U}(x)$ el potencial al cual estásometida la partícula. En
el caso que estudiemos un potencial $\mathcal{U}(x)=\frac{1}{2}\mu\omega
^{2}x^{2}$ en el cual la frecuencia angular del oscilador viene representada
por $\omega$. La ecuación de Schr\"{o}dinger se convierte en
\[
\frac{\mathrm{d}^{2}}{\mathrm{d}x^{2}}\psi(x)+\frac{2\mu}{\hbar^{2}}\left[
E-\frac{1}{2}\mu\omega^{2}x^{2}\right]  \psi(x)=0
\]
haciendo un cambio de variable $\xi=x\sqrt{\mu\omega/\hbar}$ para
adimensionalizar la ecuación, se obtiene
\[
\psi^{\prime\prime}(\xi)+\left[  \frac{2E}{\hbar\omega}-\xi^{2}\right]
\psi(\xi)=0
\]
la cual corresponde a la forma autoadjunta de la Ecuación de Hermite y por
lo tanto identificamos
\[
\frac{2E}{\hbar\omega}=2n+1\quad\Rightarrow\quad E=\left(  n+\frac{1}%
{2}\right)  \hbar\omega
\]
con lo cual comprobamos la forma como viene cuantizada la energía en este
sistema y la energía del estado fundamental. Por su parte, la función
de onda se podráexpresar en la base de soluciones de esa ecuación
\[
\psi(\xi)=\sum_{n=0}^{\infty}c_{n}\ \psi_{n}(\xi)=\sum_{n=0}^{\infty}%
c_{n}\ e^{-\xi^{2}/2}H_{n}(\xi)
\]

\begin{center}%
\begin{figure}
[ptb]
\begin{center}
\includegraphics[
natheight=7.489300in,
natwidth=9.989500in,
height=3.3987in,
width=4.5238in
]%
{hermite1.jpg}%
\end{center}
\end{figure}
\end{center}

y se mantenemos la normalización
\[
\int_{-\infty}^{\infty}\psi_{n}^{2}(\xi)\mathrm{d}\xi=1 \qquad \text{con }c_{n}=\left(  \frac{\mu\omega}{\pi\hbar}\right)  ^{1/4}\frac{1}{\sqrt{2^{n}%
n!}}
\]

\subsection{Planteamiento General para Polinomios Ortogonales}
Hemos considerado un par de ejemplos de Polinomios Ortogonales. En ambos podemos idenficar algunas características comunes. En base a estas características comunes definiremos otras familias de polinomios ortogonales.

\subsubsection{Producto interno genérico, Norma y ortogonalidad}
Los polinomios ortogonales se definen como un conjunto de polinomios $\{ p_{n}(x) \}$ de orden $n$ definidos en un determinado intervalo $a \leq x \leq b$, los cuales son ortogonales respecto a una definición de producto interno 
\[
\langle \mathbf{p}_m\ | \mathbf{p}_n\rangle=\int_{a}^{b} w(x) p_{m}(x) p_{n}(x)\mathrm{d}x = h_{n} \delta_{nm} \quad \text{con } w(x) > 0 \text{ una función peso en } a \leq x \leq b
\] que garantiza que la norma sea finita en ese intervalo. Dado que el Teorema de Weierstrass garantiza que el conjunto de polinomios  $\{ 1, x, x^2, \cdots, x^n, \cdots \}$ es una base completa para un espacio vectorial $\mathbb{E}^{\infty}$, se procede a ortogonalizar esa base con la definición de producto interno y el intervalo que corresponda. Para cada caso tendremos una base ortogonal de polinomios. 

Haremos ahora un catálogo de las propiedades más resaltantes de estos polinomios. En el cuadro \ref{PropiedadesGenericas} resumimos las propiedades más resaltantes, com lo son: la función peso en el producto interno, el intervalo en el cual están definidas estas fuciones y su norma.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|} \hline
\textbf{Nomenclatura} & \textbf{Nombre} 	& $a$ & $b$ & $w(x)$ & $N_{n}$ & $N_{0}$ \\ \hline \hline 
$P_{n}(x)$ 	& Legendre 		& $-1$ 	& $1$ 	& $1$ 		& $\dfrac{2}{2n +1}$ &  \\\hline 
$T_{n}(x)$ 	& Tchebychev 1E 	& $-1$ 	& $1$ 	& $\dfrac{1}{\sqrt{1 -x^{2}}}$ & $\dfrac{\pi}{2}$ & $\pi$ \\\hline 
$U_{n}(x)$ 	& Tchebychev 2E 	& $-1$ 	& $1$ 	& $\sqrt{1 -x^{2}}$ & $\dfrac{\pi}{2}$ &  \\\hline 
$H_{n}(x)$ 	& Hermite 		& $-\infty$& $\infty$ & $e^{-x^{2}}$ & $2^{n}n! \sqrt{\pi}$ &  \\\hline 
$L_{n}(x)$ 	& Laguerre 		& $0$ 	& $\infty$& $e^{-x}$ & $1$ &  \\\hline 
$L_{n}^{\alpha}(x)$ & Laguerre G 	& $0$ 	& $\infty$ & $x^{\alpha}e^{-x}$ con $\alpha > -1$ & 
	$\frac{\Gamma(n + \alpha +1) }{n!}$ &  \\\hline 
$P_{n}^{\alpha \beta}(x)$ & Jacobi & $-1$ 	& $1$ & $(1-x)^{\alpha} (1+x)^{\beta}$ & ver leyenda & \\ \hline \hline
\end{tabular} 
\caption{Propiedades genéricas de los Polinomios Ortogonales, $N_{n}$  indica la norma del polinomio de grado $n$. En el caso de los polinomios de Jacobi, la norma es $N_{n} = \dfrac{2^{\alpha + \beta +1}}{2n + \alpha + \beta +1} 
\dfrac{ \Gamma(n + \alpha  +1) \Gamma(n +  \beta +1)}{n! \Gamma(n + \alpha + \beta +1) } $ con $\alpha > -1$ y $\beta > -1$}
\label{PropiedadesGenericas}
\end{table}


\subsubsection{Fórmula de Rodrigues genelarizada}
En general todos los polinomios ortogonales $\{ p_{n}(x) \}$ vienen definidos por la fórmula de Rodrigues generalizada 
\[
p_{n}(x)= \dfrac{1}{w(x) \mu_{n}}  \dfrac{\mathrm{d}^{n} }{\mathrm{d}x^{n} } \left( w(x) q(x)^{n} \right)
\]
donde $w(x), q(x)$ y $ \mu_{n}$ vienen especficados en el cuadro \ref{FormulaRodrigues} para cada conjunto de polinomios ortogonales

\begin{table}[h]
  \centering 
  \begin{tabular}{|c|c|c|c|} \hline \hline
% after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
  \textbf{Polinomio} 	& $\mu_{n}$ 													   & $w(x)$ 				& $q(x)$ \\  \hline \hline
  $P_{n}$ 			& $(-1)^{n} 2^{n}n!  $ 											   & $1$					&$1 -x^{2}$  \\  \hline  
  $T_{n}$ 			& $\dfrac{(-1)^{n}}{\sqrt{\pi}} 2^{n+1} \Gamma \left(n + \frac{1}{2} \right)$ 	   & $\dfrac{1}{\sqrt{1 -x^{2}}}$ & $1 -x^{2}$  \\  \hline 
  $U_{n}$ 			& $\dfrac{(-1)^{n}}{(n+1)\sqrt{\pi}} 2^{n+1} \Gamma \left(n + \frac{3}{2} \right) $ &$ \sqrt{1 -x^{2}}$ 			&$1 -x^{2}$  \\  \hline
  $H_{n}$ 			& $(-1)^{n}$ 													   &$e^{-x^{2}} $ 			&$1$ \\  \hline 
  $L_{n}$ 			& $n!$ 														   & $e^{-x} $ 				&$x$ \\  \hline 
  $L_{n}^{\alpha}$ 	& $n!$ 														   & $x^{\alpha}e^{-x} $ 	& $x$\\  \hline 
\hline
\end{tabular}
  \caption{Funciones para determinar la Fórmula de Rodrigues generalizada } 
  \label{FormulaRodrigues}
\end{table}

\subsubsection{Ejemplos de Polinomios Ortogonales}
Utilizando la fórmula de Rodrigues generalizada, podemos construir algunos polinomios generalizados. El cuadro \ref{EjemplosPolinomios} muestra algunos de ejemplos de estos polinomios ortogonales

\begin{table}[h]
  \centering 
  \begin{tabular}{|c|c|c|c|c|c|} \hline \hline
% after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
\textbf{Polinomio} &$n=0$ 	& $n=1$ & $n=2$ 				&$n=3$ 					& $n=4$ 				 \\  \hline \hline
  $P_{n}$ 			&$1$ 	& $x$	&$ \dfrac{1}{2}(3x^{2} -1)$&$ \dfrac{1}{2}(5x^{3} -3x)$	& $ \dfrac{1}{8}(35x^{4} -30x^{2} +3)$  \\  \hline  
  $T_{n}$ 			& $1 $ 	& $x$ 	& $2x^{2} -1$ 			&$4x^{3} -3x$ 				&$8x^{4} -8x^{2} +1$ 	 \\  \hline 
  $U_{n}$ 			& $1 $ 	&$ 2x $ 	&$4x^{2} -1$ 			&$8x^{3} -4x$				&$16x^{4} -12x^{2} +1$	  \\  \hline
  $H_{n}$ 			& $1 $ 	&$2x$ 	&$4x^{2} -2$  			&$8x^{3} -12x$ 			&$16x^{4} -48x^{2} +12$ 	 \\  \hline 
  $L_{n}$ 			& $1$ 	& $1-x $ 	&$\frac{1}{2}x^{2} -2x +1$ &$\frac{-1}{6}(x^{3} -9x^{2} +18x -6)$ &$\frac{1}{24}(x^{4} -16x^{3} +72x^{2} -96x +24)$   \\  \hline 
\hline
\end{tabular}
  \caption{Ejemplos de Polinomios Ortogonales } 
  \label{EjemplosPolinomios}
\end{table}

\subsubsection{Relaciones de Recurrencia}
También se pueden formular, de manera genérica las realciones de recurrencia. Obviamente, las relaciones de recurrencia también constituyen una forma alternativa de ir construyendo los polinomios ortogonales. Así, un polinomio ortogonal genérico, $p_{n}(x)$, cumplirá
\[
p_{n+1}(x) = ( a_{n} +xb_{n} ) p_{n}(x) - c_{n}p_{n-1}(x)
\]
El cuadro \ref{RelacionRecurrencia} contiene las expresiones de los coeficientes para construir las relaciones de recurrencia generalizadas para cada uno de los polinomios

\begin{table}[h]
  \centering 
  \begin{tabular}{|c|c|c|c|} \hline \hline
% after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
  \textbf{Polinomio}&$a_{n}$ 				& $b_{n}$ 		& $c_{n}$ 		\\  \hline \hline
  $P_{n}$ 			&$0 $ 				& $\dfrac{2n+1}{n+1}$&$\dfrac{n}{n+1}$  \\  \hline  
  $T_{n}$ 			&$0 $ 				& $2$ 			& $1$  			\\  \hline 
  $U_{n}$ 			&$0 $ 				& $2$ 			& $1$  			\\  \hline 
  $H_{n}$ 			&$0 $ 				& $2$ 			& $2n$  			\\  \hline 
  $L_{n}$ 			&$\dfrac{2n+1}{n+1}$ 	& $-\dfrac{1}{n+1}$ & $\dfrac{n}{n+1}$  \\  \hline 
  $L_{n}^{\alpha}$ 	&$\dfrac{2n+1+\alpha}{n+1}$ & $-\dfrac{1}{n+1}$ & $\dfrac{n+\alpha}{n+1}$  \\  \hline 
\end{tabular}
  \caption{Funciones para determinar la Relación de Recurrencia Generalizada } 
  \label{RelacionRecurrencia}
\end{table}


\subsubsection{Función generatriz generalizada}
Para todos los polinomimos ortogonales podemos definir una función generatriz $\mathcal{G}(x,t)$, de tal manera que cada uno de los polinomios ortogonales $\{ p_{n}(x) \}$ será proporcional al coeficiente de $t^{n}$ del desarrollo en series de Taylor, en potencias de $t$ alrededor del punto $x=0$. Esta función generatriz que constituye una forma alternativa de definir los polinomios ortogonales viene expresada por la serie
\[
\mathcal{G}(x,t) = \sum_{n=0}^{\infty}  C_{n}  p_{n}(x) \ t^{n} \qquad \text{con } a_{n} \text{ constante}
\]
Las funciones generatrices no son exclusivas de los polinomios ortogonales. Como veremos más adelante, existen funciones generatrices para las funciones de Bessel.

\begin{table}[h]
  \centering 
  \begin{tabular}{|c|c|c|c|} \hline \hline
  \textbf{Polinomio} & $C_{n} $ 			& $\mathcal{G}(x,t)$ 			\\  \hline \hline
  $P_{n}$ 			& $1 $ 				& $ \dfrac{1}{\sqrt{1 -2xt +t^{2}}} $	\\  \hline  
  $T_{n}$ 			& $2$ 				& $ \dfrac{1-t^{2}}{ 1 -2xt +t^{2} } +1$   \\  \hline 
  $U_{n}$ 			& $1$ 				& $ \dfrac{1}{ 1 -2xt +t^{2} } $  \\  \hline
  $H_{n}$ 			& $\dfrac{1}{n!}$ 		&$e^{2xt -x^{2}} $ 			 \\  \hline 
  $H_{2n}$ 		& $\dfrac{1^{n}}{(2n)!}$ 	&$ \cos(2xt) e^{t^{2}} $ 			 \\  \hline 
  $H_{2n+1}$ 		& $\dfrac{1^{n}}{(2n+1)!}$ &$ \mathrm{sen}(2xt) e^{t^{2}}$ 			 \\  \hline 
  $L_{n}$ 			& $1$ 				& $\dfrac{1}{1-t}e^{-\frac{xt}{1-t}} $ 			 \\  \hline 
  $L_{n}^{\alpha}$ 	& $1$ 				& $\dfrac{1}{(1-t)^{\alpha}}e^{-\frac{xt}{1-t}} $ 	\\  \hline 
\hline
\end{tabular}
  \caption{Funciones para determinar la función generatriz generalizada } 
  \label{FuncionGeneratriz}
\end{table}

\subsubsection{Ecuación diferencial para los Polinomios Ortogonales}
Cada uno de los polinomios ortogonales habrá de ser solución de una ecuación diferencial ordinaria de la forma
\[
g_{2}(x)  \dfrac{\mathrm{d}^{2} p_{n}(x) }{\mathrm{d}x^{2} } + g_{1}(x)  \dfrac{\mathrm{d} p_{n}(x) }{\mathrm{d}x } + \alpha_{n} p_{n}(x) =0
\]En el cuadro \ref{SolEcuacionDiferencial} mostramos las expresiones para los coeficientes de las ecuaciones correspondientes a las ecuaciones diferenciales para las cuales cada uno de los polinomio ortogonales es solución

\begin{table}[h]
  \centering 
  \begin{tabular}{|c|c|c|c|} \hline \hline
% after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
  \textbf{Polinomio}&$g_{2}(x)$ 	& $g_{1}(x)$ & $\alpa_{n}$ 	\\  \hline \hline
  $P_{n}$ 			&$1- x^{2} $ 	& $-2x$	&$n(n+1)$ 	 \\  \hline  
  $T_{n}$ 			&$1- x^{2} $  	& $-x$ 	& $n^{2}$  	\\  \hline 
  $U_{n}$ 			&$1- x^{2} $ 	& $-2x$	&$n(n+1)$ 	 \\  \hline  
  $H_{n}$ 			&$1 $ 		& $-2x$ 	& $2n$  		\\  \hline 
  $L_{n}$ 			&$x$ 		& $1-x $ 	& $n$  		\\  \hline 
  $L_{n}^{\alpha}$ 	&$x$ 		& $1-x+\alpha $ 	& $n$  		\\  \hline
  $P_{n}^{\alpha \beta}$ 	&$1- x^{2} $ & $\beta -\alpha -x(2 + \alpha +\beta ) $ 	& $n(n + \alpha +\beta +1)$  \\  \hline
\end{tabular}
  \caption{Funciones para determinar la ecuación diferencial para la cual son solución los polinomios ortogonales}
  \label{SolEcuacionDiferencial}
\end{table}

\subsection{Cuadratura de Gauss}
\subsubsection{Cuadratura de Gauss-Legendre}
Una de los usos más comunes de los polinomios ortogonales es para aproximar funciones, en particular integrales que requieren ser resueltas numéricamente. La idea es aproximar una integral, para una funcion $f(x)$, definida en el intervalo $\left[a,b \right]$ y suficientemente bien comportada, por una suma finita de términos $ c_{k} f(x_{k})$ y estimar el error que cometemos en esta aproximación. Esto es
\[
\int^{b}_{a} f(x) \mathrm{d}x = \sum_{k=1}^{N} c_{k} f(x_{k}) + E_{N}
\]
Nótese que la intención es utilizar la función a integrar evaluada en un conjunto de puntos estratégicos para los cuales están definidos unos coeficientes, también inteligentemente seleccionados. Es decir se requieren $2N$ números ($c_{k}$ y los $x_{k}$ con $k=1,2,\cdots N$). Más aún, esas $2N$ cantidades pueden ser seleccionadas de forma tal que la aproximación es exacta $E_{N}=0$ cuando $f(x)$ es un polinomio de grado $\leq 2N-1$

Supongamos, para empezar que la función $f(x)$ está definida para $x \in \left[ -1,1\right]$\footnote{ésta no es una limitación muy severa porque siempre podemos hacer un cambio de variable del tipo $x = \left( \frac{b-a}{2} \right) t + \left( \frac{b+a}{2} \right)$ y convertir cualquier intervalo cerrado $\left[ a, b\right]$ en un intervalo cerrado $\left[ -1, 1 \right]$} y por lo tanto los polinomios ortogonales que seleccionaremos para aproximar la integral (y la función) serán los del Legendre (igual pudimos haber utilizado los polinomios de Tchebychev), con lo cual
\[
f(x) = \sum_{k=0}^{\infty} a_{k} P_{k}(x) \qquad \text{donde, como siempre } 
a_{k}= \left( n +\frac{1}{2} \right) \int_{-1}^{1}\mathrm{d}x \ f(x) P_{k}(x) \text{ y } a_{0}= \frac{1}{2}  \int_{-1}^{1}\mathrm{d}x \ f(x)
\]
Con lo cual
\[
\int^{1}_{-1} f(x) \mathrm{d}x \approx  \sum_{k=1}^{N} c_{k} f(x_{k}) =  
\sum_{k=1}^{N} c_{k} \left( \sum_{n=0}^{\infty} a_{n} P_{n}(x_{k}) \right) = 
\sum_{n=0}^{\infty} a_{n} \left( \sum_{k=1}^{N} c_{k}  P_{n}(x_{k}) \right)
\]
quedan todavía por determinar los pesos $c_{k}$ y los puntos $x_{k}$. Para ello procedemos de la siguiente forma. Notamos que $P_{N}(x)$ tiene $N$ raíces, $x=x_{j}$, en el intervalo $-1 \leq x \leq 1$. Entonces, si seleccionamos esos puntos $x=x_{j}$ para evaluar la función $f(x_{k})$ se anulan el coeficiente para el término $a_{N}$ y, además podremos encontrar los pesos $c_{k}$ resolviendo el sistema de $N$ ecuaciones de la forma 
\[
\sum_{j=1}^{N} c_{j}P_{0}(x_{j}) = \sum_{j=1}^{N} c_{j} = 2 \qquad \wedge \qquad 
\sum_{j=1}^{N} c_{j}P_{k}(x_{j}) = 0 \quad \text{para } k=1,2,\cdots N-1
\] donde los $P_{k}(x_{j}$ son los distintos polinomios evaluados en las raíces del polinomio de grado $N$, i.e. $P_{N}(x_{j}) =0$

Se puede demostrar que la solución de este sistema provee los pesos escritos de la forma
\[
c_{j} = \dfrac{2}{(1 -x_{j}^2) \left( \left. \dfrac{\mathrm{d} P_{N}(x) }{\mathrm{d}x } \right|_{x=x_{j}}  \right)^{2} }
\]

Más aún, podremos, de esta forma, escribir 
\[
\int^{1}_{-1} f(x) \mathrm{d}x \approx  \sum_{k=1}^{N} c_{k} f(x_{k}) = 2a_{0} + E_{N} \qquad \text{con }
E_{N} =\sum_{n=N+1}^{\infty} a_{n} \left( \sum_{k=1}^{N} c_{k}  P_{n}(x_{k}) \right)
\]
pero como 
\[
a_{0}= \frac{1}{2}  \int_{-1}^{1}\mathrm{d}x \ f(x) \quad \Rightarrow  
\int_{-1}^{1}\mathrm{d}x \ f(x) = \sum_{k=1}^{N} c_{k} f(x_{k}) - E_{N}
\]
Es decir, demostramos que es posible aproximar la integral del la función con un promedio pesado de la función evaluada en unos puntos estratégicos. Los puntos estratégicos son los ceros del polinomio de Legendre de grado igual al número de puntos con los cuales se quiere aproximar la función y los pesos vienen de resolver las ecuaciones para los coeficientes de la expansión. 

En el cuadro \ref{PesosPuntos} se ilustran los valores de los puntos de interpolación y sus pesos correspondientes. 
\begin{table}
  \centering
  \begin{tabular}{|c|c|c|c|}\hline
% after \\ : \hline or \cline{col1-col2} \cline{col3-col4} ...
 $\mathbf{N}$  & $x_{j} =$ \texttt{fsolve(P(N,x),x,complex)}  & $c_{j} = \dfrac{2}{(1 -x_{j}^2) \left( \left. \dfrac{\mathrm{d} P_{N}(x) }{\mathrm{d}x } \right|_{x=x_{j}}  \right)^{2} }$ & $2N -1$ \\  \hline \hline
  $2$& $\pm 0.5773502692$  	&$1.0$ 		 & $3$ 	\\ \hline \hline
  $3$	& $0.0$ 				&$0.88888889$& $5$ 	\\
   	& $\pm 0.7745966692$ 	&$0.55555555$&  		\\ \hline \hline
  $4$ & $\pm 0.3399810436$ 	&$0.65214515$& $7$ 	\\
   	&  $\pm 0.8611363116$	&$0.34785485$& 		\\  \hline \hline
  $5$&  $0.0$ 				&$0.56888889$& $9$ 	\\
   	& $\pm 0.5384693101$  	&$0.47862867$ & 		\\
   	& $\pm 0.9061798459$  	&$0.23692689$  & 		\\  \hline \hline
  $6$& $\pm 0.2386191861$ 	&$0.46791393$ & $11$ 	\\
  	& $\pm 0.6612093865$ 	&$0.36076157$ &  		\\ 
   	& $\pm 0.9324695142$ 	&$0.17132449$ &  		\\  \hline \hline 
   $\vdots$& $\vdots$ & $\vdots$ & $\vdots$ \\ 
   \hline
\end{tabular} 
  \caption{Puntos y pesos para una cuadratura de Gauss-Legendre }\label{PesosPuntos}
\end{table}

Es inmediato comprobar que si $f(x)$ es un polinomio de grado $\leq N-1$ la aproximación es exacta y el error es nulo. Pero lo que realmente hace útil a este tipo de aproximaciones es que también será exacta para polinomios de grado $\leq 2N -1$. Esto se puede ver si expresamos un polinomio de grado $2N -1$ como la suma de dos polinomios
\[
f(x)=P_{N}(x)Y_{1}(x) + Y_{2}(x)
\]
donde $Y_{1}$ y $Y_{2}$ son polinomios de grado $N-1$. Entonces, al integrar miembro a miembro 
\[
\int_{-1}^{1}\mathrm{d}x \ f(x)= \underbrace{\int_{-1}^{1}\mathrm{d}x \ P_{N}(x)Y_{1}(x)}_{=0} + \int_{-1}^{1}\mathrm{d}x \ Y_{2}(x)
\]
el primer término se anula por ser $P_{N(x)}$ ortogonal a cualquier polinomio de grado inferior, y el segundo término no es más que el caso que analizamos anteriormente de un polinomio de grado $\leq N-1$

\subsubsection{Estrategia General para cuadraturas de Gauss}
Para el caso general. Vale decir la aproximación de una integral 
\[
\int_{a}^{b}\mathrm{d}x \ w(x) f(x) \approx  \sum_{k=1}^{N} c_{k} f(x_{k}) 
\]
donde las $\{ x_{1},\cdots x_{k},\cdots x_{N} \}$ son los ceros del polinomio ortogonal, de grado $N$, $p_{N}(x)$, elegido para hacer esta aproximación. Los $N$ pesos $\{ c_{1},\cdots c_{k},\cdots c_{N} \}$ surgen de resolver el sistema de ecuaciones 
\[
\sum_{j=1}^{N} c_{j} = \dfrac{h_{0}}{p_{0}^{2}} \quad \text{con } 
h_{0} = \int_{a}^{b} w(x) p_{0}^{2}(x) \mathrm{d}x  \qquad \wedge \qquad 
\sum_{j=1}^{N} c_{j}P_{k}(x_{j}) = 0 \quad \text{para } k=1,2,\cdots N-1
\] 
Así para aproximar integrales con funciones pesos, $w(x)$, utilizaremos cuadraturas adaptadas a los polinomios ortogonales. Esto es
\[
\int_{0}^{\infty} \mathrm{d}x \ e^{-x} f(x) \Rightarrow \text{Laguerre} \quad 
\int_{-\infty}^{\infty} \mathrm{d}x \ e^{-x^{2}} f(x) \Rightarrow \text{Hermite} \quad 
\int_{-1}^{1} \mathrm{d}x \  \dfrac{f(x)}{\sqrt{1-x^{2}}} \Rightarrow \text{Tchebychev} 
\]

\section{Series de Fourier}
\subsection{Generalidades}
Otro de los casos de expansión en una base completa de funciones lo constituyen la base de Fourier. En este caso la serie de Fourier la constituyen funciones continuas, reales de variable real y definidas en $\left[  0,2\pi\right]  $, $\mathcal{C}_{\left[0,2\pi\right]  }^{\infty}$, en término de funciones trigonométricas. Esto es el conjunto de funciones 
$\left\{  \left|  \mathbf{u}_{1}\right\rangle ,\ \left|  \mathbf{u}_{2}\right\rangle ,\ \left|  \mathbf{u}_{3}\right\rangle ,\cdots,\left|  \mathbf{u}_{n}\right\rangle \cdots\right\}  $ representadas
por
\[
\left|  \mathbf{u}_{0}\right\rangle =1,\qquad\left|  \mathbf{u}_{2n}\right\rangle =\cos nx\qquad\text{y}\qquad\left|  \mathbf{u}_{2n-1}\right\rangle =\operatorname{sen}nx,\qquad\text{con }n=1,2,3,\cdots
\]
Es claro que $\left\{  \left|  \mathbf{u}_{1}\right\rangle ,\ \left|
\mathbf{u}_{2}\right\rangle ,\ \left|  \mathbf{u}_{3}\right\rangle
\cdots,\left|  \mathbf{u}_{n}\right\rangle ,\cdots\right\}  $ es un conjunto
de funciones ortogonales por cuanto%
\[
\left\langle \mathbf{u}_{n}\right.  \left|  \mathbf{u}_{m}\right\rangle
=\delta_{nm}\left\|  \left|  \mathbf{u}_{n}\right\rangle \right\|
^{2}\Rightarrow\left\{
\begin{array}
[c]{rcl}%
0\quad\text{si} & n\neq m & \left\{
\begin{array}
[c]{c}%
\int_{0}^{2\pi}\mathrm{d}x\ \operatorname{sen}nx\operatorname{sen}mx=0\\
\int_{0}^{2\pi}\mathrm{d}x\ \cos nx\operatorname{sen}mx=0\\
\int_{0}^{2\pi}\mathrm{d}x\ \cos nx\cos mx=0
\end{array}
\right. \\
&  & \\
\left\|  \left|  \mathbf{u}_{n}\right\rangle \right\|  ^{2}\quad\text{si} &
n=m & \left\{
\begin{array}
[c]{lcl}%
\int_{0}^{2\pi}\mathrm{d}x\ =2\pi & \text{si} & n=m=0\\
&  & \\
\int_{0}^{2\pi}\mathrm{d}x\ \cos^{2}nx=\pi & \text{si} & i=j=2l\\
&  & \\
\int_{0}^{2\pi}\mathrm{d}x\ \operatorname{sen}^{2}nx=\pi & \text{si} & i=j=2l-1
\end{array}
\right.
\end{array}
\right.
\]
con $l=1,2,3,\cdots$ también. 
\begin{figure}
\begin{center}
\includegraphics[width=5in]{FourierSeriesExamples.jpg}
\caption{ 
Expansiones de Varias funciones en sumas parciales de  Series de Fourier. Tomado de Eric W. Weisstein. \mathbf{Fourier Series}. \textit{MathWorld--A Wolfram Web Resource.} \url{http://mathworld.wolfram.com/FourierSeries.html}
}
\label{FourierSeriesExamples}
\end{center}
\end{figure}
Por lo tanto, podremos construir una base ortonormal de funciones \newline $\left\{  \left|  \mathbf{e}_{1}\right\rangle
,\ \left|  \mathbf{e}_{2}\right\rangle ,\ \left|  \mathbf{e}_{3}\right\rangle
,\cdots,\left|  \mathbf{e}_{n}\right\rangle ,\cdots\right\}  $ de la forma%
\[
\left|  \mathbf{e}_{0}\right\rangle =\frac{1}{\sqrt{2\pi}},\qquad\left|
\mathbf{e}_{2n}\right\rangle =\frac{1}{\sqrt{\pi}}\cos nx\qquad
\text{y}\qquad\left|  \mathbf{e}_{2n-1}\right\rangle =\frac{1}{\sqrt{\pi}%
}\operatorname{sen}nx.
\]
Tal y como se muestra en la figura \ref{FourierSeriesExamples} disntintas funciones pueden ser expandidas con sumas parciales de Fourier. 
A diferencia de las series de potencias, que imponen que las funciones a ser expandidas deben ser contínuas y contínuamente diferenciables en el intervalo, la series de Fourier pueden representar funciones contínuas a trozos, siempre y cuando cumplan con algunas condiciones.

Por lo tanto cualquier función definida en el intervalo $\left[ 0,2\pi\right]  $ puede expresarse en términos de esta base como
\[
\left|  \mathbf{f}\right\rangle = \sum_{i=1}^{\infty}  c_{i}\ \left| \mathbf{e}_{i} \right\rangle \quad \Rightarrow 
c_{i} = \left\langle \mathbf{e}_{i} \right.  \left|  \mathbf{f}\right\rangle =
\left\{
\begin{array}[c]{lcl}%
\frac{1}{\sqrt{2\pi} } \int_{0}^{2\pi} \mathrm{d}x  \ f(x)  = c_{0} \equiv a_{0} & \text{si} & i=0 \\
&  & \\
\int_{0}^{2\pi}\mathrm{d}x\ f( x)  \ \cos(nx) = c_{2n} \equiv a_{m} & \text{si} & i=2n\\
&  & \\
\int_{0}^{2\pi}\mathrm{d}x\ f( x )  \ \operatorname{sen}(nx) = c_{2n-1} \equiv b_{m} & \text{si} & i=2n-1
\end{array}
\right.
\]
donde los $c_{i}$ son los coeficientes de Fourier, con lo cual 
\[
F(x) = \frac{a_{0}}{2} +  \sum_{n=1}^{\infty} \left( a_{n} \cos(nx) + b_{n} \mathrm{sen}(nx) \right) 
\]
y equivalentemente si el período es $T$ y para un un $x_{0}$ genérico
\[
F(x) = \frac{a_{0}}{2} +  \sum_{n=1}^{\infty} \left( a_{n} \cos \left( \frac{2 \pi nx}{T} \right) + b_{n} \mathrm{sen}\left( \frac{2 \pi nx}{T} \right) \right) \quad \text{con }
\left\{
\begin{array}{cl}
   a_{0} =  & \frac{2}{T} \int_{x_{0}}^{x_{0} + T} \mathrm{d}x  \ f(x) \\
  & \\
   a_{n} =  &  \frac{2}{T} \int_{x_{0}}^{x_{0} + T} \mathrm{d}x\ f( x) \cos \left( \frac{2 \pi nx}{T} \right)   \\
    & \\
   b_{n} =  &  \frac{2}{T} \int_{x_{0}}^{x_{0} + T}\mathrm{d}x\ f( x) \ \mathrm{sen} \left( \frac{2 \pi nx}{T} \right)  
\end{array}
\right. 
\]
La figura \ref{FourierSeriesExamples} muestra la aproximación de las distintas sumas parciales para distintas funciones. A medida que aumentamos el número de términos la aproximación mejora. Nótese que hemos utilizado $F(x) \equiv F_{\infty}(x)$ para indicar el límite de la suma parcial $F_{N}(x)$ para $n=N$ de la expresión de una función $f(x)$ expresada en series de Fourier. 

Pero más aún, podemos expresar la expansión de una serie de Fourier de manera más compacta atendendiendo a las expresiones anteriores. Esta expresión se conoce en algunos ámbitos como la expresión integral para la series de Fourier
\begin{eqnarray}
F(x)	& = & \frac{1}{\sqrt{2 \pi} } \int_{0}^{2\pi} \mathrm{d}t  \ f(t)  \nonumber \\
 	& = & \quad +  \sum_{n=1}^{\infty}  \left( \left( \int_{0}^{2\pi}\mathrm{d}t \ f(t)  \cos(nt) \right) \cos(nx) + \left( \int_{0}^{2\pi}\mathrm{d}t \ f(t) \operatorname{sen}(nt) \right) \mathrm{sen}(nx) \right) 											\nonumber \\
	 	&  &  	\nonumber \\
F(x)		& = &  \frac{1}{\sqrt{2 \pi} } \int_{0}^{2\pi} \mathrm{d}x  \ f(x) 
+  \sum_{n=1}^{\infty} \int_{0}^{2\pi}\mathrm{d}x\ f( x)  \cos(n(t-x)) 
 \nonumber
\end{eqnarray}

También es muy común expresar una serie de Fourier en término de una base compleja. Vale decir  $ \big\{ \cdots | \mathbf{\tilde{\phi}}_{k}  \rangle \cdots \big\} \leftrightarrow \{ \cdots e^{-ikx} \cdots \} }  $ con $k=0,\pm1,\pm2,\cdots$. Con lo cual 
\[
| \mathbf{f} \rangle = \sum_{k=-\infty }^{\infty}  \tilde{C}_{k} | \mathbf{\tilde{\phi}}_{k} \rangle \equiv \sum_{k=-\infty }^{\infty}  \tilde{C}_{k} e^{-ikx} \qquad \text{con} \quad
\tilde{C}_{k}  = \frac{ \langle \mathbf{ \tilde{\phi}}_{k} |  \mathbf{f}  \rangle  }{ \langle \mathbf{\tilde{\phi}}_{k} | \mathbf{\tilde{\phi}}_{k}  \rangle} = \frac{1 }{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}x \ e^{-ikx} f(x)
\]

Utilizando esta otra expresión podremos reescribir (una vez más) la expresión de una suma parcial de la Serie de Fourier. Dado que 
\[
a_{n}\cos(nx) + b_{n} \operatorname{sen}(nx) = \frac{1}{\pi} \int_{-\pi}^{\pi} \mathrm{d}t  \ f(t) \cos(n(t-x)) 
\]
tendremos que
\begin{eqnarray}
F_{n}(x) & = &  \frac{a_{0}}{2} +  \sum_{k=1}^{n} \left( a_{k} \cos(kx) + b_{k} \mathrm{sen}(kx) \right) =  \frac{a_{0}}{2} +  \sum_{k=1}^{n} \left( \frac{1}{\pi} \int_{-\pi}^{\pi} \mathrm{d}t  \ f(t) \cos(n(t-x)) \right)  \nonumber \\
 & = & \Re \left[  \int_{-\pi}^{\pi} \mathrm{d}t  \ f(t) \Big\{ \frac{1}{2} + \sum_{k=1}^{n} \left( 
 e^{-i(t-x)k}
 \right) \Big\} \right]  \nonumber
\end{eqnarray}
y al sumar la progresión geometrica que representa una serie de exponenciales llegamos a
\[
F_{n}(x)  = \frac{1}{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}t \ f(t)
 \left[ \dfrac{ \operatorname{sen}\left( \left( n +\frac{1}{2} \right)(t -x)  \right)}{ \operatorname{sen}\left( \frac{1}{2}(t -x)  \right)}  \right] \equiv  \frac{1}{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}t \ f(t) \ \mathcal{K}(x,n,t)
\]
la cual siempre es convergente y el término 
\[
\mathcal{K}(x,n,t) = \left[ \dfrac{ \operatorname{sen}\left( \left( n +\frac{1}{2} \right)(t -x)  \right)}{ \operatorname{sen}\left( \frac{1}{2}(t -x)  \right)}  \right] 
\] se conoce como el núcleo de la transformación de $F$, el (\textit{Kernel}) de Dirichlet

La pregunta básica que sigue es, en todos estos casos,: ¿ cómo se relaciona la expansión de Fourier $\left|  \mathbf{f}\right\rangle \Leftrightarrow F(x)$ con la función $f(x)$ que genera los coeficientes de la expansión ? Arriba Nótese que es una forma de mirar una relación entre $F(x) \leftrightarrow f(t)$.  Pasamos de $f(t)$ a $F(t)$ mediante una ``transformación'' 
\[
F_{n}(x)  = \frac{1}{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}t \ f(t) \ \mathcal{K}(x,n,t)
\]
Este tipo de relaciones se denomina transformación integral y en particular ésta es una de las expresiones de las llamadas \textit{Transformaciones de Fourier} las cuales trataremos más adelante.

\subsection{Las Condiciones de Dirichlet y  el Teorema de Fourier}
\subsubsection{Condiciones de Dirichlet}
Las condiciones que una determinada función $f(x)$ debe cumplir para poder ser representada como una serie de Fourier, se conocen con el nombre de condiciones de Dirichlet\footnote{\textbf{Johann Peter Gustav Lejeune Dirichlet} 1805 - 1859 Matemático Alemán con importantes contribuciones en Teorías de números Algebráica, Series y aproximaciones de funciones y ecuaciones diferenciales parciales} las cuales pueden ser esquematizadas en los siguientes puntos. Para que una función $f(x)$ sea susceptible de ser expandida en series de Fourier debe ser 
\begin{itemize}
  \item periódica
  \item univaluada y contínua a trozos (contínua menos, en un número finito de puntos) con un número finito de máximos y mínimos
  \item la integral $\int_{-T/2}^{T/2} \mathrm{d}x |f(x)|$ debe ser convergente. Donde $\left[ -T/2, T/2 \right]$ quiere indicar el intervalo de definición de una función con período $T$.
\end{itemize}
Podemos formalizar un poco más las condiciones de Dirichlet en el llamado Teorema de Fourier.

\subsubsection{Teorema de Fourier}
\label{TeoremaFourier}
Sea $f(x)$ una función en el intervalo $-\pi \leq x \leq \pi$ y definida para el resto de la recta real tal que cumpla con $f(x =2\pi) = f(x)$. Es decir $f(x)$ es $2\pi-$periódica. Supongamos además que existe
\[
\int_{-\pi}^{\pi} \mathrm{d}x \  f(x) \quad \text{con lo cual} \quad \tilde{C}_{k}  = \frac{1 }{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}x \ e^{-ikx} f(x) \quad  \text{con } k=0,\pm1,\pm2,\cdots.
\]  
y si $|f(x)|$ está acotada para un intervalo $[a,b]$ con $-\pi < a \leq x \leq b < \pi$, entonces
\[
F(x) = \sum_{k=-\infty }^{\infty}  \tilde{C}_{k} e^{-ikx} \quad \text{es convergente al valor  }
F(x) = \frac{1}{2} \left( \lim_{\epsilon \rightarrow 0_{+}} f(x + \epsilon ) +  \lim_{\epsilon \rightarrow 0_{-}} f(x - \epsilon ) \right)
\]
y si $f(x)$ es contínua en $x=x_{0}$ entonces $ F(x_{0}) \rightarrow f(x_{0})$. 

En este punto se pueden puntualizar varias cosas
\begin{itemize}
  \item El valor $F(x) = \frac{1}{2} \left( \lim_{\epsilon \rightarrow 0_{+}} f(x + \epsilon ) +  \lim_{\epsilon \rightarrow 0_{+}} f(x - \epsilon ) \right)$ al cual converge la expansión de Fourier, cobra particular importancia cuando el punto $x =x_{0}$ es una discontinuidad. Tal y como veremos más adelante (sección \ref{FourierDiscontinuidad}) y expresa este teorema, las series de Fourier son particularmente apropiadas para expandir funciones discontínuas (en un número finito de puntos en el intervalo), sin embargo, por ser una base de funciones contínuas no puede reproducir la discontinuidad como tal. La expansión de Fourier alrededor de un punto de discontinuidad $x \rightarrow x_{\pm 0}$ tenderá al valor $F(x) \rightarrow F(x_{\pm 0}) \equiv F_{m}$ donde $F_{m} = \frac{F(x_{+ 0}) + F(x_{- 0}) }{2}$. Es decir, tenderá al valor medio de los valores de la discontinuidad por la izquierda $F(x_{- 0}) $ y por la derecha $F(x_{+ 0}) $.  
  \item Si los coeficientes de Fourier tienen variaciones acotadas en el intervalo y $|\tilde{C}_{k}  | \rightarrow 0$  con $k \rightarrow \infty $. Entonces 
\[
 \sum_{k=-\infty }^{\infty} | \tilde{C}_{k} |^{2} =  \frac{1 }{2 \pi} \int_{-\pi}^{\pi} \mathrm{d}x \ | f(x) |^{2}
 \qquad \Leftrightarrow \qquad \frac{1}{2} a_{0}^{2} +  \sum_{n=1 }^{\infty} | a_{n}^{2} + b_{n}^{2} |=  \frac{1 }{\pi} \int_{-\pi}^{\pi} \mathrm{d}x \ | f(x) |^{2}
\]
que no es otra cosa que la expresión de la completitud de esta base de funciones. 
\end{itemize}
\subsubsection{Un par de Ejemplos}
\label{FourierEjemplos}
Para ilustrar esta relación entre la función $f(x)$ y su expansión en serie de Fourier $F(x)$ analicemos, para empezar, el caso de una función muy conocida en el ámbito de los circuitos eléctrico. Una onda cuadrada
\[
f(t)=
\left\{
\begin{array}{rl}
    -1  & \text{si } -\frac{1}{2}T \leq t <  0   \\
      &    \\
    +1  & \text{si } 0 \leq t \leq \frac{1}{2}T      
\end{array}
\right. \quad \Rightarrow b_{n} =   \frac{2}{T} \int_{-\frac{T}{2}}^{\frac{T}{2}} \mathrm{d}t \ f( t) \ \mathrm{sen} \left( \frac{2 \pi n t}{T} \right) =
\frac{4}{T} \int_{0}^{\frac{T}{2}} \mathrm{d}t \ f( t) \ \mathrm{sen} \left( \frac{2 \pi n t}{T} \right)  
\]
porque los coeficientes pares ($a_{n}$) se anulan. Entonces
\[
b_{n} =   \frac{2}{n \pi} \left( 1- (-1)^{n} \right) \quad \Rightarrow 
f(t) = \frac{4}{\pi} \left( \mathrm{sen} \omega t + \frac{\mathrm{sen} 3 \omega t}{3} +  \frac{\mathrm{sen} 5 \omega t}{5} +  \frac{\mathrm{sen} 7 \omega t}{7} + \cdots \right)
\]
los coeficientes pares $b_{2n}$ se anulan y además hemos denotado $\omega = 2 \pi/ T $. 

Otro caso, complementario al anterior por sus propiedades de simetría, es la expansión en series de Fourier de la función $f(x)= x^{2}$ para $-\pi <  x  < \pi$. Entonces los coeficientes se la expansión serán 
\[
f(x)= x^{2} \quad \Rightarrow 
\left\{ 
\begin{array}{ll}
  a_{0}  = \frac{1}{\pi} \int_{-\pi }^{\pi} \mathrm{d}x \ x^{2}  & = \frac{2 \pi^{2}}{3}    \\ 
  	&  	\\
  a_{n} = \frac{2}{\pi} \int_{0}^{\pi} \mathrm{d}x \ x^{2} \cos(nx)   & =   \frac{4(-1)^{n}}{ \pi^{2} n^{2}}   
\end{array}
\right.
\] ya que los coeficientes correspondientes a los términos impares $b_{n}$ se anulan. Con lo cual
\[
x^{2} = \frac{ \pi^{2}}{3} + 4 \sum_{n=1}^{\infty} \frac{(-1)^n \cos(nx) }{n^{2}}
\]
Nótese que como un resultado particular, al evaluar en $x= \pi$, se tiene la función zeta de Riemann $\zeta(2)$
\[
\pi^{2} = \frac{\pi^{2}}{3} + 4\sum_{n=1}^{\infty} \frac{1}{n^{2}} \quad \Rightarrow 
\zeta(2) \equiv \sum_{n=1}^{\infty} \frac{1}{n^{2}} =  \frac{ \pi^{2}}{6}
\]

Pero este caso se presta también para considerar funciones no periódicas. Supongamos que queremos desarrollar la expansión de Fourier para $f(x)= x^{2}$ pero en este caso con $0 <  x  < 2 $. Si este fuera el caso, empezamos por suponer que la función tienen un período, digamos $T =4$. Esto es $-2 \leq x \leq 2$. Con lo cual
\begin{eqnarray}
 a_{0} & = & \frac{2}{4} \int_{-2 }^{2} \mathrm{d}x \ x^{2}  =\frac{4}{4} \int_{0}^{2} \mathrm{d}x \ x^{2}  =\frac{8}{3}  \nonumber \\
a_{n} & = &  \frac{2}{4} \int_{-2}^{2} \mathrm{d}x \ x^{2} \cos \left( \frac{2 \pi nx}{4} \right)   =  
 \frac{4}{4} \int_{0}^{2} \mathrm{d}x \ x^{2} \cos \left( \frac{ \pi nx}{2} \right)   =  \frac{16}{\pi^{2} n^{2}} \cos n\pi = \frac{16}{ \pi^{2} n^{2}}(-1)^{n}   
		\nonumber 
\end{eqnarray}
Con lo cual tendremos que 
\[
x^{2}= \frac{4}{3} + 16 \sum_{n=1}^{\infty} \frac{(-1)^n }{\pi^{2} n^{2}} \cos \left( \frac{ \pi nx}{2}  \right) \quad \text{para } 0 < x \leq 2
\]

\subsection{Consideraciones de Simetría}
Es de hacer notar que esta propiedades de simetría respecto al período de la función ($f(x) = f(-x)$ simetría y $f(x) = -f(-x)$ antisimetría) para un período $-\frac{T}{2} \leq x \leq \frac{T}{2}$ pueden y deben ser explotadas para simplificar los cálculos. Esto se puede resumir en 
\[
f(x) = f(-x) \Rightarrow 
\left\{ 
\begin{array}{l}
  a_{n} \neq 0      \\
  b_{n} = 0        
\end{array}
\right. \quad \text{y alternativamente} \quad
f(x) = -f(-x) \Rightarrow 
\left\{ 
\begin{array}{ l}
  a_{n} = 0      \\
  b_{n}  \neq 0        
\end{array}
\right.
\]

Pero más interesante aún es cuando estas propiedades de simet'ria se presentan en un cuarto del período. Vale decir, que $f(x)$ será par o impar respecto a $T/4$ i.e. $f\left( \frac{T}{4} +x \right) = \pm f\left( \frac{T}{4} -x \right) \Rightarrow f(-s) = \pm f(s)$ donde $s = \frac{T}{4} -x$. Entonces
\[
 b_{n} =   \frac{2}{T} \int_{x_{0}}^{x_{0} + T} \mathrm{d}s \ f( s) \ \mathrm{sen} \left( \frac{2 \pi ns}{T}  + \frac{\pi n}{2} \right) 
\]
Donde los límites de integración no se han visto alterados porque la función es periódica. Es inmediato comprobar que 
\[
\mathrm{sen} \left( \frac{2 \pi ns}{T}  + \frac{\pi n}{2} \right)  =
\mathrm{sen} \left( \frac{2 \pi ns}{T} \right)  \cos \left( \frac{\pi n}{2} \right) + 
 \cos \left( \frac{2 \pi ns}{T} \right)   \mathrm{sen} \left( \frac{\pi n}{2} \right)
\]
es decir 
\[
b_{n} =   \frac{2}{T} 
\left(
\cos \left( \frac{\pi n}{2} \right) \int_{x_{0}}^{x_{0} + T} \mathrm{d}s \ f( s) \mathrm{sen} \left( \frac{2 \pi ns}{T} \right)   + 
\mathrm{sen} \left( \frac{\pi n}{2} \right) \int_{x_{0}}^{x_{0} + T} \mathrm{d}s \ f( s) \ \cos \left( \frac{2 \pi ns}{T} \right)   
\right)
\]
por lo que si $n =2k \Rightarrow \mathrm{sen} \left( \frac{\pi n}{2} \right) = \mathrm{sen}( \pi k ) = 0 $ 
y si $n =2k-1 \Rightarrow \cos \left( \frac{ 2k-1}{2} \pi\right) = 0 $. Ta misma consideración se puede hacer para los coeficientes $a_{n}$ (queda como ejercicio para el lector) y se puede concluir que 
\begin{itemize}
  \item Si $f(x)$ par en $T/4$ entonces $a_{2n -1} = b_{2n} = 0$ 
  \item Si $f(x)$ impar en $T/4$ entonces $a_{2n} = b_{2n-1} = 0$
\end{itemize}

\begin{figure}
\begin{center}
\includegraphics[width=5in]{EscalonFourier.jpg}
\caption{ Aproximación por series de Fourier para la función escalón $\{ f(x) = 0 \text{ para } -\infty < x < 1 \text{ y } f(x)=1 \text{ para } x \geq 0 \}$ Las curvas corresponden a sumas parciales de Fourier: $F_{40}(x), F_{100}(x), F_{200}(x), $ }
\label{EscalonFourier}
\end{center}
\end{figure}

\subsection{Tratamiento de discontinuidades}
\label{FourierDiscontinuidad}
Tal y como hemos mencionado, a diferencia de las series de potencias, las series de Fourier manejan razonablemente bien las discontinuidades, pero por ser una base de funciones contínuas, no puede reproducirlas. Tal y como comentamos en el Teorema de Fourier (sección \ref{TeoremaFourier}) y muestra la figura \ref{EscalonFourier} el valor de las sumas parciales de Fourier en un punto de discontinuidad $x = x_{\pm 0}$ será el promedio de los valores $F(x_{- 0}) $ (por la izquierda) y $F(x_{+ 0}) $ (por la derecha)  en la discontinuidad.  Esto es la expansión de Fourier alrededor de un punto de discontinuidad $x \rightarrow x_{\pm 0}$ tenderá al valor $F(x) \rightarrow F(x_{\pm 0}) \equiv F_{m}$ donde $F_{m} = \frac{F(x_{+ 0}) + F(x_{- 0}) }{2}$. 

\subsubsection{El Fenómeno de Gibbs}
Pero también se muestra en esa figura  \ref{EscalonFourier} que, tanto por la izquierda como por la derecha la discontinuidad de la función escalón, las sumas parciales de Fourier oscilan y no convergen a los valores $x_{\pm 0}$. El comportamiento oscilante de las sumas parciales de Fourier alrdedor de las discontinuidades, que no desaparecen ni en el límite se   denominan \textit{fenómeno de Gibbs} en honor a su descubridor Josiah Willard Gibbs\footnote{\textbf{Josiah Willard Gibbs} 1839 - 1903 Algunos lo consideran el primer Físico Norteamericano, de hecho fue el primero en recibir un título de doctorado por una universidad norteameicana (Yale University). Hizo importantes aportes en electromagnetismo  y sobre todo en termodinámica y física estadística, sentando las bases matemáticas para estas disciplinas. En matemáticas es conocido su estudio de las oscilaciones de las expansiones de las series de Fourier en los puntos de discontinuidad. Más detalles            \url{http://www-history.mcs.st-and.ac.uk}  }

Para entender qué pasa en la discontinuidad consideremos una variación de la onda cuadrada considerada anteriormente (\ref{FourierEjemplos}). Entonces sus sumas parciales serán
\[
f(t)=
\left\{
\begin{array}{rl}
    1  & \text{si } 0 \leq t <  \pi   \\
      &    \\
    0  & \text{si } \pi \leq t < 2 \pi      
\end{array}
\right. \quad \Rightarrow F_{2n}^{c}(x) = \frac{1}{2} + \frac{2}{\pi} \sum_{k=1}^{n} \frac{1}{2k - 1}
\mathrm{sen}((2k - 1)x)  
\]
porque los coeficientes pares ($a_{n}$) se anulan. 
Para estudiar el fenómeno de Gibbs reescribimos la suma parcial anterior de una manera ingeniosa 
\[
F_{2n}^{c}(t) =   \frac{1}{2} +  \frac{2}{\pi} \sum_{k=1}^{n} \left( \int_{0}^{t} \mathrm{d}s \ \cos(2k -1)s \right) =  
 \frac{1}{2} +  \frac{2}{\pi} \int_{0}^{t} \mathrm{d}s \left( \sum_{k=1}^{n} \cos(2k -1)s  \right) = 
  \frac{1}{2} +  \frac{1}{\pi} \int_{0}^{t} \mathrm{d}s \left( \frac{ \mathrm{sen}(2ns)}{ \mathrm{sen}( s)} \right)
\]
donde, utilizando la fórmula de Moivre y convirtiendo esa serie de cosenos en una de exponenciales la cual, a su vez es una progresión geométrica (y le queda la comprobación al lector), hemos sustituido
\[
\sum_{k=1}^{n} \cos(2k -1)s = \frac{ \mathrm{sen}(2ns)}{ \mathrm{sen}( s)}
\]

Es inmediato convencerse que las sumas parciales $F_{2n}^{c}(x)$ siempre tendrán máximos y mínimos 
\[
\dfrac{ \mathrm{d} F_{2n}^{c}(x)}{ \mathrm{d}x} =  \frac{ \mathrm{sen}(2nx)}{ \mathrm{sen}( x)} = 0 \qquad \Rightarrow \text{para  }
x = \frac{m \pi}{2n } \quad \text{con } m=1,2,3,\cdots 
\]
Las Series de Fourier tienden a sobre-estimar el valor de los puntos de discontinuidad en $\pm 18 \%$ esto es un valor de $\approx 1.1789797$. La inclusión de más términos en las sumas parciales no mejoran la situación. El fenómeno de Gibbs no se restringe a Series de Fourier sino que también se presenta en las demás series de funciones (ver detalles en la referencia \cite{ArfkenWeberWeber2000}) .

El fenómeno de Gibbs fue observado ¡ experimentalmente ! por primera vez por Albert Michelson\footnote{\textbf{Albert Abraham Michelson} Strelno, Prussia, 1852 - Pasadena EEUU. 1931. Premio Nobel en Física (1907) uno de los físicos experimentales más habilidosos de todos los tiempos. La precisión y lo ingenioso de los instrumentos creados por él son famosos.  Con importantes contribuciones en medidas de fenómenos en óptica. Una de sus contribuciones más conocidas son los experimentos para mostrar la inexistencia del Ether como medio de trasmisión para el fenómeno electromagnético. Más detalles \url{http://nobelprize.org/physics/laureates/1907/michelson-bio.html}} Para finales de 1800 Michelson había creado un dispositivo mecánico para medir las componentes de Fourier de señales eléctricas. Al incorporarle una onda cuadrada observó que una oscilación inesperada en los puntos de discontinuidad. Creyó que esa oscilación se debía a defectos del dispositivo. Luego de probar múltiples tipos de señales periódicas y observar un comportamiento similar, decidió comentárselo a su amigo Willard Gibbs, de la Universidad Yale. Al poco tiempo Gibbs volvió una explicación que dejó intacta la fama de Michelson como instrumentista. El fenómeno es una consecuencia de la teor'ria de series de Fourier y no del equipo diseñado por Michelson\footnote{Más detalles \url{http://en.wikipedia.org/wiki/Gibbs_phenomenon}}. 

\subsubsection{Corrección al fenómeno de Gibbs: Factor $\sigma$ de Lanczos}
Una de las estrategia para corregir las oscilaciones del fenómeno de Gibbs se le debe a Lanczos\footnote{Cornelius Lanczos 1893 - 1974 Hungría. Matemático húngaro con contribuciones importante en Relatividad y Física Teórica. En matemáticas es conocido inventar la transformada rápida de Fourier. Más detalles en \url{http://www-history.mcs.st-and.ac.uk/Biographies/Lanczos.html}}  Considerando el mismo caso de la función onda cuadrada, se puede intentar sustituir la función oscilante $F_{n}^{c}(x)$ por su promedio $\bar{F}_{n}^{c}(x)$ alrededor del punto $x$. Vale decir
\[
F_{2n}^{c}(x) \rightarrow \bar{F}_{2n}^{c}(x) = \frac{n}{\pi}  \int_{x - \frac{\pi}{2n} }^{x + \frac{\pi}{2n} } \mathrm{d}s \ F_{2n}^{c}(s) =  \frac{n}{\pi}  \int_{x - \frac{\pi}{2n} }^{x + \frac{\pi}{2n} } \mathrm{d}s \left[ \frac{1}{2} + \frac{2}{\pi} \sum_{k=1}^{n} \frac{1}{2k - 1} \mathrm{sen}((2k - 1)s) \right]
\]
desarrando tendremos que 
\begin{eqnarray}
\bar{F}_{2n}^{c}(x) & = &   \frac{n}{\pi}  \int_{x - \frac{\pi}{2n} }^{x + \frac{\pi}{2n} } \mathrm{d}s \left[ \frac{1}{2} + \frac{2}{\pi} \sum_{k=1}^{n} \dfrac{1}{2k - 1} \mathrm{sen}((2k - 1)s) \right] 
\nonumber \\
 & = &  \frac{n}{\pi} \left[ \frac{\pi}{2n} + \frac{2}{\pi} \sum_{k=1}^{n}  \left. \dfrac{1}{(2k - 1)^{2}} \cos((2k-1)s) \right|_{x - \frac{\pi}{2n} }^{x + \frac{\pi}{2n} }  \right] \nonumber \\
\bar{F}_{2n}^{c}(x) & = &  \frac{1}{2} + \frac{2}{\pi}  \sum_{k=1}^{n}  \dfrac{1}{2k - 1}
\underbrace{ \left[ \dfrac{ \mathrm{sen}\left( \frac{\pi}{2n} (2k - 1) \right) }{\frac{\pi}{2n}(2k - 1)}\right] }_{\sigma} \mathrm{sen}((2k - 1)x)	\nonumber
\end{eqnarray}
Con lo cual hemos identificado el factor $\sigma$ de Lanczos. Siguiendo este mismo proceso se puede generalizar para cualquier función de tal modo que una serie de Fourier genérica podrá ser corregida con un factor $\sigma$ para lograr
\[
\bar{F}_{n}(x) =  \frac{a_{0}}{2} +  \sum_{k=1}^{n-1} 
\left[ \dfrac{ \mathrm{sen}\left( \frac{k\pi}{n} \right) }{\left( \frac{k\pi}{n} \right) } \right]
 \left( a_{k} \cos(kx) + b_{k} \mathrm{sen}(kx) \right) \equiv
  \frac{a_{0}}{2} +  \sum_{k=1}^{n-1} \sigma_{k} \left( a_{k} \cos(kx) + b_{k} \mathrm{sen}(kx) \righ)
\]
\subsection{Tranformadas Discretas de Fourier}
Hemos visto que una de las posibles expresiones para una serie de Fourier en el intervalo $-T \leq x \leq T$ es
\[
F(x)		 =   \frac{1}{2 T } \int_{-T}^{T} \mathrm{d}t  \ f(t) 
+  \frac{1}{T} \sum_{n=1}^{\infty} \int_{-T}^{T}\mathrm{d}t \ f( t)  \cos \left( \frac{n \pi }{T}(t-x) \right) 
\] Ahora bien, podemos hacer $T \rightarrow \infty$ con lo cual $[-T,T]  \rightarrow [-\infty,\infty]$ pero además 
\[
\text{si definimos } \omega = \frac{n \pi }{T} \quad \text{entonces } T \rightarrow \infty \Rightarrow  \frac{ \pi }{T} = \frac{\omega}{n} \approx \Delta \omega \quad \text{y además }
\frac{ \int_{-T}^{T} \mathrm{d}t  \ f(t)}{2 T } \rightarrow 0
\]
ya que hemos supuesto que $\int_{-\infty }^{\infty } \mathrm{d}t  \ f(t)$ existe y es acotada. De este modo podremos escribir  
\[
F(x) \rightarrow \frac{1}{\pi}  \Delta \omega  \sum_{n=1}^{\infty} \int_{-\infty}^{\infty} \mathrm{d}t \ f( t)  \cos \left( \omega {T}(t-x) \right) \rightarrow F(x) = \int_{0}^{\infty} \mathrm{d}\omega 
\int_{-\infty}^{\infty}\mathrm{d}t \ f( t)  \cos( \omega (t-x) )
\]
Esta última expresión se conoce con el nombre de tranformada de Fourier y la estudiaremos con detalle más adelante como parte del tema de transformadas integrales del tipo.
\[
 F(x) \propto \int_{a}^{b} \mathrm{d}t \ f(t) \ \mathcal{K}(x,t)
\] Donde la cantidad $ \mathcal{K}(x,n,t)$ de denominará el \textit{kernel} (núcleo en Alemán). Dependiendo del núcleo tendremos una transformación distinta. 

Aquí haremos algo más contemporaneo que será estudiar la versión discreta de esta transformación. En general las integrales, en su mayoría, no se pueden resolver analíticamente por lo que tenemos que proceder a resolverlas de forma numérica. La mayor parte de los métodos numéricos involucra convertir  integrales en sumatorias. Es decir en series de funciones.

\subsubsection{Ortogonalidad en puntos y Transformadas Discretas}
Probaremos que las funciones $e^{2 \pi i p t_{k}/ T }$ y  $e^{2 \pi i q t_{k}/ T }$ serán ortogonales $\propto \delta_{qp}$ en un conjunto de puntos $t_{k}$. Esto es
\[
\sum_{k=0}^{2N-1} \left[ e^{ \frac{2 \pi i p t_{k} }{ T} } \right]^{*}e^{\frac{2 \pi i q t_{k}}{ T}} = \sum_{k=0}^{2N-1} e^{\frac{2 \pi i s t_{k} }{ T} } = \sum_{k=0}^{2N-1} e^{\frac{2 \pi i s k}{2N }} =
\left\{
\begin{array}{l l}
 \dfrac{1 -r^{2N}}{1-r} = 0 & r \neq 1       \\
       &   \\
  2N       & r=1
\end{array}
\right.
\]donde hemos sustituido $s = q - p$, y evaluado en los puntos $ t_{k} = \dfrac{kT}{2N} $ con $k=1,2,3,\cdots,2N -1$. Nótese que la última de las series es una serie finita y geométrica con razón $r = e^{(\pi i s)/N}$, que comienza con $1$ y por lo tanto suma (dependiendo del valor de $r$) lo que aparece en la llave. Es inmediato convencerse que, para todo $N$ se cumple que $r^{2N} = e^{2\pi i s} = 1 $ (con $s$ entero) con lo cual se cumple la relación de ortogonaliadad que buscamos
\[
\sum_{k=0}^{2N-1} \left[ e^{ \frac{2 \pi i p t_{k} }{ T} } \right]^{*}e^{\frac{2 \pi i q t_{k}}{ T}} =
2N \delta_{qp} \quad \text{con } k=1,2,3,\cdots,2N -1
\]
Si hacemos un ligero cambio de notación y llamamos $\omega_{m} \dfrac{2\pi m}{T}$

\subsubsection{Transformada Rápida de Fourier FFT}

\begin{thebibliography}{9}

\bibitem{ArfkenWeberWeber2000}Arfken, G. B., Weber, H., y Weber, H.J. (2000)
\textbf{Mathematical Methods for Physicists} 5ta Edición (\textit{Academic Press, Nueva York})

\bibitem{ByronFuller1970}Byron, F.W. y Fuller W.F. (1970) \textbf{Mathematics of Classical and Quantum Physics } (\textit{Dover Publications, New York})

\bibitem{Cushing1975}Cushing, J. (1975)\textbf{ Applied Analytical Mathematics for Physical Sciences } (\textit{John Wiley \& Sons, New York})

\bibitem{Hamming1973} Hamming R.W. (1973) \textbf{Numerical Methods For Scientist and Engineers, 2nd ed. } (Dover, New York.)

\bibitem{Hassani1991}  Hassani, S. (1991) \textbf{Foundations of Mathematical Physics} 
(\textit{Prentice Hall, International Edition, London:})

\bibitem{Lebedev1972} Lebedev, N.N. (1972) \textbf{Special Functions \& Their Applications} (\textit{Dover Publications, New York})

\bibitem{Richards2002} Richards, D. (2002) \textbf{Advanced Mathematical Methods with MAPLE} (\textit{Cambridge University Press Cambridge})

\bibitem{RileyHobsonBence2002}  Riley, K.F., Hobson, M.P. y Bence, S.J. (2002)
\textbf{Mathematical Methods for Physics and Engineering } (\textit{Cambridge University Press Cambridge})

\bibitem{WeissteinURL} Weisstein, E. W.,  \textbf{MathWorld} \url{http://mathworld.wolfram.com/}

\end{thebibliography}

\end{document}

  